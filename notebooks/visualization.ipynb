{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Log entries to be parsed\n",
    "logs = \"\"\"\n",
    "24-05-01 12:33:55.898 : <epoch:  0, iter:     200, lr:1.000e-03> G_loss: 8.677e-03 F_loss: 4.688e+01 D_loss: 1.248e+03 D_real: -5.158e+04 D_fake: -2.495e+05 \n",
    "24-05-01 12:40:44.783 : <epoch:  1, iter:     400, lr:1.000e-03> G_loss: 3.437e-03 F_loss: 2.757e+01 D_loss: 9.833e+02 D_real: -5.255e+04 D_fake: -1.967e+05 \n",
    "24-05-01 15:17:38.281 : <epoch: 24, iter:   5,000, lr:1.000e-03, Average PSNR : 11.00dB\n",
    "24-05-01 18:08:12.884 : <epoch: 48, iter:  10,000, Average PSNR : 15.19dB\n",
    "24-05-02 08:20:24.238 : <epoch:169, iter:  35,000, Average PSNR : 24.73dB\n",
    "24-05-02 11:10:36.328 : <epoch:194, iter:  40,000, Average PSNR : 26.08dB\n",
    "\"\"\"\n",
    "\n",
    "# Regular expression to extract data from logs\n",
    "psnr_pattern = r\"<epoch:\\s*(\\d+),\\s*iter:\\s*(\\d+)[^<]*Average PSNR\\s*:\\s*([\\d\\.]+)dB\"\n",
    "\n",
    "# Extract PSNR values\n",
    "matches = re.findall(psnr_pattern, logs)\n",
    "epochs_psnr = [int(m[0]) for m in matches]\n",
    "psnr_values = [float(m[2]) for m in matches]\n",
    "\n",
    "gan_pattern = r\"<epoch:\\s*(\\d+),\\s*iter:\\s*(\\d+)[^>]*> G_loss:\\s*([\\d\\.e\\+\\-]+)\\s*F_loss:\\s*([\\d\\.e\\+\\-]+)\\s*D_loss:\\s*([\\d\\.e\\+\\-]+)\"\n",
    "\n",
    "# Extract GAN loss values\n",
    "gan_matches = re.findall(gan_pattern, logs)\n",
    "epochs_gan = [int(m[0]) for m in gan_matches]\n",
    "g_losses = [float(m[2]) for m in gan_matches]\n",
    "f_losses = [float(m[3]) for m in gan_matches]\n",
    "d_losses = [float(m[4]) for m in gan_matches]\n",
    "\n",
    "# Creating a combined plot for GAN losses and PSNR\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plotting GAN losses\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('GAN Loss', color='tab:red')\n",
    "ax1.plot(epochs_gan, g_losses, marker='o', color='r', linestyle='-', label='G Loss')\n",
    "ax1.plot(epochs_gan, f_losses, marker='x', color='g', linestyle='--', label='F Loss')\n",
    "ax1.plot(epochs_gan, d_losses, marker='^', color='orange', linestyle=':', label='D Loss')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Plotting PSNR values on secondary y-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('PSNR (dB)', color='tab:blue')\n",
    "ax2.plot(epochs_psnr, psnr_values, marker='o', color='b', label='PSNR (dB)')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Adding grid and title\n",
    "plt.grid(True)\n",
    "plt.title('GAN Losses and PSNR Over Epochs')\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmath import e\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "epochs = list(range(205))\n",
    "G_losses = [8.677e-03, 3.437e-03, 1.397e-03, 1.770e-03, 1.073e-03, 1.226e-03, 8.501e-04, 6.261e-04, 5.950e-04, 8.485e-04,\n",
    "            3.111e-04, 9.087e-04, 5.342e-04, 4.222e-04, 1.295e-03, 3.696e-03, 6.512e-04, 5.410e-04, 7.432e-04, 4.429e-04,\n",
    "            4.920e-04, 2.699e-04, 4.965e-04, 6.154e-04, 7.178e-04, 6.994e-04, 3.791e-04, 3.342e-03, 6.551e-04, 6.295e-04,\n",
    "            5.138e-04, 6.655e-04, 4.634e-04, 4.424e-04, 5.505e-04, 3.476e-04, 3.907e-04, 3.819e-04, 3.268e-04, 3.436e-04,\n",
    "            6.106e-04, 2.514e-04, 2.688e-04, 2.310e-04, 1.439e-04, 2.283e-04, 2.442e-04, 2.895e-04, 2.558e-04, 2.962e-04,\n",
    "            2.830e-04, 2.259e-04, 2.665e-04, 2.282e-04, 2.322e-04, 3.205e-04, 2.425e-04, 2.326e-04, 3.254e-04, 3.629e-04,\n",
    "            2.264e-04, 3.053e-04, 3.395e-04, 2.767e-04, 3.298e-04, 2.641e-04, 4.987e-04, 2.652e-04, 3.752e-04, 4.138e-04,\n",
    "            5.050e-04, 2.848e-04, 3.777e-04, 4.019e-04, 4.595e-04, 3.633e-04, 4.924e-04, 4.384e-04, 4.253e-04, 4.336e-04,\n",
    "            5.150e-04, 5.132e-04, 6.296e-04, 4.207e-04, 6.640e-04, 5.324e-04, 1.034e-03, 2.920e-03, 9.987e-02, 2.140e-02,\n",
    "            1.890e-02, 3.495e-04, 3.236e-03, 3.100e-03, 2.584e-03, 1.735e-03, 2.607e-03, 1.474e-03, 1.772e-03, 1.261e-03,\n",
    "            6.495e-04, 4.581e-04, 3.135e-04, 1.680e-04, 1.094e-04, 7.127e-05, 4.507e-05, 7.016e-05, 6.208e-05, 6.030e-05,\n",
    "            6.302e-05, 4.489e-05, 4.025e-05, 3.671e-05, 4.386e-05, 1.419e-04, 4.726e-05, 4.816e-05, 9.083e-04, 8.754e-05,\n",
    "            1.672e-04, 1.250e-04, 5.975e-05, 4.474e-05, 3.715e-05, 6.661e-05, 4.860e-05, 5.344e-05, 5.086e-05, 4.359e-05,\n",
    "            2.815e-05, 4.875e-05, 3.133e-05, 4.287e-05, 3.111e-05, 4.992e-05, 3.531e-05, 2.826e-05, 4.004e-05, 3.161e-05,\n",
    "            5.759e-05, 4.053e-05, 3.623e-05, 4.601e-05, 5.101e-05, 6.706e-05, 9.810e-05, 5.380e-05, 3.376e-05, 5.812e-05,\n",
    "            5.222e-05, 5.747e-05, 5.362e-04, 2.851e-05, 4.165e-05, 8.852e-05, 1.002e-04, 8.062e-05, 2.582e-05, 4.739e-05,\n",
    "            5.800e-05, 8.860e-05, 4.715e-05, 7.018e-05, 2.686e-05, 3.445e-05, 9.614e-05, 1.083e-04, 8.445e-05, 1.246e-04,\n",
    "            4.934e-05, 5.361e-05, 2.959e-04, 6.030e-05, 5.063e-05, 6.921e-05, 3.306e-05, 1.144e-04, 2.030e-05, 3.417e-04,\n",
    "            3.520e-05, 2.909e-05]\n",
    "\n",
    "F_losses = [4.688e+01, 2.757e+01, 1.204e+01, 7.502e+00, 7.239e+00, 8.704e+00, 5.985e+00, 6.411e+00, 6.803e+00, 7.313e+00,\n",
    "            6.836e+00, 7.117e+00, 5.873e+00, 4.463e+00, 8.593e+00, 7.587e+00, 5.079e+00, 6.002e+00, 6.417e+00, 5.817e+00,\n",
    "            6.912e+00, 6.959e+00, 5.851e+00, 6.498e+00, 5.880e+00, 5.159e+00, 5.776e+00, 8.909e+00, 7.936e+00, 7.096e+00,\n",
    "            5.614e+00, 6.198e+00, 5.141e+00, 5.263e+00, 6.601e+00, 8.427e+00, 6.810e+00, 6.321e+00, 7.354e+00, 6.682e+00,\n",
    "            8.083e+00, 6.467e+00, 5.990e+00, 7.307e+00, 4.951e+00, 6.086e+00, 5.804e+00, 7.339e+00, 6.418e+00, 5.738e+00,\n",
    "            5.763e+00, 6.933e+00, 4.763e+00, 4.526e+00, 7.731e+00, 4.991e+00, 4.906e+00, 3.581e+00, 4.807e+00, 3.773e+00, 4.044e+00,\n",
    "             6.227e+00, 3.659e+00, 5.449e+00, 5.264e+00, 5.652e+00, 6.346e+00, 6.464e+00, 4.539e+00, 3.918e+00, 5.217e+00,\n",
    "             5.407e+00, 7.995e+00, 5.998e+00, 9.383e+01, 7.914e+00, 8.014e+00, 7.608e+00, 7.251e+00, 6.045e+00, 5.488e+00,\n",
    "             4.656e+00, 6.406e+00, 4.777e+00, 4.351e+00, 5.463e+00, 2.460e+00, 3.792e+00, 3.617e+00, 3.348e+00, 3.503e+00,\n",
    "             3.806e+00, 3.861e+00, 3.764e+00, 4.202e+00, 3.718e+00, 4.153e+00, 3.816e+00, 2.910e+00, 2.975e+00, 3.362e+00,\n",
    "             3.573e+00, 3.242e+00, 3.075e+00, 4.430e+00, 5.447e+00, 2.648e+00, 3.392e+00, 2.576e+00, 3.044e+00, 2.825e+00,\n",
    "             2.617e+00, 4.385e+00, 2.066e+00, 3.815e+00, 3.383e+00, 2.762e+00, 3.952e+00, 2.997e+00, 3.908e+00, 2.281e+00,\n",
    "             3.266e+00, 2.779e+00, 2.342e+00, 3.727e+00, 2.481e+00, 3.731e+00, 3.476e+00, 3.453e+00, 3.487e+00, 2.793e+00,\n",
    "             2.585e+00, 2.990e+00, 2.461e+00, 3.412e+00, 3.208e+00, 3.688e+00, 2.339e+00, 2.366e+00, 2.051e+00, 3.394e+00,\n",
    "             2.916e+00, 2.643e+00, 2.857e+00, 3.663e+00, 2.938e+00, 3.320e+00, 2.397e+00, 2.563e+00, 2.859e+00, 3.602e+00,\n",
    "             3.073e+00, 2.988e+00, 2.511e+00, 2.315e+00, 2.678e+00, 2.992e+00, 2.572e+00, 2.354e+00, 2.464e+00, 2.781e+00,\n",
    "             2.015e+00, 3.387e+00, 2.594e+00, 2.286e+00]\n",
    "\n",
    "D_losses = [1.248e+03, 9.833e+02, 7.980e+02, 8.198e+02, 7.470e+02, 7.264e+02, 8.900e+02, 4.832e+02, 2.812e+02, 2.479e+02,\n",
    "            1.661e+02, 3.510e+02, 6.841e+02, 3.734e+02, 5.705e+02, 1.427e+01, 1.177e+00, 7.353e-01, 6.791e-01, 6.077e-01,\n",
    "            5.672e-01, 7.991e-01, 7.800e-01, 9.279e-01, 1.454e+00, 2.197e+00, 1.954e+01, 1.539e+03, 9.001e+02, 7.123e+02,\n",
    "            5.549e+02, 5.358e+02, 3.732e+02, 4.592e+02, 4.497e+02, 4.227e+02, 4.464e+02, 4.025e+02, 4.483e+02, 4.718e+02,\n",
    "            3.954e+02, 2.445e-01, -7.838e-03, -2.639e-02, -2.209e-02, -2.252e-02, -2.578e-02, -3.173e-02, -2.869e-02, -3.188e-02,\n",
    "            -3.197e-02, -2.338e-02, -3.180e-02, -3.163e-02, -3.222e-02, -3.146e-02, -3.415e-02, -3.351e-02, -3.368e-02, -3.380e-02,\n",
    "            -3.381e-02, -3.198e-02, -3.405e-02, -3.303e-02, -3.390e-02, -3.369e-02, -3.206e-02, -3.492e-02, -3.413e-02, -3.161e-02,\n",
    "            -3.047e-02, -3.327e-02, -2.927e-02, -3.313e-02, -3.321e-02, -2.714e-02, -3.080e-02, -2.780e-02, -2.926e-02, -2.764e-02,\n",
    "            -2.633e-02, -2.332e-02, -8.527e-03, -2.203e-02, -2.385e-02, -1.392e-02, -1.077e-02, 8.793e+02, 7.406e-02, 9.737e+00,\n",
    "            -4.182e-01, 3.624e+02, -8.885e-01, -7.904e-01, -7.450e-01, -7.027e-01, -6.362e-01, -6.196e-01, -6.107e-01, -6.252e-01,\n",
    "            -6.397e-01, -6.670e-01, -7.050e-01, -7.397e-01, -8.510e-01, -7.816e-01, -6.789e-01, -6.363e-01, -4.000e-01, -3.731e-01,\n",
    "            -3.351e-01, -3.331e-01, -3.603e-01, -4.184e-01, -3.462e-01, -3.392e-01, -3.458e-01, -2.689e-01, -9.197e-01, -7.600e-01,\n",
    "            -3.856e-01, -2.818e-01, -2.767e-01, -2.906e-01, -2.558e-01, -2.309e-01, -2.233e-01, -2.060e-01, -2.046e-01, -1.809e-01,\n",
    "            -2.066e-01, -3.138e-01, -2.151e-01, -2.425e-01, -2.364e-01, -2.945e-01, -2.813e-01, 4.053e-05, 3.623e-05, 4.601e-05, 5.101e-05,\n",
    "             6.706e-05, 9.810e-05, 5.380e-05, 3.376e-05, 5.812e-05,\n",
    "            5.222e-05, 5.747e-05, 5.362e-04, 2.851e-05, 4.165e-05, 8.852e-05, 1.002e-04, 8.062e-05, 2.582e-05, 4.739e-05,\n",
    "            5.800e-05, 8.860e-05, 4.715e-05, 7.018e-05, 2.686e-05, 3.445e-05, 9.614e+02]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = min(len(G_losses), len(F_losses), len(D_losses))\n",
    "G_losses = G_losses[:max_epochs]\n",
    "F_losses = F_losses[:max_epochs]\n",
    "D_losses = D_losses[:max_epochs]\n",
    "epochs = list(range(max_epochs))\n",
    "\n",
    "# Plotting the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, G_losses, label='G_loss', marker='o', markersize=3)\n",
    "plt.plot(epochs, F_losses, label='F_loss', marker='o', markersize=3)\n",
    "plt.plot(epochs, D_losses, label='D_loss', marker='o', markersize=3)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss Values')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.title('Loss Values during GAN Training')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data based on the previously provided logs\n",
    "data = {\n",
    "    \"Epoch\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
    "    \"Generator Loss\": [1.061e-03, 1.084e-03, 8.870e-04, 9.943e-04, 1.382e-03, 1.274e-03, 1.210e-03,\n",
    "                       1.298e-03, 1.114e-03, 1.149e-03, 8.888e-04, 1.163e-03, 9.901e-04, 9.934e-04,\n",
    "                       1.204e-03, 1.267e-03, 1.315e-03, 1.356e-03, 1.315e-03, 1.009e-03, 9.831e-04],\n",
    "    \"Discriminator Loss\": [2.969e-03, 4.827e-03, 4.510e-03, 4.946e-03, 4.783e-03, 4.912e-03, 4.771e-03,\n",
    "                           4.927e-03, 4.833e-03, 5.022e-03, 4.852e-03, 4.856e-03, 4.779e-03, 4.887e-03,\n",
    "                           4.297e-03, 5.006e-03, 5.057e-03, 4.900e-03, 5.016e-03, 4.925e-03, 4.934e-03],\n",
    "    \"PSNR\": [13.56, 14.16, 13.29, 14.10, 12.56, 12.71, 12.30, 12.37, 12.90, 14.34, 15.14, 16.39,\n",
    "             18.89, 19.71, 21.08, 21.27, 20.18, 19.76, 19.39, 20.74, 21.41]  # Sample PSNR data\n",
    "}\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plotting\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss', color=color)\n",
    "ax1.plot(df['Epoch'], df['Generator Loss'], label='Generator Loss', color='red')\n",
    "ax1.plot(df['Epoch'], df['Discriminator Loss'], label='Discriminator Loss', color='green', linestyle='--')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('PSNR (dB)', color=color)\n",
    "ax2.plot(df['Epoch'], df['PSNR'], label='PSNR', color=color, linestyle='-.')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.title('Training Progress: Loss and PSNR over Epochs GATN')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assume img is your input image tensor of shape [1, Channels, Height, Width]\n",
    "# Let's create a random image tensor for demonstration\n",
    "channels, height, width = 3, 128, 128  # Example dimensions\n",
    "img = torch.randn(1, channels, height, width)\n",
    "\n",
    "block_size = 16\n",
    "stride = 8  # 50% overlap\n",
    "\n",
    "# Step 1: Divide into overlapping blocks\n",
    "unfolded = F.unfold(img, kernel_size=block_size, stride=stride)\n",
    "\n",
    "# Step 2: Manipulate blocks (if needed, you can process your blocks here)\n",
    "\n",
    "# Step 3: Reconstruct the image\n",
    "# Reconstructing the image\n",
    "reconstructed = F.fold(unfolded, output_size=(height, width), kernel_size=block_size, stride=stride)\n",
    "\n",
    "# Correction factor for overlapping patches\n",
    "# Create a ones tensor of the same shape as img\n",
    "one_tensor = torch.ones_like(img)\n",
    "unfolded_ones = F.unfold(one_tensor, kernel_size=block_size, stride=stride)\n",
    "corrected_ones = F.fold(unfolded_ones, output_size=(height, width), kernel_size=block_size, stride=stride)\n",
    "\n",
    "# Normalize the reconstructed image by the correction tensor\n",
    "reconstructed /= corrected_ones\n",
    "\n",
    "# Check if the reconstruction is perfect\n",
    "print(\"Reconstruction error (should be close to 0):\", torch.norm(img - reconstructed).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and transform image\n",
    "image_path = '/home/ozkan/works/n-smoe/utils/test.png'\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "img = transform(image).unsqueeze(0)\n",
    "\n",
    "# Image processing parameters\n",
    "channels, height, width = 3, 128, 128\n",
    "block_size = 16\n",
    "stride = 8\n",
    "\n",
    "# Divide into overlapping blocks\n",
    "unfolded = F.unfold(img, kernel_size=block_size, stride=stride)\n",
    "\n",
    "# Reconstruct the image\n",
    "reconstructed = F.fold(unfolded, output_size=(height, width), kernel_size=block_size, stride=stride)\n",
    "\n",
    "# Correction for overlapping patches\n",
    "one_tensor = torch.ones_like(img)\n",
    "unfolded_ones = F.unfold(one_tensor, kernel_size=block_size, stride=stride)\n",
    "corrected_ones = F.fold(unfolded_ones, output_size=(height, width), kernel_size=block_size, stride=stride)\n",
    "reconstructed /= corrected_ones\n",
    "\n",
    "# Verify reconstruction\n",
    "print(\"Reconstruction error (should be close to 0):\", torch.norm(img - reconstructed).item())\n",
    "\n",
    "# Plot the original and reconstructed images\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].imshow(transforms.ToPILImage()(img.squeeze(0)))\n",
    "ax[0].set_title('Original Image')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(transforms.ToPILImage()(reconstructed.squeeze(0)))\n",
    "ax[1].set_title('Reconstructed Image')\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfolded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "# GPU device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import cuda_block_ops\n",
    "\n",
    "# Load and preprocess the image\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    transform = transforms.ToTensor()\n",
    "    return transform(image).to(device)  # Move tensor to GPU\n",
    "\n",
    "# Extract overlapping blocks\n",
    "def extract_blocks(img_tensor, block_size, overlap):\n",
    "    blocks = []\n",
    "    step = block_size - overlap\n",
    "    for i in range(0, img_tensor.shape[1] - block_size + 1, step):\n",
    "        for j in range(0, img_tensor.shape[2] - block_size + 1, step):\n",
    "            block = img_tensor[:, i:i+block_size, j:j+block_size]\n",
    "            blocks.append(block)\n",
    "    return torch.stack(blocks).to(device)  # Stack and move blocks to GPU\n",
    "\n",
    "# Reconstruct the image from blocks\n",
    "def reconstruct_image(blocks, original_dims, block_size, overlap):\n",
    "    height, width = original_dims\n",
    "    step = block_size - overlap\n",
    "    recon_image = torch.zeros(3, height, width).to(device)\n",
    "    count_matrix = torch.zeros(3, height, width).to(device)\n",
    "\n",
    "    idx = 0\n",
    "    for i in range(0, height - block_size + 1, step):\n",
    "        for j in range(0, width - block_size + 1, step):\n",
    "            recon_image[:, i:i+block_size, j:j+block_size] += blocks[idx]\n",
    "            count_matrix[:, i:i+block_size, j:j+block_size] += 1\n",
    "            idx += 1\n",
    "\n",
    "    recon_image /= count_matrix\n",
    "    return recon_image\n",
    "\n",
    "\n",
    "image_path = '/home/ozkan/works/n-smoe/utils/test.png'\n",
    "image_tensor = load_image(image_path)\n",
    "blocks =  cuda_block_ops.extract_blocks(image_tensor, 16, 1) # extract_blocks(image_tensor, 16, 1)\n",
    "# reconstructed_image = cuda_block_ops.extract_blocks(blocks, image_tensor.shape[1:], 16, 1) # reconstruct_image(blocks, image_tensor.shape[1:], 16, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].imshow(transforms.ToPILImage()(image_tensor.squeeze(0)))\n",
    "ax[0].set_title('Original Image')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(transforms.ToPILImage()(reconstructed_image.squeeze(0)))\n",
    "ax[1].set_title('Reconstructed Image')\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import functools\n",
    "from typing import TypeVar, Generic, Literal, TypedDict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class BatchedViews(TypedDict):\n",
    "    image: torch.FloatTensor  # Shape: [batch, channels, height, width]\n",
    "\n",
    "T = TypeVar('T')\n",
    "\n",
    "class Backbone(nn.Module, Generic[T]):\n",
    "    def __init__(self, cfg: T):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def forward(self, context: BatchedViews) -> torch.FloatTensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def d_out(self) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "@dataclass\n",
    "class BackboneResnetCfg:\n",
    "    name: Literal[\"resnet\"]\n",
    "    model: Literal[\"resnet18\", \"resnet34\", \"resnet50\", \"resnet101\", \"resnet152\", \"dino_resnet50\"]\n",
    "    num_layers: int\n",
    "    use_first_pool: bool\n",
    "    d_out: int\n",
    "\n",
    "class BackboneResnet(Backbone[BackboneResnetCfg]):\n",
    "    def __init__(self, cfg: BackboneResnetCfg):\n",
    "        super().__init__(cfg)\n",
    "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n",
    "        model = getattr(torchvision.models, cfg.model)(pretrained=False, norm_layer=norm_layer)\n",
    "        # Remove the last fully connected layer and average pooling layer\n",
    "        self.model = nn.Sequential(*list(model.children())[:-2])\n",
    "        \n",
    "        self.projections = nn.ModuleDict()\n",
    "        out_channels = self.model[-1][-1].conv3.out_channels if hasattr(self.model[-1][-1], 'conv3') else 256\n",
    "        for index in range(cfg.num_layers):\n",
    "            self.projections[f'layer{index}'] = nn.Conv2d(out_channels, cfg.d_out, 1)\n",
    "\n",
    "    def forward(self, context: BatchedViews) -> torch.FloatTensor:\n",
    "        x = context['image']\n",
    "        x = self.model(x)\n",
    "        features = [self.projections[f'layer{index}'](x) for index in range(self.cfg.num_layers)]\n",
    "        output = torch.stack(features).sum(dim=0)\n",
    "        return output\n",
    "\n",
    "    @property\n",
    "    def d_out(self) -> int:\n",
    "        return self.cfg.d_out\n",
    "\n",
    "cfg = BackboneResnetCfg('resnet', 'resnet50', 5, False, 256)\n",
    "model = BackboneResnet(cfg)\n",
    "dummy_data = torch.rand(3, 3, 224, 224)  # Batch size of 3, 3 channels, 224x224 each\n",
    "context = BatchedViews(image=dummy_data)\n",
    "output = model(context)\n",
    "print(f'Output shape: {output.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from einops import rearrange\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor, nn\n",
    "from torchvision.models import ResNet\n",
    "\n",
    "from typing import Callable, Literal, TypedDict\n",
    "\n",
    "from jaxtyping import Float, Int64\n",
    "from torch import Tensor\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Generic, TypeVar\n",
    "\n",
    "\n",
    "class BatchedViews(TypedDict, total=False):\n",
    "    extrinsics: Float[Tensor, \"batch _ 4 4\"]  # batch view 4 4\n",
    "    intrinsics: Float[Tensor, \"batch _ 3 3\"]  # batch view 3 3\n",
    "    image: Float[Tensor, \"batch _ _ _ _\"]  # batch view channel height width\n",
    "    near: Float[Tensor, \"batch _\"]  # batch view\n",
    "    far: Float[Tensor, \"batch _\"]  # batch view\n",
    "    index: Int64[Tensor, \"batch _\"]  # batch view\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "class Backbone(nn.Module, ABC, Generic[T]):\n",
    "    cfg: T\n",
    "\n",
    "    def __init__(self, cfg: T) -> None:\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(\n",
    "        self,\n",
    "        context: BatchedViews,\n",
    "    ) -> Float[Tensor, \"batch view d_out height width\"]:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def d_out(self) -> int:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BackboneResnetCfg:\n",
    "    name: Literal[\"resnet\"]\n",
    "    model: Literal[\n",
    "        \"resnet18\", \"resnet34\", \"resnet50\", \"resnet101\", \"resnet152\", \"dino_resnet50\"\n",
    "    ]\n",
    "    num_layers: int\n",
    "    use_first_pool: bool\n",
    "    d_out: int\n",
    "\n",
    "\n",
    "class BackboneResnet(Backbone[BackboneResnetCfg]):\n",
    "    model: ResNet\n",
    "\n",
    "    def __init__(self, cfg: BackboneResnetCfg, d_in: int) -> None:\n",
    "        super().__init__(cfg)\n",
    "\n",
    "        assert d_in == 3\n",
    "\n",
    "        norm_layer = functools.partial(\n",
    "            nn.InstanceNorm2d,\n",
    "            affine=False,\n",
    "            track_running_stats=False,\n",
    "        )\n",
    "\n",
    "        if cfg.model == \"dino_resnet50\":\n",
    "            self.model = torch.hub.load(\"facebookresearch/dino:main\", \"dino_resnet50\")\n",
    "        else:\n",
    "            self.model = getattr(torchvision.models, cfg.model)(norm_layer=norm_layer)\n",
    "\n",
    "        # Set up projections\n",
    "        self.projections = nn.ModuleDict({})\n",
    "        for index in range(1, cfg.num_layers):\n",
    "            key = f\"layer{index}\"\n",
    "            block = getattr(self.model, key)\n",
    "            conv_index = 1\n",
    "            try:\n",
    "                while True:\n",
    "                    d_layer_out = getattr(block[-1], f\"conv{conv_index}\").out_channels\n",
    "                    conv_index += 1\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            self.projections[key] = nn.Conv2d(d_layer_out, cfg.d_out, 1)\n",
    "\n",
    "        # Add a projection for the first layer.\n",
    "        self.projections[\"layer0\"] = nn.Conv2d(\n",
    "            self.model.conv1.out_channels, cfg.d_out, 1\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        context: BatchedViews,\n",
    "    ) -> Float[Tensor, \"batch view d_out height width\"]:\n",
    "        # Merge the batch dimensions.\n",
    "        b, v, _, h, w = context[\"image\"].shape\n",
    "        x = rearrange(context[\"image\"], \"b v c h w -> (b v) c h w\")\n",
    "\n",
    "        # Run the images through the resnet.\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        features = [self.projections[\"layer0\"](x)]\n",
    "\n",
    "        # Propagate the input through the resnet's layers.\n",
    "        for index in range(1, self.cfg.num_layers):\n",
    "            key = f\"layer{index}\"\n",
    "            if index == 0 and self.cfg.use_first_pool:\n",
    "                x = self.model.maxpool(x)\n",
    "            x = getattr(self.model, key)(x)\n",
    "            features.append(self.projections[key](x))\n",
    "\n",
    "        # Upscale the features.\n",
    "        features = [\n",
    "            F.interpolate(f, (h, w), mode=\"bilinear\", align_corners=True)\n",
    "            for f in features\n",
    "        ]\n",
    "        features = torch.stack(features).sum(dim=0)\n",
    "\n",
    "        # Separate batch dimensions.\n",
    "        return rearrange(features, \"(b v) c h w -> b v c h w\", b=b, v=v)\n",
    "\n",
    "    @property\n",
    "    def d_out(self) -> int:\n",
    "        return self.cfg.d_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import torch\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "class TestBackboneResnet(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        \n",
    "        self.config = BackboneResnetCfg(\n",
    "            name=\"resnet\",\n",
    "            model=\"resnet50\",\n",
    "            num_layers=3,\n",
    "            use_first_pool=True,\n",
    "            d_out=10\n",
    "        )\n",
    "        self.model = BackboneResnet(self.config, d_in=3)\n",
    "        self.input_tensor = torch.randn(2, 1, 3, 224, 224)\n",
    "        self.context = BatchedViews(image=self.input_tensor)\n",
    "\n",
    "    def test_forward_pass(self):\n",
    "       \n",
    "        output = self.model(self.context)\n",
    "\n",
    "       \n",
    "        expected_shape = (2, 1, self.config.d_out, 224, 224)\n",
    "        self.assertEqual(output.shape, expected_shape, \"The output shape should match the expected shape\")\n",
    "\n",
    "     \n",
    "        self.assertEqual(output.dtype, torch.float32, \"Output tensor should have float32 data type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from dataclasses import dataclass\n",
    "from typing import TypeVar, Generic, Literal, TypedDict\n",
    "\n",
    "class BatchedViews(TypedDict):\n",
    "    image: torch.Tensor\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "class Backbone(nn.Module, Generic[T]):\n",
    "    def __init__(self, cfg: T):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def forward(self, context: BatchedViews) -> torch.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def d_out(self) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "@dataclass\n",
    "class BackboneResnetCfg:\n",
    "    name: Literal[\"resnet\"]\n",
    "    model: Literal[\"resnet18\", \"resnet34\", \"resnet50\", \"resnet101\", \"resnet152\", \"dino_resnet50\"]\n",
    "    num_layers: int\n",
    "    use_first_pool: bool\n",
    "    d_out: int\n",
    "\n",
    "class BackboneResnet(Backbone[BackboneResnetCfg]):\n",
    "    def __init__(self, cfg: BackboneResnetCfg, d_in: int):\n",
    "        super().__init__(cfg)\n",
    "        assert d_in == 3, \"Input depth should be 3 for RGB images\"\n",
    "        \n",
    "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n",
    "        self.model = getattr(torchvision.models, cfg.model)(pretrained=False, norm_layer=norm_layer)\n",
    "\n",
    "        self.projections = nn.ModuleDict()\n",
    "        previous_output_channels = self.model.conv1.out_channels  \n",
    "        self.projections['layer0'] = nn.Conv2d(previous_output_channels, cfg.d_out, 1)\n",
    "\n",
    "        layers = [self.model.layer1, self.model.layer2, self.model.layer3, self.model.layer4]\n",
    "        for i, layer_group in enumerate(layers[:cfg.num_layers - 1]):\n",
    "            \n",
    "            output_channels = layer_group[-1].conv3.out_channels\n",
    "            self.projections[f'layer{i+1}'] = nn.Conv2d(output_channels, cfg.d_out, 1)\n",
    "\n",
    "    def forward(self, context: BatchedViews) -> torch.Tensor:\n",
    "        x = context['image']\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        \n",
    "        features = [self.projections['layer0'](x)]\n",
    "        layers = [self.model.layer1, self.model.layer2, self.model.layer3, self.model.layer4]\n",
    "        for index in range(1, self.cfg.num_layers):\n",
    "            x = layers[index - 1](x)\n",
    "            features.append(self.projections[f'layer{index}'](x))\n",
    "        \n",
    "        h, w = context['image'].shape[2:]\n",
    "        features = [F.interpolate(feature, (h, w), mode='bilinear', align_corners=True) for feature in features]\n",
    "        output = torch.stack(features).sum(dim=0)\n",
    "        return output\n",
    "\n",
    "    @property\n",
    "    def d_out(self) -> int:\n",
    "        return self.cfg.d_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BackboneResnetCfg(\n",
    "name=\"resnet\",\n",
    "model=\"resnet50\", # Use resnet50 as an example model\n",
    "num_layers=4, # Number of layers to use in projections\n",
    "use_first_pool=True,\n",
    "d_out=10 # Number of output channels\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the BackboneResnet model\n",
    "model = BackboneResnet(config, d_in=3)\n",
    "\n",
    "# Create a dummy BatchedViews with random data\n",
    "dummy_data = torch.randn(3, 3, 224, 224)  # Simulate 3 batches, 3 color channels, 224x224 image size\n",
    "context = BatchedViews(image=dummy_data)\n",
    "\n",
    "# Perform the forward pass\n",
    "output = model(context)\n",
    "print(\"Output shape:\", output.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1000.0, 1001.0, 999.0])\n",
    "e_direct = torch.exp(x)\n",
    "sum_e_direct = torch.sum(e_direct)\n",
    "log_sum_exp_e = torch.log(sum_e_direct)\n",
    "\n",
    "# Using log-sum-exp trick\n",
    "max_x = torch.max(x)\n",
    "log_sum_exp_trick = max_x + torch.log(torch.sum(torch.exp(x - max_x)))\n",
    "\n",
    "print(\"Direct Log-Sum-Exp:\", log_sum_exp_e)\n",
    "print(\"Log-Sum-Exp Trick:\", log_sum_exp_trick)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Generic, Optional, TypeVar\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from utils_n.nn import avg_pool_nd, checkpoint, conv_nd, zero_module, GroupNorm32\n",
    "\n",
    "def normalization(channels, groups):\n",
    "    return nn.GroupNorm(groups, channels)\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "class Backbone(nn.Module, Generic[T]):\n",
    "    def __init__(self, cfg: T):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def d_out(self) -> int:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AttentionBlockConfig:\n",
    "    channels: int = 3\n",
    "    num_heads: int = 1\n",
    "    num_head_channels: int = -1\n",
    "    use_checkpoint: bool = False\n",
    "    use_new_attention_order: bool = False\n",
    "    use_self_attention: bool = False\n",
    "    num_groups: int = 32\n",
    "\n",
    "class AttentionBlock(Backbone[AttentionBlockConfig]):\n",
    "    def __init__(self, cfg: AttentionBlockConfig):\n",
    "        super().__init__(cfg)\n",
    "        if cfg.num_head_channels == -1:\n",
    "            self.num_heads = cfg.num_heads\n",
    "        else:\n",
    "            assert (\n",
    "                cfg.channels % cfg.num_head_channels == 0\n",
    "            ), f\"q,k,v channels {cfg.channels} is not divisible by num_head_channels {cfg.num_head_channels}\"\n",
    "            self.num_heads = cfg.channels // cfg.num_head_channels\n",
    "\n",
    "        self.norm = normalization(cfg.channels, cfg.num_groups)\n",
    "        self.qkv = conv_nd(1, cfg.channels, cfg.channels * 3, 1)\n",
    "\n",
    "        if cfg.use_self_attention:\n",
    "            self.attention = SelfAttention(cfg.channels, self.num_heads)\n",
    "        elif cfg.use_new_attention_order:\n",
    "            self.attention = QKVAttention(self.num_heads)\n",
    "        else:\n",
    "            self.attention = QKVAttentionLegacy(self.num_heads)\n",
    "\n",
    "        self.proj_out = zero_module(conv_nd(1, cfg.channels, cfg.channels, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return checkpoint(self._forward, (x,), self.parameters(), self.cfg.use_checkpoint)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        b, c, *spatial = x.shape\n",
    "        x = x.reshape(b, c, -1)\n",
    "        qkv = self.qkv(self.norm(x))\n",
    "        if self.cfg.use_self_attention:\n",
    "            qkv = qkv.permute(0, 2, 1)  # B, C, N to B, N, C for SelfAttention\n",
    "            h = self.attention(qkv)\n",
    "            h = h.permute(0, 2, 1)  # B, N, C back to B, C, N\n",
    "        else:\n",
    "            h = self.attention(qkv)\n",
    "        h = self.proj_out(h)\n",
    "        return (x + h).reshape(b, c, *spatial)\n",
    "\n",
    "class QKVAttentionLegacy(nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        bs, width, length = qkv.shape\n",
    "        assert width % (3 * self.n_heads) == 0\n",
    "        ch = width // (3 * self.n_heads)\n",
    "        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n",
    "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
    "        weight = torch.einsum(\n",
    "            \"bct,bcs->bts\", q * scale, k * scale\n",
    "        )\n",
    "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
    "        a = torch.einsum(\"bts,bcs->bct\", weight, v)\n",
    "        return a.reshape(bs, -1, length)\n",
    "\n",
    "class QKVAttention(nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        bs, width, length = qkv.shape\n",
    "        assert width % (3 * self.n_heads) == 0\n",
    "        ch = width // (3 * self.n_heads)\n",
    "        q, k, v = qkv.chunk(3, dim=1)\n",
    "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
    "        weight = torch.einsum(\n",
    "            \"bct,bcs->bts\",\n",
    "            (q * scale).view(bs * self.n_heads, ch, length),\n",
    "            (k * scale).view(bs * self.n_heads, ch, length),\n",
    "        )\n",
    "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
    "        a = torch.einsum(\n",
    "            \"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length)\n",
    "        )\n",
    "        return a.reshape(bs, -1, length)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.num_heads), qkv)\n",
    "\n",
    "        dots = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = dots.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        out = (attn @ v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = AttentionBlockConfig(\n",
    "    channels=64,\n",
    "    num_heads=4,\n",
    "    num_head_channels=16,\n",
    "    use_checkpoint=True,\n",
    "    use_self_attention=True,\n",
    "    num_groups=32\n",
    ")\n",
    "\n",
    "attention_block = AttentionBlock(cfg)\n",
    "\n",
    "# Create a dummy input tensor\n",
    "input_tensor = torch.randn(2, 64, 32, 32)  # Batch size of 2, 64 channels, 32x32 spatial dimensions\n",
    "\n",
    "# Forward pass\n",
    "output_tensor = attention_block(input_tensor)\n",
    "print(output_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"https://upload.wikimedia.org/wikipedia/commons/f/fe/Giant_Panda_in_Beijing_Zoo_1.JPG\" #@param {type:\"string\"}\n",
    "image_file = 'panda.jpg'\n",
    "!wget {file_path} -O {image_file}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms.functional import resize\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class MullerResizer(nn.Module):\n",
    "    \"\"\"Learned Laplacian resizer in PyTorch, fixed Gaussian blur for channel handling.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_resize_method=\"bilinear\",\n",
    "        antialias=False,\n",
    "        kernel_size=5,\n",
    "        stddev=1.0,\n",
    "        num_layers=2,\n",
    "        avg_pool=False,\n",
    "        dtype=torch.float32,\n",
    "        init_weights=None,\n",
    "        name=\"muller_resizer\",\n",
    "    ):\n",
    "        super(MullerResizer, self).__init__()\n",
    "        self.name = name\n",
    "        self.base_resize_method = base_resize_method\n",
    "        self.antialias = (\n",
    "            antialias  # Note: PyTorch does not support antialiasing in resizing.\n",
    "        )\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stddev = stddev\n",
    "        self.num_layers = num_layers\n",
    "        self.avg_pool = avg_pool\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.weights = nn.ParameterList()\n",
    "        self.biases = nn.ParameterList()\n",
    "        for layer in range(num_layers):\n",
    "            weight = nn.Parameter(\n",
    "                torch.zeros(1, dtype=dtype)\n",
    "                if init_weights is None\n",
    "                else torch.tensor([init_weights[2 * layer]], dtype=dtype)\n",
    "            )\n",
    "            bias = nn.Parameter(\n",
    "                torch.zeros(1, dtype=dtype)\n",
    "                if init_weights is None\n",
    "                else torch.tensor([init_weights[2 * layer + 1]], dtype=dtype)\n",
    "            )\n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "\n",
    "    def _base_resizer(self, inputs, target_size):\n",
    "        if self.avg_pool:\n",
    "            stride_h = inputs.shape[2] // target_size[0]\n",
    "            stride_w = inputs.shape[3] // target_size[1]\n",
    "            if stride_h > 1 and stride_w > 1:\n",
    "                inputs = F.avg_pool2d(\n",
    "                    inputs,\n",
    "                    kernel_size=(stride_h, stride_w),\n",
    "                    stride=(stride_h, stride_w),\n",
    "                )\n",
    "        return F.interpolate(\n",
    "            inputs, size=target_size, mode=self.base_resize_method, align_corners=False\n",
    "        )\n",
    "\n",
    "    def _gaussian_blur(self, inputs):\n",
    "        sigma = max(self.stddev, 0.5)  # Ensure sigma is not too small\n",
    "        radius = self.kernel_size // 2\n",
    "        kernel_size = 2 * radius + 1\n",
    "        x_coord = (\n",
    "            torch.arange(kernel_size, dtype=inputs.dtype, device=inputs.device) - radius\n",
    "        )\n",
    "        y_grid = x_coord.repeat(kernel_size, 1)\n",
    "        x_grid = x_coord.view(-1, 1).repeat(1, kernel_size)\n",
    "        xy_grid = torch.sqrt(x_grid**2 + y_grid**2)\n",
    "        kernel = torch.exp(-(xy_grid**2) / (2 * sigma**2))\n",
    "        kernel_sum = kernel.sum()\n",
    "        if kernel_sum.item() == 0:\n",
    "            kernel += 1e-10\n",
    "        kernel /= kernel_sum\n",
    "\n",
    "        kernel = kernel.view(1, 1, kernel_size, kernel_size).repeat(\n",
    "            inputs.shape[1], 1, 1, 1\n",
    "        )\n",
    "        blurred = F.conv2d(inputs, kernel, padding=radius, groups=inputs.shape[1])\n",
    "        return blurred\n",
    "\n",
    "    def forward(self, inputs, target_size):\n",
    "        inputs = inputs.to(dtype=self.dtype)\n",
    "        net = self._base_resizer(inputs, target_size)\n",
    "        for weight, bias in zip(self.weights, self.biases):\n",
    "            blurred = self._gaussian_blur(inputs)\n",
    "            residual_image = blurred - inputs\n",
    "            resized_residual = self._base_resizer(residual_image, target_size)\n",
    "            scaled_residual = weight * resized_residual + bias\n",
    "            # net += torch.tanh(scaled_residual.clamp(min=-3, max=3))  # Old. Clamping to prevent extreme values\n",
    "            net += F.relu(scaled_residual.clamp(min=0, max=1))\n",
    "            inputs = blurred\n",
    "        return net\n",
    "\n",
    "def test_muller_resizer(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.ToTensor()\n",
    "    tensor = transform(image)\n",
    "\n",
    "    resizer = MullerResizer(\n",
    "        base_resize_method=InterpolationMode.BILINEAR,\n",
    "        antialias=True,\n",
    "        kernel_size=5,\n",
    "        stddev=1.0,\n",
    "        num_layers=2,\n",
    "        avg_pool=False,\n",
    "        dtype=torch.float32,\n",
    "        init_weights=[1.892, -0.014, -11.295, 0.003],  # Example weights\n",
    "        name='muller_resizer'\n",
    "    )\n",
    "\n",
    "    output = resizer(tensor)\n",
    "    output_image = transforms.ToPILImage()(output.squeeze(0))  \n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(output_image)\n",
    "    plt.title('Resized Image')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "image_path = '/home/ozkan/works/n-smoe/panda.jpg'  \n",
    "\n",
    "\n",
    "test_muller_resizer(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "class MullerResizer_1(nn.Module):\n",
    "    \"\"\"Learned Laplacian resizer in PyTorch, fixed Gaussian blur for channel handling.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_size=(224, 224),\n",
    "        base_resize_method=\"bilinear\",\n",
    "        antialias=False,\n",
    "        kernel_size=5,\n",
    "        stddev=1.0,\n",
    "        num_layers=2,\n",
    "        avg_pool=False,\n",
    "        dtype=torch.float32,\n",
    "        init_weights=None,\n",
    "        name=\"muller_resizer\",\n",
    "    ):\n",
    "        super(MullerResizer, self).__init__()\n",
    "        self.target_size = target_size\n",
    "        self.base_resize_method = base_resize_method\n",
    "        self.antialias = antialias\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stddev = stddev\n",
    "        self.num_layers = num_layers\n",
    "        self.avg_pool = avg_pool\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.weights = nn.ParameterList()\n",
    "        self.biases = nn.ParameterList()\n",
    "        for layer in range(num_layers):\n",
    "            weight = nn.Parameter(\n",
    "                torch.zeros(1, dtype=dtype)\n",
    "                if init_weights is None\n",
    "                else torch.tensor([init_weights[2 * layer]], dtype=dtype)\n",
    "            )\n",
    "            bias = nn.Parameter(\n",
    "                torch.zeros(1, dtype=dtype)\n",
    "                if init_weights is None\n",
    "                else torch.tensor([init_weights[2 * layer + 1]], dtype=dtype)\n",
    "            )\n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "\n",
    "    def _base_resizer(self, inputs):\n",
    "        if self.avg_pool:\n",
    "            stride_h = inputs.shape[2] // self.target_size[0]\n",
    "            stride_w = inputs.shape[3] // self.target_size[1]\n",
    "            if stride_h > 1 and stride_w > 1:\n",
    "                inputs = F.avg_pool2d(\n",
    "                    inputs,\n",
    "                    kernel_size=(stride_h, stride_w),\n",
    "                    stride=(stride_h, stride_w),\n",
    "                )\n",
    "        return F.interpolate(\n",
    "            inputs, size=self.target_size, mode=self.base_resize_method, align_corners=False\n",
    "        )\n",
    "\n",
    "    def _gaussian_blur(self, inputs):\n",
    "        sigma = max(self.stddev, 0.5)  # Ensure sigma is not too small\n",
    "        radius = self.kernel_size // 2\n",
    "        kernel_size = 2 * radius + 1\n",
    "        x_coord = (\n",
    "            torch.arange(kernel_size, dtype=inputs.dtype, device=inputs.device) - radius\n",
    "        )\n",
    "        y_grid = x_coord.repeat(kernel_size, 1)\n",
    "        x_grid = x_coord.view(-1, 1).repeat(1, kernel_size)\n",
    "        xy_grid = torch.sqrt(x_grid**2 + y_grid**2)\n",
    "        kernel = torch.exp(-(xy_grid**2) / (2 * sigma**2))\n",
    "        kernel_sum = kernel.sum()\n",
    "        if kernel_sum.item() == 0:\n",
    "            kernel += 1e-10\n",
    "        kernel /= kernel_sum\n",
    "\n",
    "        kernel = kernel.view(1, 1, kernel_size, kernel_size).repeat(\n",
    "            inputs.shape[1], 1, 1, 1\n",
    "        )\n",
    "        blurred = F.conv2d(inputs, kernel, padding=radius, groups=inputs.shape[1])\n",
    "        return blurred\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.to(dtype=self.dtype)\n",
    "        net = self._base_resizer(inputs)\n",
    "        for weight, bias in zip(self.weights, self.biases):\n",
    "            blurred = self._gaussian_blur(inputs)\n",
    "            residual_image = blurred - inputs\n",
    "            resized_residual = self._base_resizer(residual_image)\n",
    "            scaled_residual = weight * resized_residual + bias\n",
    "            net += torch.tanh(scaled_residual)\n",
    "            inputs = blurred\n",
    "        return net\n",
    "\n",
    "\n",
    "\n",
    "class MullerResizer(nn.Module):\n",
    "    \"\"\"Learned Laplacian resizer in PyTorch, fixed Gaussian blur for channel handling.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_resize_method=\"bilinear\",\n",
    "        antialias=False,\n",
    "        kernel_size=5,\n",
    "        stddev=1.0,\n",
    "        num_layers=2,\n",
    "        avg_pool=False,\n",
    "        dtype=torch.float32,\n",
    "        init_weights=None,\n",
    "        name=\"muller_resizer\",\n",
    "    ):\n",
    "        super(MullerResizer, self).__init__()\n",
    "        self.name = name\n",
    "        self.base_resize_method = base_resize_method\n",
    "        self.antialias = (\n",
    "            antialias  # Note: PyTorch does not support antialiasing in resizing.\n",
    "        )\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stddev = stddev\n",
    "        self.num_layers = num_layers\n",
    "        self.avg_pool = avg_pool\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.weights = nn.ParameterList()\n",
    "        self.biases = nn.ParameterList()\n",
    "        for layer in range(num_layers):\n",
    "            weight = nn.Parameter(\n",
    "                torch.zeros(1, dtype=dtype)\n",
    "                if init_weights is None\n",
    "                else torch.tensor([init_weights[2 * layer]], dtype=dtype)\n",
    "            )\n",
    "            bias = nn.Parameter(\n",
    "                torch.zeros(1, dtype=dtype)\n",
    "                if init_weights is None\n",
    "                else torch.tensor([init_weights[2 * layer + 1]], dtype=dtype)\n",
    "            )\n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "\n",
    "    def _base_resizer(self, inputs, target_size):\n",
    "        if self.avg_pool:\n",
    "            stride_h = inputs.shape[2] // target_size[0]\n",
    "            stride_w = inputs.shape[3] // target_size[1]\n",
    "            if stride_h > 1 and stride_w > 1:\n",
    "                inputs = F.avg_pool2d(\n",
    "                    inputs,\n",
    "                    kernel_size=(stride_h, stride_w),\n",
    "                    stride=(stride_h, stride_w),\n",
    "                )\n",
    "        return F.interpolate(\n",
    "            inputs, size=target_size, mode=self.base_resize_method, align_corners=False\n",
    "        )\n",
    "\n",
    "    def _gaussian_blur(self, inputs):\n",
    "        sigma = max(self.stddev, 0.5)  # Ensure sigma is not too small\n",
    "        radius = self.kernel_size // 2\n",
    "        kernel_size = 2 * radius + 1\n",
    "        x_coord = (\n",
    "            torch.arange(kernel_size, dtype=inputs.dtype, device=inputs.device) - radius\n",
    "        )\n",
    "        y_grid = x_coord.repeat(kernel_size, 1)\n",
    "        x_grid = x_coord.view(-1, 1).repeat(1, kernel_size)\n",
    "        xy_grid = torch.sqrt(x_grid**2 + y_grid**2)\n",
    "        kernel = torch.exp(-(xy_grid**2) / (2 * sigma**2))\n",
    "        kernel_sum = kernel.sum()\n",
    "        if kernel_sum.item() == 0:\n",
    "            kernel += 1e-10\n",
    "        kernel /= kernel_sum\n",
    "\n",
    "        kernel = kernel.view(1, 1, kernel_size, kernel_size).repeat(\n",
    "            inputs.shape[1], 1, 1, 1\n",
    "        )\n",
    "        blurred = F.conv2d(inputs, kernel, padding=radius, groups=inputs.shape[1])\n",
    "        return blurred\n",
    "\n",
    "    def forward(self, inputs, target_size):\n",
    "        inputs = inputs.to(dtype=self.dtype)\n",
    "        net = self._base_resizer(inputs, target_size)\n",
    "        for weight, bias in zip(self.weights, self.biases):\n",
    "            blurred = self._gaussian_blur(inputs)\n",
    "            residual_image = blurred - inputs\n",
    "            resized_residual = self._base_resizer(residual_image, target_size)\n",
    "            scaled_residual = weight * resized_residual + bias\n",
    "            # net += torch.tanh(scaled_residual.clamp(min=-3, max=3))  # Old. Clamping to prevent extreme values\n",
    "            net += F.relu(scaled_residual.clamp(min=0, max=1))\n",
    "            inputs = blurred\n",
    "        return net\n",
    "\n",
    "\n",
    "def test_muller_resizer(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.ToTensor()\n",
    "    tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    resizer = MullerResizer(\n",
    "        base_resize_method=\"bilinear\",  # Use string directly\n",
    "        antialias=True,\n",
    "        kernel_size=5,\n",
    "        stddev=1.0,\n",
    "        num_layers=2,\n",
    "        avg_pool=False,\n",
    "        dtype=torch.float32,\n",
    "        init_weights=[1.892, -0.014, -11.295, 0.003],  \n",
    "        name='muller_resizer'\n",
    "    )\n",
    "\n",
    "    output = resizer(tensor,  target_size=(tensor.size(2), tensor.size(3)))\n",
    "    output_image = transforms.ToPILImage()(output.squeeze(0))  # Remove batch dimension\n",
    "\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(output_image)\n",
    "    plt.title('Resized Image')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "image_path = '/home/ozkan/works/n-smoe/notebooks/panda.jpg'\n",
    "test_muller_resizer(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import tensorflow as tf2\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class MullerResizer(tf2.keras.layers.Layer):\n",
    "  \"\"\"Learned Laplacian resizer in Keras Layer.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      target_size: Tuple[int, int] = (224, 224),\n",
    "      base_resize_method: tf2.image.ResizeMethod = tf2.image.ResizeMethod.BILINEAR,\n",
    "      antialias: bool = False,\n",
    "      kernel_size: int = 5,\n",
    "      stddev: float = 1.0,\n",
    "      num_layers: int = 2,\n",
    "      avg_pool: bool = False,\n",
    "      dtype: tf2.DType = tf2.float32,\n",
    "      init_weights: list = None,\n",
    "      name: str = 'muller_resizer',\n",
    "  ):\n",
    "    \"\"\"Applies a multilayer Laplacian filter on the input images.\n",
    "\n",
    "    Args:\n",
    "      target_size:  A tuple with target diemnsions (target_height,\n",
    "        target_width).\n",
    "      base_resize_method: Base image resizing method from\n",
    "        tf2.image.ResizeMethod.\n",
    "      antialias:  Whether to use antialias in resizer. Only tf2 resizer supports\n",
    "        this feature.\n",
    "      kernel_size: Size of the Gaussian filter.\n",
    "      stddev: An optional float stddev, if provided will be directly used\n",
    "        otherwise is determined using kernel_size.\n",
    "      num_layers: Specifies the number of Laplacian layers.\n",
    "      avg_pool: Whether to apply an average pooling before the base image\n",
    "        resizer. The average pooling is only effective when input is downsized.\n",
    "      dtype: Represents the data type used to cast input and the resizer\n",
    "        weights. It should be consistent with the training / evaluation\n",
    "        framework.\n",
    "      init_weights: Wether to initialize the weights of the resizer.\n",
    "      name: name scope of this layer.\n",
    "    \"\"\"\n",
    "    super().__init__(name=name)\n",
    "\n",
    "    self._target_size = target_size\n",
    "    self._base_resize_method = base_resize_method\n",
    "    self._antialias = antialias\n",
    "    self._kernel_size = kernel_size\n",
    "    self._stddev = stddev\n",
    "    self._num_layers = num_layers\n",
    "    self._avg_pool = avg_pool\n",
    "    self._dtype = dtype\n",
    "    self._init_weights = init_weights\n",
    "\n",
    "  def build(self, input_shape: tf2.TensorShape) -> None:\n",
    "    self._weights = []\n",
    "    self._biases = []\n",
    "    for layer in range(1, self._num_layers + 1):\n",
    "      weight = self.add_weight(\n",
    "          name='weight_' + str(layer),\n",
    "          shape=[],\n",
    "          dtype=self._dtype,\n",
    "          initializer=tf2.keras.initializers.Constant(self._init_weights[2*layer-2])\n",
    "          if self._init_weights else tf2.keras.initializers.zeros(),\n",
    "      )\n",
    "      bias = self.add_weight(\n",
    "          name='bias_' + str(layer),\n",
    "          shape=[],\n",
    "          dtype=self._dtype,\n",
    "          initializer=tf2.keras.initializers.Constant(self._init_weights[2*layer-1])\n",
    "          if self._init_weights else tf2.keras.initializers.zeros(),\n",
    "      )\n",
    "      self._weights.append(weight)\n",
    "      self._biases.append(bias)\n",
    "\n",
    "    super().build(input_shape)\n",
    "\n",
    "  def _base_resizer(self, inputs: tf2.Tensor) -> tf2.Tensor:\n",
    "    \"\"\"Base resizer function for muller.\"\"\"\n",
    "    stride = [\n",
    "        1,\n",
    "        inputs.get_shape().as_list()[1] // self._target_size[0],\n",
    "        inputs.get_shape().as_list()[2] // self._target_size[1],\n",
    "        1\n",
    "    ]\n",
    "    if self._avg_pool and stride[1] > 1 and stride[2] > 1:\n",
    "      pooling_shape = [1, stride[1], stride[2], 1]\n",
    "      inputs = tf2.nn.avg_pool(inputs, pooling_shape, stride, padding='SAME')\n",
    "\n",
    "    return tf2.cast(\n",
    "        tf2.image.resize(\n",
    "            inputs,\n",
    "            self._target_size,\n",
    "            method=self._base_resize_method,\n",
    "            antialias=self._antialias),\n",
    "        self._dtype)\n",
    "\n",
    "  def _gaussian_blur(self, inputs: tf2.Tensor) -> tf2.Tensor:\n",
    "    \"\"\"Gaussian blur function for muller.\"\"\"\n",
    "    stddev = tf2.cast(self._stddev, self._dtype)\n",
    "    size = self._kernel_size\n",
    "    radius = size // 2\n",
    "    x = tf2.cast(tf2.range(-radius, radius + 1), self._dtype)\n",
    "    blur_filter = tf2.exp(-tf2.pow(x, 2.0) / (2.0 * tf2.pow(stddev, 2.0)))\n",
    "    blur_filter /= tf2.reduce_sum(blur_filter)\n",
    "    # cast to dtype\n",
    "    blur_v = tf2.reshape(blur_filter, [size, 1, 1, 1])\n",
    "    blur_h = tf2.reshape(blur_filter, [1, size, 1, 1])\n",
    "    num_channels = inputs.get_shape()[-1]\n",
    "    blur_h = tf2.tile(blur_h, [1, 1, num_channels, 1])\n",
    "    blur_v = tf2.tile(blur_v, [1, 1, num_channels, 1])\n",
    "    blurred = tf2.nn.depthwise_conv2d(\n",
    "        inputs, blur_h, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    blurred = tf2.nn.depthwise_conv2d(\n",
    "        blurred, blur_v, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return blurred\n",
    "\n",
    "  def call(\n",
    "      self,\n",
    "      inputs: tf2.Tensor,\n",
    "  ) -> tf2.Tensor:\n",
    "    inputs.get_shape().assert_has_rank(4)\n",
    "\n",
    "    if inputs.dtype != self._dtype:\n",
    "      inputs = tf2.cast(inputs, self._dtype)\n",
    "\n",
    "    # Creates the base resized image.\n",
    "    net = self._base_resizer(inputs)\n",
    "\n",
    "    # Multi Laplacian resizer.\n",
    "    for weight, bias in zip(self._weights, self._biases):\n",
    "      # Gaussian blur.\n",
    "      blurred = self._gaussian_blur(inputs)\n",
    "      # Residual image\n",
    "      residual_image = blurred - inputs\n",
    "      # Resize residual image.\n",
    "      resized_residual = self._base_resizer(residual_image)\n",
    "      # Add the residual to the input image.\n",
    "      net = net + tf2.nn.tanh(weight * resized_residual + bias)\n",
    "      inputs = blurred\n",
    "    return net\n",
    "\n",
    "     \n",
    "#@title Define model configs\n",
    "\n",
    "_CONFIGS = {\n",
    "  \"target_size\": (512, 512),\n",
    "  \"base_resize_method\": \"bilinear\",\n",
    "  \"antialias\": False,\n",
    "  \"kernel_size\": 5,\n",
    "  \"stddev\": 1.0,\n",
    "  \"num_layers\": 2,\n",
    "  \"avg_pool\": False,\n",
    "  \"dtype\": tf2.float32,\n",
    "  \"name\": 'muller_resizer',\n",
    "  \"init_weights\": [1.892, -0.014, -11.295, 0.003],  # pre-trained weights for ResNet-50\n",
    "}\n",
    "\n",
    "model = MullerResizer(**_CONFIGS)\n",
    "\n",
    "def inference(image_path, model):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = np.asarray(image) / 255.\n",
    "    image = tf2.expand_dims(image, axis=0)\n",
    "    preds = model(image)\n",
    "    preds = np.array(preds[0], np.float32)\n",
    "    return np.array(np.clip(preds, 0.0, 1.0))\n",
    "image_file=\"/home/ozkan/works/n-smoe/notebooks/panda.jpg\"\n",
    "pred_image = inference(image_path=image_file, model=model)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n",
    "input_image = np.asarray(Image.open(image_file).convert(\"RGB\")) / 255.0\n",
    "axes[0].imshow(input_image, aspect=\"auto\")\n",
    "axes[1].imshow(pred_image, aspect=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "class MullerResizerTorch(nn.Module):\n",
    "    def __init__(self, target_size=(224, 224), base_resize_method='bilinear', kernel_size=5, stddev=1.0, num_layers=2, avg_pool=False, init_weights=None, dtype=torch.float32):\n",
    "        super(MullerResizerTorch, self).__init__()\n",
    "        self.target_size = target_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stddev = stddev\n",
    "        self.num_layers = num_layers\n",
    "        self.avg_pool = avg_pool\n",
    "        self.dtype = dtype\n",
    "\n",
    "        interpolation_methods = {\n",
    "            'bilinear': 'bilinear',\n",
    "            'nearest': 'nearest',\n",
    "            'bicubic': 'bicubic'\n",
    "        }\n",
    "        self.interpolation_method = interpolation_methods.get(base_resize_method, 'bilinear')\n",
    "\n",
    "        self.weights = nn.ParameterList()\n",
    "        self.biases = nn.ParameterList()\n",
    "        if init_weights is not None:\n",
    "            for i in range(num_layers):\n",
    "                self.weights.append(nn.Parameter(torch.tensor(init_weights[2 * i], dtype=dtype)))\n",
    "                self.biases.append(nn.Parameter(torch.tensor(init_weights[2 * i + 1], dtype=dtype)))\n",
    "        else:\n",
    "            for _ in range(num_layers):\n",
    "                self.weights.append(nn.Parameter(torch.zeros((), dtype=dtype)))\n",
    "                self.biases.append(nn.Parameter(torch.zeros((), dtype=dtype)))\n",
    "\n",
    "        self.gaussian_kernel = self.create_gaussian_kernel(kernel_size, stddev)\n",
    "\n",
    "    def create_gaussian_kernel(self, kernel_size, stddev):\n",
    "        t = torch.arange(kernel_size, dtype=self.dtype) - (kernel_size - 1) / 2\n",
    "        gaussian_kernel = torch.exp(-t.pow(2) / (2 * stddev**2))\n",
    "        gaussian_kernel /= gaussian_kernel.sum()\n",
    "        gaussian_kernel = gaussian_kernel.view(1, 1, kernel_size, 1) * gaussian_kernel.view(1, 1, 1, kernel_size)\n",
    "        gaussian_kernel = gaussian_kernel.repeat(3, 1, 1, 1)\n",
    "        return gaussian_kernel\n",
    "\n",
    "    def _apply_gaussian_blur(self, x):\n",
    "        padding = self.kernel_size // 2\n",
    "        x = F.pad(x, (padding, padding, padding, padding), mode='reflect')\n",
    "        return F.conv2d(x, self.gaussian_kernel, groups=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(dtype=self.dtype)\n",
    "        if self.avg_pool:\n",
    "            x = F.avg_pool2d(x, kernel_size=2, stride=2)\n",
    "        net = F.interpolate(x, size=self.target_size, mode=self.interpolation_method, align_corners=False)\n",
    "        \n",
    "        for weight, bias in zip(self.weights, self.biases):\n",
    "            blurred = self._apply_gaussian_blur(x)\n",
    "            residual = blurred - x\n",
    "            resized_residual = F.interpolate(residual, size=self.target_size, mode=self.interpolation_method, align_corners=False)\n",
    "            net = net + torch.tanh(weight * resized_residual + bias)\n",
    "            x = blurred\n",
    "\n",
    "        return net\n",
    "\n",
    "class MullerResizerTF(tf.keras.layers.Layer):\n",
    "    def __init__(self, target_size=(224, 224), base_resize_method=tf.image.ResizeMethod.BILINEAR, antialias=False, kernel_size=5, stddev=1.0, num_layers=2, avg_pool=False, dtype=tf.float32, init_weights=None):\n",
    "        super().__init__()\n",
    "        self._target_size = target_size\n",
    "        self._base_resize_method = base_resize_method\n",
    "        self._antialias = antialias\n",
    "        self._kernel_size = kernel_size\n",
    "        self._stddev = stddev\n",
    "        self._num_layers = num_layers\n",
    "        self._avg_pool = avg_pool\n",
    "        self._dtype = dtype\n",
    "        self._init_weights = init_weights\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self._weights = []\n",
    "        self._biases = []\n",
    "        for layer in range(1, self._num_layers + 1):\n",
    "            weight = self.add_weight(name='weight_' + str(layer), shape=[], dtype=self._dtype, initializer=tf.keras.initializers.Constant(self._init_weights[2*layer-2]) if self._init_weights else tf.keras.initializers.zeros())\n",
    "            bias = self.add_weight(name='bias_' + str(layer), shape=[], dtype=self._dtype, initializer=tf.keras.initializers.Constant(self._init_weights[2*layer-1]) if self._init_weights else tf.keras.initializers.zeros())\n",
    "            self._weights.append(weight)\n",
    "            self._biases.append(bias)\n",
    "\n",
    "    def _base_resizer(self, inputs):\n",
    "        stride = [1, inputs.get_shape().as_list()[1] // self._target_size[0], inputs.get_shape().as_list()[2] // self._target_size[1], 1]\n",
    "        if self._avg_pool and stride[1] > 1 and stride[2] > 1:\n",
    "            pooling_shape = [1, stride[1], stride[2], 1]\n",
    "            inputs = tf.nn.avg_pool(inputs, pooling_shape, stride, padding='SAME')\n",
    "        return tf.cast(tf.image.resize(inputs, self._target_size, method=self._base_resize_method, antialias=self._antialias), self._dtype)\n",
    "\n",
    "    def _gaussian_blur(self, inputs):\n",
    "        stddev = tf.cast(self._stddev, self._dtype)\n",
    "        size = self._kernel_size\n",
    "        radius = size // 2\n",
    "        x = tf.cast(tf.range(-radius, radius + 1), self._dtype)\n",
    "        blur_filter = tf.exp(-tf.pow(x, 2.0) / (2.0 * tf.pow(stddev, 2.0)))\n",
    "        blur_filter /= tf.reduce_sum(blur_filter)\n",
    "        blur_v = tf.reshape(blur_filter, [size, 1, 1, 1])\n",
    "        blur_h = tf.reshape(blur_filter, [1, size, 1, 1])\n",
    "        num_channels = inputs.get_shape()[-1]\n",
    "        blur_h = tf.tile(blur_h, [1, 1, num_channels, 1])\n",
    "        blur_v = tf.tile(blur_v, [1, 1, num_channels, 1])\n",
    "        blurred = tf.nn.depthwise_conv2d(inputs, blur_h, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        blurred = tf.nn.depthwise_conv2d(blurred, blur_v, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        return blurred\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs.get_shape().assert_has_rank(4)\n",
    "        if inputs.dtype != self._dtype:\n",
    "            inputs = tf.cast(inputs, self._dtype)\n",
    "        net = self._base_resizer(inputs)\n",
    "        for weight, bias in zip(self._weights, self._biases):\n",
    "            blurred = self._gaussian_blur(inputs)\n",
    "            residual_image = blurred - inputs\n",
    "            resized_residual = self._base_resizer(residual_image)\n",
    "            net = net + tf.nn.tanh(weight * resized_residual + bias)\n",
    "            inputs = blurred\n",
    "        return net\n",
    "\n",
    "image_path = '/home/ozkan/works/n-smoe/notebooks/panda.jpg'\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "transform = transforms.ToTensor()\n",
    "tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "def default_resizer(inputs, target_size):\n",
    "    return F.interpolate(inputs, size=target_size, mode='bilinear', align_corners=False, antialias=True)\n",
    "\n",
    "output_default = default_resizer(tensor, target_size=(tensor.size(2), tensor.size(3)))\n",
    "\n",
    "resizer_torch = MullerResizerTorch(target_size=(tensor.size(2), tensor.size(3)),base_resize_method ='bilinear',  kernel_size=5, \n",
    "                                   stddev=1.0, num_layers=2, avg_pool=False, dtype=torch.float32, \n",
    "                                   init_weights=[1.9280042333186972, -91.42857142857143, 9.99854848098652489, 333.3333333333333]*5)\n",
    "\n",
    "if tensor.dim() == 3:\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "\n",
    "resizer_torch = resizer_torch.eval()\n",
    "with torch.no_grad():\n",
    "    output_torch = resizer_torch(tensor)\n",
    "\n",
    "_CONFIGS = {\n",
    "    \"target_size\": (image.height, image.width),\n",
    "    \"base_resize_method\": tf.image.ResizeMethod.BILINEAR,\n",
    "    \"antialias\": False,\n",
    "    \"kernel_size\": 5,\n",
    "    \"stddev\": 1.0,\n",
    "    \"num_layers\": 2,\n",
    "    \"avg_pool\": False,\n",
    "    \"dtype\": tf.float32,\n",
    "    \"init_weights\": [1.892, -0.014, -11.295, 0.003]\n",
    "}\n",
    "\n",
    "model_tf = MullerResizerTF(**_CONFIGS)\n",
    "\n",
    "def inference_tf(image_path, model):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = np.asarray(image) / 255.\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    preds = model(image)\n",
    "    preds = np.array(preds[0], np.float32)\n",
    "    return np.array(np.clip(preds, 0.0, 1.0))\n",
    "\n",
    "output_image_tf = inference_tf(image_path, model_tf)\n",
    "\n",
    "output_default = transforms.ToPILImage()(output_default.squeeze(0))\n",
    "output_image_tf = transforms.ToPILImage()(output_image_tf)\n",
    "output_image_torch = transforms.ToPILImage()(output_torch.squeeze(0))\n",
    "\n",
    "output_tf_np = np.array(output_image_tf).astype(np.float32) / 255.0\n",
    "output_torch_np = np.array(output_image_torch).astype(np.float32) / 255.0\n",
    "difference = np.abs(output_tf_np - output_torch_np)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(60, 20))\n",
    "\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(output_default)\n",
    "axes[1].set_title('Default Resizer')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(output_image_tf)\n",
    "axes[2].set_title('TF Muller Resizer')\n",
    "axes[2].axis('off')\n",
    "\n",
    "axes[3].imshow(output_image_torch)\n",
    "axes[3].set_title('PT Muller Resizer')\n",
    "axes[3].axis('off')\n",
    "\n",
    "axes[4].imshow(difference) \n",
    "axes[4].set_title('Difference')\n",
    "axes[4].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate the same random image\n",
    "np.random.seed(42)\n",
    "random_image = np.random.rand(256, 256, 3).astype(np.float32)\n",
    "\n",
    "# TensorFlow resizing\n",
    "def resize_tf(image):\n",
    "    tf_image = tf.convert_to_tensor(image)\n",
    "    tf_resized_image = tf.image.resize(tf_image, [128, 128], method='bilinear')\n",
    "    return tf_resized_image.numpy()\n",
    "\n",
    "# PyTorch resizing using torch.nn.functional.interpolate\n",
    "def resize_pytorch(image):\n",
    "    image_tensor = torch.tensor(image.transpose(2, 0, 1)).unsqueeze(0)\n",
    "    resized_image_tensor = F.interpolate(image_tensor, size=(128, 128), mode='bilinear', align_corners=False)\n",
    "    resized_image_np = resized_image_tensor.squeeze(0).numpy().transpose(1, 2, 0)\n",
    "    return resized_image_np\n",
    "\n",
    "# Perform resizing\n",
    "resized_image_tf = resize_tf(random_image)\n",
    "resized_image_pytorch = resize_pytorch(random_image)\n",
    "\n",
    "# Compare the results using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(resized_image_tf.flatten(), resized_image_pytorch.flatten())\n",
    "\n",
    "print(f\"Mean Squared Error between TensorFlow and PyTorch resized images: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# TensorFlow setup\n",
    "input_channels = 3\n",
    "kernel_size = 3\n",
    "input_tensor_tf = tf.random.normal([1, 32, 32, input_channels])\n",
    "depthwise_conv_tf = tf.keras.layers.DepthwiseConv2D(kernel_size=kernel_size, padding='same')\n",
    "depthwise_conv_tf.build(input_tensor_tf.shape)\n",
    "depthwise_kernel_tf = tf.random.normal(depthwise_conv_tf.weights[0].shape)\n",
    "depthwise_conv_tf.set_weights([depthwise_kernel_tf, tf.zeros(depthwise_conv_tf.weights[1].shape)])\n",
    "\n",
    "# PyTorch setup\n",
    "depthwise_conv_pt = nn.Conv2d(in_channels=input_channels,\n",
    "                              out_channels=input_channels,\n",
    "                              kernel_size=kernel_size,\n",
    "                              padding='same',\n",
    "                              groups=input_channels)\n",
    "\n",
    "# Ensure same initialization\n",
    "with torch.no_grad():\n",
    "    depthwise_conv_pt.weight.copy_(torch.tensor(depthwise_kernel_tf.numpy().transpose(2, 3, 0, 1)))\n",
    "    depthwise_conv_pt.bias.copy_(torch.zeros(depthwise_conv_pt.bias.shape))\n",
    "\n",
    "input_tensor_pt = torch.tensor(input_tensor_tf.numpy().transpose(0, 3, 1, 2))\n",
    "\n",
    "# Apply convolutions\n",
    "output_tensor_tf = depthwise_conv_tf(input_tensor_tf)\n",
    "output_tensor_pt = depthwise_conv_pt(input_tensor_pt)\n",
    "\n",
    "# Convert outputs to numpy arrays\n",
    "output_tensor_pt_np = output_tensor_pt.detach().numpy().transpose(0, 2, 3, 1)\n",
    "output_tensor_tf_np = output_tensor_tf.numpy()\n",
    "\n",
    "# Calculate and print MSE\n",
    "mse = np.mean((output_tensor_tf_np - output_tensor_pt_np) ** 2)\n",
    "print(\"Mean Squared Error between TensorFlow and PyTorch outputs:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def generate_complexity_map(image):\n",
    "    # Example using Sobel operator to estimate edge density\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    magnitude = np.sqrt(sobelx**2 + sobely**2)\n",
    "    return cv2.normalize(magnitude, None, 0, 1, norm_type=cv2.NORM_MINMAX)\n",
    "\n",
    "def adaptive_patch_extraction(image, complexity_map, min_size=32, max_size=128):\n",
    "    height, width = image.shape[:2]\n",
    "    patches = []\n",
    "    i = 0\n",
    "    while i < height:\n",
    "        j = 0\n",
    "        while j < width:\n",
    "            complexity = complexity_map[i, j]\n",
    "            # Determine patch size by complexity: smaller patch for higher complexity\n",
    "            patch_size = int(max_size - (complexity * (max_size - min_size)))\n",
    "            patch_size = min(patch_size, height - i, width - j)  # Adjust if near borders\n",
    "\n",
    "            # Extract patch\n",
    "            patch = image[i:i+patch_size, j:j+patch_size]\n",
    "            patches.append(patch)\n",
    "\n",
    "            # Calculate overlap\n",
    "            overlap = int(0.2 * patch_size)  # Example: 20% overlap\n",
    "            j += patch_size - overlap\n",
    "        i += patch_size - overlap\n",
    "\n",
    "    return patches\n",
    "image_path = '/home/ozkan/works/n-smoe/notebooks/panda.jpg'\n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "complexity_map = generate_complexity_map(image)\n",
    "patches = adaptive_patch_extraction(image, complexity_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_complexity_map(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    magnitude = np.sqrt(sobelx**2 + sobely**2)\n",
    "    return cv2.normalize(magnitude, None, 0, 1, norm_type=cv2.NORM_MINMAX)\n",
    "\n",
    "def adaptive_patch_extraction(image, complexity_map, min_size=32, max_size=128):\n",
    "    height, width = image.shape[:2]\n",
    "    patches = []\n",
    "    i = 0\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # Show the original image\n",
    "    plt.title('Adaptive Patch Extraction')\n",
    "    ax = plt.gca()\n",
    "\n",
    "    while i < height:\n",
    "        j = 0\n",
    "        while j < width:\n",
    "            complexity = complexity_map[i, j]\n",
    "            patch_size = int(max_size - (complexity * (max_size - min_size)))\n",
    "            patch_size = min(patch_size, height - i, width - j)\n",
    "\n",
    "            # Draw rectangle around the patch\n",
    "            rect = plt.Rectangle((j, i), patch_size, patch_size, edgecolor='red', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            patch = image[i:i+patch_size, j:j+patch_size]\n",
    "            patches.append(patch)\n",
    "\n",
    "            overlap = int(0.2 * patch_size)  # 20% overlap\n",
    "            j += patch_size - overlap\n",
    "        i += patch_size - overlap\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    return patches\n",
    "\n",
    "# Load and process the image\n",
    "image_path = '/home/ozkan/works/n-smoe/notebooks/panda.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "complexity_map = generate_complexity_map(image)\n",
    "patches = adaptive_patch_extraction(image, complexity_map)\n",
    "\n",
    "# Optionally, show the complexity map and some patches\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(complexity_map, cmap='gray')\n",
    "plt.title('Complexity Map')\n",
    "plt.axis('off')\n",
    "\n",
    "# Display first few patches\n",
    "plt.subplot(1, 2, 2)\n",
    "for n, patch in enumerate(patches[:4]):  # Show the first 4 patches\n",
    "    plt.subplot(2, 2, n+1)\n",
    "    plt.imshow(cv2.cvtColor(patch, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f'Patch {n+1}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "image_path = \"/mnt/e/Medical/mri/data_for_train/brain2/no_tumor/image(145).jpg\"\n",
    "image = Image.open(image_path)\n",
    "image = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "height, width = image.shape\n",
    "block_size = 16\n",
    "\n",
    "num_blocks_x = width // block_size\n",
    "num_blocks_y = height // block_size\n",
    "\n",
    "adjusted_width = num_blocks_x * block_size\n",
    "adjusted_height = num_blocks_y * block_size\n",
    "image = image[:adjusted_height, :adjusted_width]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "num_layers = 4\n",
    "offset = 5\n",
    "\n",
    "for i in range(num_layers):\n",
    "    blurred_image = cv2.GaussianBlur(image, (11, 11), 5 + 2 * i)\n",
    "    ax.imshow(blurred_image, cmap='gray', alpha=0.5, extent=(i * offset, adjusted_width + i * offset, adjusted_height + i * offset, i * offset))\n",
    "\n",
    "ax.imshow(image, cmap='gray', alpha=1.0, extent=(num_layers * offset, adjusted_width + num_layers * offset, adjusted_height + num_layers * offset, num_layers * offset))\n",
    "\n",
    "border_color = 'cyan'\n",
    "highlight_color = 'limegreen'\n",
    "for i in range(0, adjusted_width + 1, block_size):\n",
    "    ax.axvline(i + num_layers * offset, color=border_color, linewidth=0.5)\n",
    "for j in range(0, adjusted_height + 1, block_size):\n",
    "    ax.axhline(j + num_layers * offset, color=border_color, linewidth=0.5)\n",
    "\n",
    "cerebellum_x_start = (num_blocks_x // 2) * block_size + num_layers * offset\n",
    "cerebellum_x_end = cerebellum_x_start + 3 * block_size\n",
    "cerebellum_y_start = (num_blocks_y - 4) * block_size + num_layers * offset\n",
    "cerebellum_y_end = cerebellum_y_start + 3 * block_size\n",
    "\n",
    "for i in range(cerebellum_x_start, cerebellum_x_end, block_size):\n",
    "    for j in range(cerebellum_y_start, cerebellum_y_end, block_size):\n",
    "        rect = patches.Rectangle((i, j), block_size, block_size, linewidth=2, edgecolor=highlight_color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "fig.subplots_adjust(left=0.15)\n",
    "\n",
    "ax.spines['left'].set_color('red')\n",
    "ax.spines['left'].set_linewidth(2)\n",
    "ax.yaxis.label.set_color('red')\n",
    "ax.tick_params(axis='y', colors='red')\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.spines['right'].set_color('blue')\n",
    "ax2.spines['right'].set_linewidth(2)\n",
    "ax2.yaxis.label.set_color('blue')\n",
    "ax2.tick_params(axis='y', colors='blue')\n",
    "\n",
    "ax.set_xticks(np.arange(0, adjusted_width + 1, block_size * 2))\n",
    "ax.set_xticklabels(np.arange(0, adjusted_width + 1, block_size * 2))\n",
    "ax.set_yticks(np.arange(0, adjusted_height + 1, block_size * 2))\n",
    "ax.set_yticklabels(np.arange(0, adjusted_height + 1, block_size * 2))\n",
    "\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matlab.engine\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class MullerResizerTorch(nn.Module):\n",
    "    def __init__(self, target_size=(224, 224), base_resize_method='bilinear', kernel_size=5, stddev=1.0, num_layers=2, avg_pool=False, init_weights=None, dtype=torch.float32):\n",
    "        super(MullerResizerTorch, self).__init__()\n",
    "        self.target_size = target_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stddev = stddev\n",
    "        self.num_layers = num_layers\n",
    "        self.avg_pool = avg_pool\n",
    "        self.dtype = dtype\n",
    "\n",
    "        interpolation_methods = {\n",
    "            'bilinear': 'bilinear',\n",
    "            'nearest': 'nearest',\n",
    "            'bicubic': 'bicubic'\n",
    "        }\n",
    "        self.interpolation_method = interpolation_methods.get(base_resize_method, 'bilinear')\n",
    "\n",
    "        self.weights = nn.ParameterList()\n",
    "        self.biases = nn.ParameterList()\n",
    "        if init_weights is not None:\n",
    "            for i in range(num_layers):\n",
    "                self.weights.append(nn.Parameter(torch.tensor(init_weights[2 * i], dtype=dtype)))\n",
    "                self.biases.append(nn.Parameter(torch.tensor(init_weights[2 * i + 1], dtype=dtype)))\n",
    "        else:\n",
    "            for _ in range(num_layers):\n",
    "                self.weights.append(nn.Parameter(torch.zeros((), dtype=dtype)))\n",
    "                self.biases.append(nn.Parameter(torch.zeros((), dtype=dtype)))\n",
    "\n",
    "        self.gaussian_kernel = self.create_gaussian_kernel(kernel_size, stddev)\n",
    "\n",
    "    def create_gaussian_kernel(self, kernel_size, stddev):\n",
    "        t = torch.arange(kernel_size, dtype=self.dtype) - (kernel_size - 1) / 2\n",
    "        gaussian_kernel = torch.exp(-t.pow(2) / (2 * stddev**2))\n",
    "        gaussian_kernel /= gaussian_kernel.sum()\n",
    "        gaussian_kernel = gaussian_kernel.view(1, 1, kernel_size, 1) * gaussian_kernel.view(1, 1, 1, kernel_size)\n",
    "        gaussian_kernel = gaussian_kernel.repeat(3, 1, 1, 1)\n",
    "        return gaussian_kernel\n",
    "\n",
    "    def _apply_gaussian_blur(self, x):\n",
    "        padding = self.kernel_size // 2\n",
    "        x = F.pad(x, (padding, padding, padding, padding), mode='reflect')\n",
    "        return F.conv2d(x, self.gaussian_kernel, groups=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(dtype=self.dtype)\n",
    "        if self.avg_pool:\n",
    "            x = F.avg_pool2d(x, kernel_size=2, stride=2)\n",
    "        net = F.interpolate(x, size=self.target_size, mode=self.interpolation_method, align_corners=False)\n",
    "        \n",
    "        for weight, bias in zip(self.weights, self.biases):\n",
    "            blurred = self._apply_gaussian_blur(x)\n",
    "            residual = blurred - x\n",
    "            resized_residual = F.interpolate(residual, size=self.target_size, mode=self.interpolation_method, align_corners=False)\n",
    "            net = net + torch.tanh(weight * resized_residual + bias)\n",
    "            x = blurred\n",
    "\n",
    "        return net\n",
    "\n",
    "class MullerResizerTF(tf.keras.layers.Layer):\n",
    "    def __init__(self, target_size=(224, 224), base_resize_method=tf.image.ResizeMethod.BILINEAR, antialias=False, kernel_size=5, stddev=1.0, num_layers=2, avg_pool=False, dtype=tf.float32, init_weights=None):\n",
    "        super().__init__()\n",
    "        self._target_size = target_size\n",
    "        self._base_resize_method = base_resize_method\n",
    "        self._antialias = antialias\n",
    "        self._kernel_size = kernel_size\n",
    "        self._stddev = stddev\n",
    "        self._num_layers = num_layers\n",
    "        self._avg_pool = avg_pool\n",
    "        self._dtype = dtype\n",
    "        self._init_weights = init_weights\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self._weights = []\n",
    "        self._biases = []\n",
    "        for layer in range(1, self._num_layers + 1):\n",
    "            weight = self.add_weight(name='weight_' + str(layer), shape=[], dtype=self._dtype, initializer=tf.keras.initializers.Constant(self._init_weights[2*layer-2]) if self._init_weights else tf.keras.initializers.zeros())\n",
    "            bias = self.add_weight(name='bias_' + str(layer), shape=[], dtype=self._dtype, initializer=tf.keras.initializers.Constant(self._init_weights[2*layer-1]) if self._init_weights else tf.keras.initializers.zeros())\n",
    "            self._weights.append(weight)\n",
    "            self._biases.append(bias)\n",
    "\n",
    "    def _base_resizer(self, inputs):\n",
    "        stride = [1, inputs.get_shape().as_list()[1] // self._target_size[0], inputs.get_shape().as_list()[2] // self._target_size[1], 1]\n",
    "        if self._avg_pool and stride[1] > 1 and stride[2] > 1:\n",
    "            pooling_shape = [1, stride[1], stride[2], 1]\n",
    "            inputs = tf.nn.avg_pool(inputs, pooling_shape, stride, padding='SAME')\n",
    "        return tf.cast(tf.image.resize(inputs, self._target_size, method=self._base_resize_method, antialias=self._antialias), self._dtype)\n",
    "\n",
    "    def _gaussian_blur(self, inputs):\n",
    "        stddev = tf.cast(self._stddev, self._dtype)\n",
    "        size = self._kernel_size\n",
    "        radius = size // 2\n",
    "        x = tf.cast(tf.range(-radius, radius + 1), self._dtype)\n",
    "        blur_filter = tf.exp(-tf.pow(x, 2.0) / (2.0 * tf.pow(stddev, 2.0)))\n",
    "        blur_filter /= tf.reduce_sum(blur_filter)\n",
    "        blur_v = tf.reshape(blur_filter, [size, 1, 1, 1])\n",
    "        blur_h = tf.reshape(blur_filter, [1, size, 1, 1])\n",
    "        num_channels = inputs.get_shape()[-1]\n",
    "        blur_h = tf.tile(blur_h, [1, 1, num_channels, 1])\n",
    "        blur_v = tf.tile(blur_v, [1, 1, num_channels, 1])\n",
    "        blurred = tf.nn.depthwise_conv2d(inputs, blur_h, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        blurred = tf.nn.depthwise_conv2d(blurred, blur_v, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        return blurred\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs.get_shape().assert_has_rank(4)\n",
    "        if inputs.dtype != self._dtype:\n",
    "            inputs = tf.cast(inputs, self._dtype)\n",
    "        net = self._base_resizer(inputs)\n",
    "        for weight, bias in zip(self._weights, self._biases):\n",
    "            blurred = self._gaussian_blur(inputs)\n",
    "            residual_image = blurred - inputs\n",
    "            resized_residual = self._base_resizer(residual_image)\n",
    "            net = net + tf.nn.tanh(weight * resized_residual + bias)\n",
    "            inputs = blurred\n",
    "        return net\n",
    "\n",
    "image_path = '/home/ozkan/works/n-smoe/notebooks/panda.jpg'\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "transform = transforms.ToTensor()\n",
    "tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "def default_resizer(inputs, target_size):\n",
    "    return F.interpolate(inputs, size=target_size, mode='bilinear', align_corners=False, antialias=True)\n",
    "\n",
    "output_default = default_resizer(tensor, target_size=(tensor.size(2), tensor.size(3)))\n",
    "\n",
    "resizer_torch = MullerResizerTorch(target_size=(tensor.size(2), tensor.size(3)), base_resize_method='bilinear',\n",
    "                                   kernel_size=5, stddev=1.0, num_layers=2, avg_pool=False, dtype=torch.float32,\n",
    "                                   init_weights=[1.9280042333186972, -91.42857142857143, 9.99854848098652489, 333.3333333333333] * 5)\n",
    "\n",
    "if tensor.dim() == 3:\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "\n",
    "resizer_torch = resizer_torch.eval()\n",
    "with torch.no_grad():\n",
    "    output_torch = resizer_torch(tensor)\n",
    "\n",
    "_CONFIGS = {\n",
    "    \"target_size\": (image.height, image.width),\n",
    "    \"base_resize_method\": tf.image.ResizeMethod.BILINEAR,\n",
    "    \"antialias\": False,\n",
    "    \"kernel_size\": 5,\n",
    "    \"stddev\": 1.0,\n",
    "    \"num_layers\": 2,\n",
    "    \"avg_pool\": False,\n",
    "    \"dtype\": tf.float32,\n",
    "    \"init_weights\": [1.892, -0.014, -11.295, 0.003]\n",
    "}\n",
    "\n",
    "model_tf = MullerResizerTF(**_CONFIGS)\n",
    "\n",
    "def inference_tf(image_path, model):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = np.asarray(image) / 255.\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    preds = model(image)\n",
    "    preds = np.array(preds[0], np.float32)\n",
    "    return np.array(np.clip(preds, 0.0, 1.0))\n",
    "\n",
    "output_image_tf = inference_tf(image_path, model_tf)\n",
    "\n",
    "output_default = transforms.ToPILImage()(output_default.squeeze(0))\n",
    "output_image_tf = transforms.ToPILImage()(output_image_tf)\n",
    "output_image_torch = transforms.ToPILImage()(output_torch.squeeze(0))\n",
    "\n",
    "output_tf_np = np.array(output_image_tf).astype(np.float32) / 255.0\n",
    "output_torch_np = np.array(output_image_torch).astype(np.float32) / 255.0\n",
    "original_np = np.array(image).astype(np.float32) / 255.0\n",
    "difference = np.abs(output_tf_np - output_torch_np)\n",
    "\n",
    "\n",
    "eng = matlab.engine.start_matlab()\n",
    "\n",
    "original_image_mat = eng.uint8(original_np * 255)\n",
    "output_image_tf_mat = eng.uint8(output_tf_np * 255)\n",
    "output_image_torch_mat = eng.uint8(output_torch_np * 255)\n",
    "\n",
    "\n",
    "mse_tf = eng.immse(original_image_mat, output_image_tf_mat)\n",
    "psnr_tf = eng.psnr(original_image_mat, output_image_tf_mat)\n",
    "ssim_tf = eng.ssim(original_image_mat, output_image_tf_mat)\n",
    "\n",
    "mse_torch = eng.immse(original_image_mat, output_image_torch_mat)\n",
    "psnr_torch = eng.psnr(original_image_mat, output_image_torch_mat)\n",
    "ssim_torch = eng.ssim(original_image_mat, output_image_torch_mat)\n",
    "\n",
    "print(f\"TF Resizer - MSE: {mse_tf}, PSNR: {psnr_tf}, SSIM: {ssim_tf}\")\n",
    "print(f\"PT Resizer - MSE: {mse_torch}, PSNR: {psnr_torch}, SSIM: {ssim_torch}\")\n",
    "\n",
    "eng.quit()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(60, 20))\n",
    "\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(output_default)\n",
    "axes[1].set_title('Default Resizer')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(output_image_tf)\n",
    "axes[2].set_title('TF Muller Resizer')\n",
    "axes[2].axis('off')\n",
    "\n",
    "axes[3].imshow(output_image_torch)\n",
    "axes[3].set_title('PT Muller Resizer')\n",
    "axes[3].axis('off')\n",
    "\n",
    "axes[4].imshow(difference) \n",
    "axes[4].set_title('Difference')\n",
    "axes[4].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matlab.engine\n",
    "\n",
    "image_path = '/home/ozkan/works/n-smoe/notebooks/panda.jpg'\n",
    "eng = matlab.engine.start_matlab()\n",
    "\n",
    "rgb = eng.imread(image_path)\n",
    "I = eng.im2gray(rgb)\n",
    "eng.imshow(I)\n",
    "\n",
    "eng.text(732,501,\"Image courtesy of Corel(R)\",\n",
    "     \"FontSize\",7,\"HorizontalAlignment\",\"right\")\n",
    "\n",
    "gmag = eng.imgradient(I)\n",
    "eng.imshow(gmag,[])\n",
    "eng.title(\"Gradient Magnitude\")\n",
    "\n",
    "\n",
    "L = eng.watershed(gmag)\n",
    "Lrgb = eng.label2rgb(L)\n",
    "eng.imshow(Lrgb)\n",
    "eng.title(\"Watershed Transform of Gradient Magnitude\")\n",
    "\n",
    "se = eng.strel(\"disk\",eng.double(20))\n",
    "Io = eng.imopen(I,se)\n",
    "eng.imshow(Io)\n",
    "eng.title(\"Opening\")\n",
    "\n",
    "\n",
    "Ie = eng.imerode(I,se)\n",
    "Iobr = eng.imreconstruct(Ie,I)\n",
    "eng.imshow(Iobr)\n",
    "eng.title(\"Opening-by-Reconstruction\")\n",
    "\n",
    "\n",
    "Ioc = eng.imclose(Io,se)\n",
    "eng.imshow(Ioc)\n",
    "eng.title(\"Opening-Closing\")\n",
    "\n",
    "\n",
    "Iobrd = eng.imdilate(Iobr,se)\n",
    "Iobrcbr = eng.imreconstruct(eng.imcomplement(Iobrd),eng.imcomplement(Iobr))\n",
    "Iobrcbr = eng.imcomplement(Iobrcbr)\n",
    "eng.imshow(Iobrcbr)\n",
    "eng.title(\"Opening-Closing by Reconstruction\")\n",
    "\n",
    "\n",
    "fgm = eng.imregionalmax(Iobrcbr)\n",
    "eng.imshow(fgm)\n",
    "eng.title(\"Regional Maxima of Opening-Closing by Reconstruction\")\n",
    "\n",
    "I2 = eng.labeloverlay(I,fgm)\n",
    "eng.imshow(I2)\n",
    "eng.title(\"Regional Maxima Superimposed on Original Image\")\n",
    "\n",
    "se2 = eng.strel(eng.ones(5,5))\n",
    "fgm2 = eng.imclose(fgm,se2)\n",
    "fgm3 = eng.imerode(fgm2,se2)\n",
    "\n",
    "fgm4 = eng.bwareaopen(fgm3,eng.double(20))\n",
    "I3 = eng.labeloverlay(I,fgm4)\n",
    "eng.imshow(I3)\n",
    "eng.title(\"Modified Regional Maxima Superimposed on Original Image\")\n",
    "\n",
    "bw = eng.imbinarize(Iobrcbr)\n",
    "eng.imshow(bw)\n",
    "eng.title(\"Thresholded Opening-Closing by Reconstruction\")\n",
    "\n",
    "\n",
    "D = eng.bwdist(bw)\n",
    "DL = eng.watershed(D)\n",
    "bgm = DL == 0\n",
    "eng.imshow(bgm)\n",
    "eng.title(\"Watershed Ridge Lines\")\n",
    "\n",
    "\n",
    "bgm_logical = eng.logical(bgm)\n",
    "fgm4_logical = eng.logical(fgm4)\n",
    "\n",
    "mask = eng.bitor(bgm_logical, fgm4_logical, nargout=1)\n",
    "\n",
    "gmag2 = eng.imimposemin(gmag, mask)\n",
    "L = eng.watershed(gmag2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numColors = 2\n",
    "# L = eng.imsegkmeans(I,numColors)\n",
    "# B = eng.labeloverlay(I,L)\n",
    "# eng.imshow(B)\n",
    "# eng.title(\"Labeled Image RGB\")\n",
    "\n",
    "lab_I = eng.rgb2lab(rgb)\n",
    "\n",
    "type(lab_I)\n",
    "\n",
    "\n",
    "ab = eng.zeros(lab_I.size[0], lab_I.size[1], 2, 'like', lab_I)\n",
    "ab[:,:,0] = lab_I[:,:,1]  # MATLAB indexing for 'a' channel\n",
    "ab[:,:,1] = lab_I[:,:,2]  # MATLAB indexing for 'b' channel\n",
    "\n",
    "\n",
    "ab_single = eng.im2single(ab)\n",
    "\n",
    "\n",
    "numColors = 3  # Define the number of colors for k-means\n",
    "pixel_labels = eng.imsegkmeans(ab_single, numColors, 'NumAttempts', 3)\n",
    "\n",
    "eng.imshow(pixel_labels)\n",
    "eng.title(\"Segmentation Result\")\n",
    "\n",
    "\n",
    "# ab = lab_I(:,:,2:3)\n",
    "# ab = eng.im2single(ab)\n",
    "# pixel_labels = eng.imsegkmeans(ab,numColors,NumAttempts=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "X, y = datasets.make_moons(n_samples=1000, noise=0.12)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:,0], X[:,1], c=y, s=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_labels_ = KMeans(n_clusters=2).fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Interleaved Moons w/ K-Means\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.scatter(X[:,0], X[:,1], c=kmeans_labels_, s=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.cluster import DBSCAN\n",
    "dbscan_labels_ = DBSCAN().fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=dbscan_labels_, s=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cudf\n",
    "from cuml.cluster import DBSCAN  # Use DBSCAN from cuml.cluster\n",
    "import matplotlib.colors as mcolors\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class MullerResizerTorch(nn.Module):\n",
    "    def __init__(self, target_size=(224, 224), base_resize_method='bilinear', kernel_size=5, stddev=1.0, num_layers=2, avg_pool=False, init_weights=None, dtype=torch.float32):\n",
    "        super(MullerResizerTorch, self).__init__()\n",
    "        self.target_size = target_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stddev = stddev\n",
    "        self.num_layers = num_layers\n",
    "        self.avg_pool = avg_pool\n",
    "        self.dtype = dtype\n",
    "\n",
    "        interpolation_methods = {\n",
    "            'bilinear': 'bilinear',\n",
    "            'nearest': 'nearest',\n",
    "            'bicubic': 'bicubic'\n",
    "        }\n",
    "        self.interpolation_method = interpolation_methods.get(base_resize_method, 'bilinear')\n",
    "\n",
    "        self.weights = nn.ParameterList()\n",
    "        self.biases = nn.ParameterList()\n",
    "        if init_weights is not None:\n",
    "            for i in range(num_layers):\n",
    "                self.weights.append(nn.Parameter(torch.tensor(init_weights[2 * i], dtype=dtype)))\n",
    "                self.biases.append(nn.Parameter(torch.tensor(init_weights[2 * i + 1], dtype=dtype)))\n",
    "        else:\n",
    "            for _ in range(num_layers):\n",
    "                self.weights.append(nn.Parameter(torch.zeros((), dtype=dtype)))\n",
    "                self.biases.append(nn.Parameter(torch.zeros((), dtype=dtype)))\n",
    "\n",
    "        self.gaussian_kernel = self.create_gaussian_kernel(kernel_size, stddev)\n",
    "\n",
    "    def create_gaussian_kernel(self, kernel_size, stddev):\n",
    "        t = torch.arange(kernel_size, dtype=self.dtype) - (kernel_size - 1) / 2\n",
    "        gaussian_kernel = torch.exp(-t.pow(2) / (2 * stddev**2))\n",
    "        gaussian_kernel /= gaussian_kernel.sum()\n",
    "        gaussian_kernel = gaussian_kernel.view(1, 1, kernel_size, 1) * gaussian_kernel.view(1, 1, 1, kernel_size)\n",
    "        gaussian_kernel = gaussian_kernel.repeat(3, 1, 1, 1)\n",
    "        return gaussian_kernel\n",
    "\n",
    "    def _apply_gaussian_blur(self, x):\n",
    "        padding = self.kernel_size // 2\n",
    "        x = F.pad(x, (padding, padding, padding, padding), mode='reflect')\n",
    "        return F.conv2d(x, self.gaussian_kernel, groups=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(dtype=self.dtype)\n",
    "        if self.avg_pool:\n",
    "            x = F.avg_pool2d(x, kernel_size=2, stride=2)\n",
    "        net = F.interpolate(x, size=self.target_size, mode=self.interpolation_method, align_corners=False)\n",
    "        \n",
    "        for weight, bias in zip(self.weights, self.biases):\n",
    "            blurred = self._apply_gaussian_blur(x)\n",
    "            residual = blurred - x\n",
    "            resized_residual = F.interpolate(residual, size=self.target_size, mode=self.interpolation_method, align_corners=False)\n",
    "            net = net + torch.tanh(weight * resized_residual + bias)\n",
    "            x = blurred\n",
    "\n",
    "        return net\n",
    "\n",
    "class MullerResizerTF(tf.keras.layers.Layer):\n",
    "    def __init__(self, target_size=(224, 224), base_resize_method=tf.image.ResizeMethod.BILINEAR, antialias=False, kernel_size=5, stddev=1.0, num_layers=2, avg_pool=False, dtype=tf.float32, init_weights=None):\n",
    "        super().__init__()\n",
    "        self._target_size = target_size\n",
    "        self._base_resize_method = base_resize_method\n",
    "        self._antialias = antialias\n",
    "        self._kernel_size = kernel_size\n",
    "        self._stddev = stddev\n",
    "        self._num_layers = num_layers\n",
    "        self._avg_pool = avg_pool\n",
    "        self._dtype = dtype\n",
    "        self._init_weights = init_weights\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self._weights = []\n",
    "        self._biases = []\n",
    "        for layer in range(1, self._num_layers + 1):\n",
    "            weight = self.add_weight(name='weight_' + str(layer), shape=[], dtype=self._dtype, initializer=tf.keras.initializers.Constant(self._init_weights[2*layer-2]) if self._init_weights else tf.keras.initializers.zeros())\n",
    "            bias = self.add_weight(name='bias_' + str(layer), shape=[], dtype=self._dtype, initializer=tf.keras.initializers.Constant(self._init_weights[2*layer-1]) if self._init_weights else tf.keras.initializers.zeros())\n",
    "            self._weights.append(weight)\n",
    "            self._biases.append(bias)\n",
    "\n",
    "    def _base_resizer(self, inputs):\n",
    "        stride = [1, inputs.get_shape().as_list()[1] // self._target_size[0], inputs.get_shape().as_list()[2] // self._target_size[1], 1]\n",
    "        if self._avg_pool and stride[1] > 1 and stride[2] > 1:\n",
    "            pooling_shape = [1, stride[1], stride[2], 1]\n",
    "            inputs = tf.nn.avg_pool(inputs, pooling_shape, stride, padding='SAME')\n",
    "        return tf.cast(tf.image.resize(inputs, self._target_size, method=self._base_resize_method, antialias=self._antialias), self._dtype)\n",
    "\n",
    "    def _gaussian_blur(self, inputs):\n",
    "        stddev = tf.cast(self._stddev, self._dtype)\n",
    "        size = self._kernel_size\n",
    "        radius = size // 2\n",
    "        x = tf.cast(tf.range(-radius, radius + 1), self._dtype)\n",
    "        blur_filter = tf.exp(-tf.pow(x, 2.0) / (2.0 * tf.pow(stddev, 2.0)))\n",
    "        blur_filter /= tf.reduce_sum(blur_filter)\n",
    "        blur_v = tf.reshape(blur_filter, [size, 1, 1, 1])\n",
    "        blur_h = tf.reshape(blur_filter, [1, size, 1, 1])\n",
    "        num_channels = inputs.get_shape()[-1]\n",
    "        blur_h = tf.tile(blur_h, [1, 1, num_channels, 1])\n",
    "        blur_v = tf.tile(blur_v, [1, 1, num_channels, 1])\n",
    "        blurred = tf.nn.depthwise_conv2d(inputs, blur_h, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        blurred = tf.nn.depthwise_conv2d(blurred, blur_v, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        return blurred\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs.get_shape().assert_has_rank(4)\n",
    "        if inputs.dtype != self._dtype:\n",
    "            inputs = tf.cast(inputs, self._dtype)\n",
    "        net = self._base_resizer(inputs)\n",
    "        for weight, bias in zip(self._weights, self._biases):\n",
    "            blurred = self._gaussian_blur(inputs)\n",
    "            residual_image = blurred - inputs\n",
    "            resized_residual = self._base_resizer(residual_image)\n",
    "            net = net + tf.nn.tanh(weight * resized_residual + bias)\n",
    "            inputs = blurred\n",
    "        return net\n",
    "\n",
    "def extract_features(image):\n",
    "    return image.reshape(-1, 1)\n",
    "\n",
    "def apply_dbscan(features, eps=5, min_samples=5):  # Use DBSCAN from cuml.cluster\n",
    "    gdf = cudf.DataFrame(features)\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    clusters = dbscan.fit_predict(gdf)\n",
    "    return clusters.to_array()\n",
    "\n",
    "def visualize_clusters(image, clusters):\n",
    "    unique_clusters = np.unique(clusters)\n",
    "    colors = list(mcolors.CSS4_COLORS.values())\n",
    "    cluster_image = np.zeros((*image.shape, 3), dtype=np.uint8)\n",
    "    \n",
    "    for cluster in unique_clusters:\n",
    "        if cluster == -1:\n",
    "            color = (0, 0, 0)  \n",
    "        else:\n",
    "            color = mcolors.hex2color(colors[cluster % len(colors)])\n",
    "            color = tuple(int(c * 255) for c in color)\n",
    "        cluster_image[clusters.reshape(image.shape) == cluster] = color\n",
    "    \n",
    "    return cluster_image\n",
    "\n",
    "def plot_results(original_image, sr_images, gt_image, sr_clusters, gt_clusters):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    plt.subplot(2, len(sr_images) + 2, 1)\n",
    "    plt.imshow(original_image, cmap='gray')\n",
    "    plt.title('Original')\n",
    "\n",
    "    for i, (sr_image, sr_cluster) in enumerate(zip(sr_images, sr_clusters), start=2):\n",
    "        plt.subplot(2, len(sr_images) + 2, i)\n",
    "        plt.imshow(sr_image, cmap='gray')\n",
    "        plt.title(f'SR {i-1}')\n",
    "        \n",
    "        plt.subplot(2, len(sr_images) + 2, i + len(sr_images) + 1)\n",
    "        plt.imshow(sr_cluster)\n",
    "        plt.title(f'SR {i-1} Clusters')\n",
    "\n",
    "    plt.subplot(2, len(sr_images) + 2, len(sr_images) + 2)\n",
    "    plt.imshow(gt_image, cmap='gray')\n",
    "    plt.title('GT')\n",
    "\n",
    "    plt.subplot(2, len(sr_images) + 2, len(sr_images) + 2 + len(sr_images) + 1)\n",
    "    plt.imshow(gt_clusters)\n",
    "    plt.title('GT Clusters')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# image_path = '/home/ozkan/works/n-smoe/notebooks/panda.jpg'\n",
    "\n",
    "image_path = 'panda.jpg'\n",
    "\n",
    "logging.info(f\"Opening image from {image_path}\")\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "def default_resizer(inputs, target_size):\n",
    "    return F.interpolate(inputs, size=target_size, mode='bilinear', align_corners=False, antialias=True)\n",
    "\n",
    "logging.info(\"Resizing with default resizer\")\n",
    "output_default = default_resizer(tensor, target_size=(tensor.size(2), tensor.size(3)))\n",
    "\n",
    "resizer_torch = MullerResizerTorch(target_size=(tensor.size(2), tensor.size(3)), base_resize_method='bilinear',\n",
    "                                   kernel_size=5, stddev=1.0, num_layers=2, avg_pool=False, dtype=torch.float32,\n",
    "                                   init_weights=[1.9280042333186972, -91.42857142857143, 9.99854848098652489, 333.3333333333333] * 5)\n",
    "\n",
    "if tensor.dim() == 3:\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "\n",
    "resizer_torch = resizer_torch.eval()\n",
    "with torch.no_grad():\n",
    "    output_torch = resizer_torch(tensor)\n",
    "\n",
    "_CONFIGS = {\n",
    "    \"target_size\": (image.height, image.width),\n",
    "    \"base_resize_method\": tf.image.ResizeMethod.BILINEAR,\n",
    "    \"antialias\": False,\n",
    "    \"kernel_size\": 5,\n",
    "    \"stddev\": 1.0,\n",
    "    \"num_layers\": 2,\n",
    "    \"avg_pool\": False,\n",
    "    \"dtype\": tf.float32,\n",
    "    \"init_weights\": [1.892, -0.014, -11.295, 0.003]\n",
    "}\n",
    "\n",
    "model_tf = MullerResizerTF(**_CONFIGS)\n",
    "\n",
    "def inference_tf(image_path, model):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = np.asarray(image) / 255.\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    preds = model(image)\n",
    "    preds = np.array(preds[0], np.float32)\n",
    "    return np.array(np.clip(preds, 0.0, 1.0))\n",
    "\n",
    "output_image_tf = inference_tf(image_path, model_tf)\n",
    "\n",
    "output_default = transforms.ToPILImage()(output_default.squeeze(0))\n",
    "output_image_tf = transforms.ToPILImage()(output_image_tf)\n",
    "output_image_torch = transforms.ToPILImage()(output_torch.squeeze(0))\n",
    "\n",
    "output_tf_np = np.array(output_image_tf).astype(np.float32) / 255.0\n",
    "output_torch_np = np.array(output_image_torch).astype(np.float32) / 255.0\n",
    "original_np = np.array(image).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "original_features = extract_features(original_np)\n",
    "output_tf_features = extract_features(output_tf_np)\n",
    "output_torch_features = extract_features(output_torch_np)\n",
    "\n",
    "logging.info(\"Clustering with DBSCAN\")\n",
    "original_clusters = apply_dbscan(original_features)\n",
    "output_tf_clusters = apply_dbscan(output_tf_features)\n",
    "output_torch_clusters = apply_dbscan(output_torch_features)\n",
    "\n",
    "# Visualize clusters\n",
    "original_cluster_image = visualize_clusters(original_np, original_clusters)\n",
    "output_tf_cluster_image = visualize_clusters(output_tf_np, output_tf_clusters)\n",
    "output_torch_cluster_image = visualize_clusters(output_torch_np, output_torch_clusters)\n",
    "\n",
    "# Plot results\n",
    "sr_images = [output_tf_np, output_torch_np]\n",
    "sr_clusters = [output_tf_cluster_image, output_torch_cluster_image]\n",
    "\n",
    "plot_results(original_np, sr_images, original_np, sr_clusters, original_cluster_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.08.00a396\n",
      "24.08.00a47\n",
      "24.08.00a80\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "cuml.internals.base_helpers.BaseMetaClass"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cudf\n",
    "import cuml\n",
    "import cugraph\n",
    "print(cudf.__version__)\n",
    "print(cuml.__version__)\n",
    "print(cugraph.__version__)\n",
    "\n",
    "from cuml.cluster import DBSCAN\n",
    "\n",
    "type(DBSCAN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    2\n",
      "2    3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import cudf\n",
    "\n",
    "print(cudf.Series([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
