{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import warnings\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, nn\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "def init_t_xy(end_x: int, end_y: int):\n",
    "    t = torch.arange(end_x * end_y, dtype=torch.float32)\n",
    "    t_x = (t % end_x).float()\n",
    "    t_y = torch.div(t, end_x, rounding_mode=\"floor\").float()\n",
    "    return t_x, t_y\n",
    "\n",
    "\n",
    "def compute_axial_cis(dim: int, end_x: int, end_y: int, theta: float = 10000.0):\n",
    "    freqs_x = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
    "    freqs_y = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
    "\n",
    "    t_x, t_y = init_t_xy(end_x, end_y)\n",
    "    freqs_x = torch.outer(t_x, freqs_x)\n",
    "    freqs_y = torch.outer(t_y, freqs_y)\n",
    "    freqs_cis_x = torch.polar(torch.ones_like(freqs_x), freqs_x)\n",
    "    freqs_cis_y = torch.polar(torch.ones_like(freqs_y), freqs_y)\n",
    "    return torch.cat([freqs_cis_x, freqs_cis_y], dim=-1)\n",
    "\n",
    "\n",
    "def get_activation_fn(activation):\n",
    "    \"\"\"Return an activation function given a string\"\"\"\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    if activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    if activation == \"glu\":\n",
    "        return F.glu\n",
    "    raise RuntimeError(f\"activation should be relu/gelu, not {activation}.\")\n",
    "\n",
    "\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "OLD_GPU = True\n",
    "USE_FLASH_ATTN = False\n",
    "MATH_KERNEL_ON = True\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[-2], x.shape[-1])\n",
    "    shape = [d if i >= ndim - 2 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_enc(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    "    repeat_freqs_k: bool = False,\n",
    "):\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = (\n",
    "        torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "        if xk.shape[-2] != 0\n",
    "        else None\n",
    "    )\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    if xk_ is None:\n",
    "        # no keys to rotate, due to dropout\n",
    "        return xq_out.type_as(xq).to(xq.device), xk\n",
    "    # repeat freqs along seq_len dim to match k seq_len\n",
    "    if repeat_freqs_k:\n",
    "        r = xk_.shape[-2] // xq_.shape[-2]\n",
    "        freqs_cis = freqs_cis.repeat(*([1] * (freqs_cis.ndim - 2)), r, 1)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq).to(xq.device), xk_out.type_as(xk).to(xk.device)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    An attention layer that allows for downscaling the size of the embedding\n",
    "    after projection to queries, keys, and values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        downsample_rate: int = 1,\n",
    "        dropout: float = 0.0,\n",
    "        kv_in_dim: int = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.kv_in_dim = kv_in_dim if kv_in_dim is not None else embedding_dim\n",
    "        self.internal_dim = embedding_dim // downsample_rate\n",
    "        self.num_heads = num_heads\n",
    "        assert (\n",
    "            self.internal_dim % num_heads == 0\n",
    "        ), \"num_heads must divide embedding_dim.\"\n",
    "\n",
    "        self.q_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "        self.k_proj = nn.Linear(self.kv_in_dim, self.internal_dim)\n",
    "        self.v_proj = nn.Linear(self.kv_in_dim, self.internal_dim)\n",
    "        self.out_proj = nn.Linear(self.internal_dim, embedding_dim)\n",
    "\n",
    "        self.dropout_p = dropout\n",
    "\n",
    "    def _separate_heads(self, x: Tensor, num_heads: int) -> Tensor:\n",
    "        b, n, c = x.shape\n",
    "        x = x.reshape(b, n, num_heads, c // num_heads)\n",
    "        return x.transpose(1, 2)  # B x N_heads x N_tokens x C_per_head\n",
    "\n",
    "    def _recombine_heads(self, x: Tensor) -> Tensor:\n",
    "        b, n_heads, n_tokens, c_per_head = x.shape\n",
    "        x = x.transpose(1, 2)\n",
    "        return x.reshape(b, n_tokens, n_heads * c_per_head)  # B x N_tokens x C\n",
    "\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
    "        # Input projections\n",
    "        q = self.q_proj(q)\n",
    "        k = self.k_proj(k)\n",
    "        v = self.v_proj(v)\n",
    "\n",
    "        # Separate into heads\n",
    "        q = self._separate_heads(q, self.num_heads)\n",
    "        k = self._separate_heads(k, self.num_heads)\n",
    "        v = self._separate_heads(v, self.num_heads)\n",
    "\n",
    "        dropout_p = self.dropout_p if self.training else 0.0\n",
    "        # Attention\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=USE_FLASH_ATTN,\n",
    "            # if Flash attention kernel is off, then math kernel needs to be enabled\n",
    "            enable_math=(OLD_GPU and dropout_p > 0.0) or MATH_KERNEL_ON,\n",
    "            enable_mem_efficient=OLD_GPU,\n",
    "        ):\n",
    "            out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
    "\n",
    "        out = self._recombine_heads(out)\n",
    "        out = self.out_proj(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class RoPEAttention(Attention):\n",
    "    \"\"\"Attention with rotary position encoding.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        rope_theta=10000.0,\n",
    "        # whether to repeat q rope to match k length\n",
    "        # this is needed for cross-attention to memories\n",
    "        rope_k_repeat=False,\n",
    "        feat_sizes=(32, 32),  # [w, h] for stride 16 feats at 512 resolution\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.compute_cis = partial(\n",
    "            compute_axial_cis, dim=self.internal_dim // self.num_heads, theta=rope_theta\n",
    "        )\n",
    "        freqs_cis = self.compute_cis(end_x=feat_sizes[0], end_y=feat_sizes[1])\n",
    "        self.freqs_cis = freqs_cis\n",
    "        self.rope_k_repeat = rope_k_repeat\n",
    "\n",
    "    def forward(\n",
    "        self, q: Tensor, k: Tensor, v: Tensor, num_k_exclude_rope: int = 0\n",
    "    ) -> Tensor:\n",
    "        # Input projections\n",
    "        q = self.q_proj(q)\n",
    "        k = self.k_proj(k)\n",
    "        v = self.v_proj(v)\n",
    "\n",
    "        # Separate into heads\n",
    "        q = self._separate_heads(q, self.num_heads)\n",
    "        k = self._separate_heads(k, self.num_heads)\n",
    "        v = self._separate_heads(v, self.num_heads)\n",
    "\n",
    "        # Apply rotary position encoding\n",
    "        w = h = math.sqrt(q.shape[-2])\n",
    "        self.freqs_cis = self.freqs_cis.to(q.device)\n",
    "        if self.freqs_cis.shape[0] != q.shape[-2]:\n",
    "            self.freqs_cis = self.compute_cis(end_x=w, end_y=h).to(q.device)\n",
    "        if q.shape[-2] != k.shape[-2]:\n",
    "            assert self.rope_k_repeat\n",
    "\n",
    "        num_k_rope = k.size(-2) - num_k_exclude_rope\n",
    "        q, k[:, :, :num_k_rope] = apply_rotary_enc(\n",
    "            q,\n",
    "            k[:, :, :num_k_rope],\n",
    "            freqs_cis=self.freqs_cis,\n",
    "            repeat_freqs_k=self.rope_k_repeat,\n",
    "        )\n",
    "\n",
    "        dropout_p = self.dropout_p if self.training else 0.0\n",
    "        # Attention\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=USE_FLASH_ATTN,\n",
    "            # if Flash attention kernel is off, then math kernel needs to be enabled\n",
    "            enable_math=(OLD_GPU and dropout_p > 0.0) or MATH_KERNEL_ON,\n",
    "            enable_mem_efficient=OLD_GPU,\n",
    "        ):\n",
    "            out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
    "\n",
    "        out = self._recombine_heads(out)\n",
    "        out = self.out_proj(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MemoryAttentionLayer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        activation: str,\n",
    "        cross_attention: nn.Module,\n",
    "        d_model: int,\n",
    "        dim_feedforward: int,\n",
    "        dropout: float,\n",
    "        pos_enc_at_attn: bool,\n",
    "        pos_enc_at_cross_attn_keys: bool,\n",
    "        pos_enc_at_cross_attn_queries: bool,\n",
    "        self_attention: nn.Module,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout_value = dropout\n",
    "        self.self_attn = self_attention\n",
    "        self.cross_attn_image = cross_attention\n",
    "\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation_str = activation\n",
    "        self.activation = get_activation_fn(activation)\n",
    "\n",
    "        # Where to add pos enc\n",
    "        self.pos_enc_at_attn = pos_enc_at_attn\n",
    "        self.pos_enc_at_cross_attn_queries = pos_enc_at_cross_attn_queries\n",
    "        self.pos_enc_at_cross_attn_keys = pos_enc_at_cross_attn_keys\n",
    "\n",
    "    def _forward_sa(self, tgt, query_pos):\n",
    "        # Self-Attention\n",
    "        tgt2 = self.norm1(tgt)\n",
    "        q = k = tgt2 + query_pos if self.pos_enc_at_attn else tgt2\n",
    "        tgt2 = self.self_attn(q, k, v=tgt2)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        return tgt\n",
    "\n",
    "    def _forward_ca(self, tgt, memory, query_pos, pos, num_k_exclude_rope=0):\n",
    "        kwds = {}\n",
    "        if num_k_exclude_rope > 0:\n",
    "            assert isinstance(self.cross_attn_image, RoPEAttention)\n",
    "            kwds = {\"num_k_exclude_rope\": num_k_exclude_rope}\n",
    "\n",
    "        # Cross-Attention\n",
    "        tgt2 = self.norm2(tgt)\n",
    "        tgt2 = self.cross_attn_image(\n",
    "            q=tgt2 + query_pos if self.pos_enc_at_cross_attn_queries else tgt2,\n",
    "            k=memory + pos if self.pos_enc_at_cross_attn_keys else memory,\n",
    "            v=memory,\n",
    "            **kwds,\n",
    "        )\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        return tgt\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt,\n",
    "        memory,\n",
    "        pos: Optional[Tensor] = None,\n",
    "        query_pos: Optional[Tensor] = None,\n",
    "        num_k_exclude_rope: int = 0,\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        # Self-Attn, Cross-Attn\n",
    "        tgt = self._forward_sa(tgt, query_pos)\n",
    "        tgt = self._forward_ca(tgt, memory, query_pos, pos, num_k_exclude_rope)\n",
    "        # MLP\n",
    "        tgt2 = self.norm3(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        return tgt\n",
    "\n",
    "\n",
    "class MemoryAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        pos_enc_at_input: bool,\n",
    "        layer: nn.Module,\n",
    "        num_layers: int,\n",
    "        batch_first: bool = True,  # Do layers expect batch first input?\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.layers = get_clones(layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.pos_enc_at_input = pos_enc_at_input\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        curr: torch.Tensor,  # self-attention inputs\n",
    "        memory: torch.Tensor,  # cross-attention inputs\n",
    "        curr_pos: Optional[Tensor] = None,  # pos_enc for self-attention inputs\n",
    "        memory_pos: Optional[Tensor] = None,  # pos_enc for cross-attention inputs\n",
    "        num_obj_ptr_tokens: int = 0,  # number of object pointer *tokens*\n",
    "    ):\n",
    "        if isinstance(curr, list):\n",
    "            assert isinstance(curr_pos, list)\n",
    "            assert len(curr) == len(curr_pos) == 1\n",
    "            curr, curr_pos = (\n",
    "                curr[0],\n",
    "                curr_pos[0],\n",
    "            )\n",
    "\n",
    "        assert (\n",
    "            curr.shape[1] == memory.shape[1]\n",
    "        ), \"Batch size must be the same for curr and memory\"\n",
    "\n",
    "        output = curr\n",
    "        if self.pos_enc_at_input and curr_pos is not None:\n",
    "            output = output + 0.1 * curr_pos\n",
    "\n",
    "        if self.batch_first:\n",
    "            # Convert to batch first\n",
    "            output = output.transpose(0, 1)\n",
    "            curr_pos = curr_pos.transpose(0, 1)\n",
    "            memory = memory.transpose(0, 1)\n",
    "            memory_pos = memory_pos.transpose(0, 1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            kwds = {}\n",
    "            if isinstance(layer.cross_attn_image, RoPEAttention):\n",
    "                kwds = {\"num_k_exclude_rope\": num_obj_ptr_tokens}\n",
    "\n",
    "            output = layer(\n",
    "                tgt=output,\n",
    "                memory=memory,\n",
    "                pos=memory_pos,\n",
    "                query_pos=curr_pos,\n",
    "                **kwds,\n",
    "            )\n",
    "        normed_output = self.norm(output)\n",
    "\n",
    "        if self.batch_first:\n",
    "            # Convert back to seq first\n",
    "            normed_output = normed_output.transpose(0, 1)\n",
    "            curr_pos = curr_pos.transpose(0, 1)\n",
    "\n",
    "        return normed_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 14, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d_model = 512  # Dimension of model (embedding size)\n",
    "num_heads = 8  # Number of attention heads\n",
    "num_layers = 3  # Number of stacked attention layers\n",
    "dropout_rate = 0.1  # Dropout rate\n",
    "batch_size = 16  # Batch size\n",
    "seq_length = 14  # Sequence length (adjusted to match memory length)\n",
    "memory_length = 14  # Memory sequence length\n",
    "\n",
    "# Initialize cross-attention (RoPE) and self-attention mechanisms\n",
    "cross_attention = RoPEAttention(\n",
    "    embedding_dim=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dropout=dropout_rate,\n",
    "    rope_theta=10000.0,\n",
    "    rope_k_repeat=True,\n",
    "    feat_sizes=(32, 32)\n",
    ")\n",
    "\n",
    "self_attention = Attention(\n",
    "    embedding_dim=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dropout=dropout_rate\n",
    ")\n",
    "\n",
    "\n",
    "memory_attn_layer = MemoryAttentionLayer(\n",
    "    activation='gelu',\n",
    "    cross_attention=cross_attention,\n",
    "    d_model=d_model,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=dropout_rate,\n",
    "    pos_enc_at_attn=True,\n",
    "    pos_enc_at_cross_attn_keys=True,\n",
    "    pos_enc_at_cross_attn_queries=True,\n",
    "    self_attention=self_attention\n",
    ")\n",
    "\n",
    "# Initialize the MemoryAttention model with multiple layers\n",
    "memory_attention_model = MemoryAttention(\n",
    "    d_model=d_model,\n",
    "    pos_enc_at_input=True,\n",
    "    layer=memory_attn_layer,\n",
    "    num_layers=num_layers\n",
    ")\n",
    "\n",
    "# Simulate input data: Current sequence and memory sequence\n",
    "curr_seq = torch.randn(batch_size, seq_length, d_model)  # Current input sequence\n",
    "memory_seq = torch.randn(batch_size, memory_length, d_model)  # Memory input sequence\n",
    "curr_pos_enc = torch.randn(batch_size, seq_length, d_model)  # Positional encodings for current sequence\n",
    "memory_pos_enc = torch.randn(batch_size, memory_length, d_model)  # Positional encodings for memory sequence\n",
    "\n",
    "\n",
    "num_obj_ptr_tokens = 0  # Adjust if certain tokens should be excluded from RoPE\n",
    "\n",
    "# Forward pass through the MemoryAttention model\n",
    "output = memory_attention_model(\n",
    "    curr=curr_seq,\n",
    "    memory=memory_seq,\n",
    "    curr_pos=curr_pos_enc,\n",
    "    memory_pos=memory_pos_enc,\n",
    "    num_obj_ptr_tokens=num_obj_ptr_tokens  # Number of object pointer tokens, if applicable\n",
    ")\n",
    "\n",
    "# The output [batch_size, seq_length, d_model]\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# PyTorch 1.7 has SiLU, but we support PyTorch 1.5.\n",
    "class SiLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * th.sigmoid(x)\n",
    "\n",
    "\n",
    "def conv_nd(dims, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Create a 1D, 2D, or 3D convolution module.\n",
    "    \"\"\"\n",
    "    if dims == 1:\n",
    "        return nn.Conv1d(*args, **kwargs)\n",
    "    elif dims == 2:\n",
    "        return nn.Conv2d(*args, **kwargs)\n",
    "    elif dims == 3:\n",
    "        return nn.Conv3d(*args, **kwargs)\n",
    "    raise ValueError(f\"unsupported dimensions: {dims}\")\n",
    "\n",
    "\n",
    "def linear(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Create a linear module.\n",
    "    \"\"\"\n",
    "    return nn.Linear(*args, **kwargs)\n",
    "\n",
    "\n",
    "def avg_pool_nd(dims, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Create a 1D, 2D, or 3D average pooling module.\n",
    "    \"\"\"\n",
    "    if dims == 1:\n",
    "        return nn.AvgPool1d(*args, **kwargs)\n",
    "    elif dims == 2:\n",
    "        return nn.AvgPool2d(*args, **kwargs)\n",
    "    elif dims == 3:\n",
    "        return nn.AvgPool3d(*args, **kwargs)\n",
    "    raise ValueError(f\"unsupported dimensions: {dims}\")\n",
    "\n",
    "\n",
    "def update_ema(target_params, source_params, rate=0.99):\n",
    "    \"\"\"\n",
    "    Update target parameters to be closer to those of source parameters using\n",
    "    an exponential moving average.\n",
    "\n",
    "    :param target_params: the target parameter sequence.\n",
    "    :param source_params: the source parameter sequence.\n",
    "    :param rate: the EMA rate (closer to 1 means slower).\n",
    "    \"\"\"\n",
    "    for targ, src in zip(target_params, source_params):\n",
    "        targ.detach().mul_(rate).add_(src, alpha=1 - rate)\n",
    "\n",
    "\n",
    "def zero_module(module):\n",
    "    \"\"\"\n",
    "    Zero out the parameters of a module and return it.\n",
    "    \"\"\"\n",
    "    for p in module.parameters():\n",
    "        p.detach().zero_()\n",
    "    return module\n",
    "\n",
    "\n",
    "def scale_module(module, scale):\n",
    "    \"\"\"\n",
    "    Scale the parameters of a module and return it.\n",
    "    \"\"\"\n",
    "    for p in module.parameters():\n",
    "        p.detach().mul_(scale)\n",
    "    return module\n",
    "\n",
    "\n",
    "def mean_flat(tensor):\n",
    "    \"\"\"\n",
    "    Take the mean over all non-batch dimensions.\n",
    "    \"\"\"\n",
    "    return tensor.mean(dim=list(range(2, len(tensor.shape))))\n",
    "\n",
    "\n",
    "class GroupNorm32(nn.GroupNorm):\n",
    "    def __init__(self, num_groups, num_channels):\n",
    "        super().__init__(num_groups=num_groups, num_channels=num_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "class BatchNorm32(nn.BatchNorm2d):\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__(num_features=num_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "class LayerNorm32(nn.LayerNorm):\n",
    "    def __init__(self, num_channels, height, width):\n",
    "        normalized_shape = [num_channels, height, width]\n",
    "        super().__init__(normalized_shape=normalized_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "class InstanceNorm32(nn.InstanceNorm2d):\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__(num_features=num_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "def normalization(channels, height=None, width=None, groups=None, norm_type='instance'):\n",
    "    \"\"\"\n",
    "    Make a standard normalization layer.\n",
    "\n",
    "    :param norm_type: type of normalization ('group', 'batch', 'layer', 'instance').\n",
    "    :param channels: number of input channels.\n",
    "    :param height: height of the input (required for LayerNorm).\n",
    "    :param width: width of the input (required for LayerNorm).\n",
    "    :param groups: number of groups (required for GroupNorm).\n",
    "    :return: an nn.Module for normalization.\n",
    "    \"\"\"\n",
    "    if norm_type == 'group':\n",
    "        if groups is None:\n",
    "            raise ValueError(\"Number of groups must be specified for GroupNorm\")\n",
    "        return GroupNorm32(groups, channels)\n",
    "    elif norm_type == 'batch':\n",
    "        return BatchNorm32(channels)\n",
    "    elif norm_type == 'layer':\n",
    "        if height is None or width is None:\n",
    "            raise ValueError(\"Height and width must be specified for LayerNorm\")\n",
    "        return LayerNorm32(channels, height, width)\n",
    "    elif norm_type == 'instance':\n",
    "        return InstanceNorm32(channels)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported normalization type: {norm_type}\")\n",
    "\n",
    "def timestep_embedding(timesteps, dim, max_period=10000):\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings.\n",
    "\n",
    "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
    "                      These may be fractional.\n",
    "    :param dim: the dimension of the output.\n",
    "    :param max_period: controls the minimum frequency of the embeddings.\n",
    "    :return: an [N x dim] Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "    half = dim // 2\n",
    "    freqs = th.exp(\n",
    "        -math.log(max_period) * th.arange(start=0, end=half, dtype=th.float32) / half\n",
    "    ).to(device=timesteps.device)\n",
    "    args = timesteps[:, None].float() * freqs[None]\n",
    "    embedding = th.cat([th.cos(args), th.sin(args)], dim=-1)\n",
    "    if dim % 2:\n",
    "        embedding = th.cat([embedding, th.zeros_like(embedding[:, :1])], dim=-1)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def checkpoint(func, inputs, params, flag):\n",
    "    \"\"\"\n",
    "    Evaluate a function without caching intermediate activations, allowing for\n",
    "    reduced memory at the expense of extra compute in the backward pass.\n",
    "\n",
    "    :param func: the function to evaluate.\n",
    "    :param inputs: the argument sequence to pass to `func`.\n",
    "    :param params: a sequence of parameters `func` depends on but does not\n",
    "                   explicitly take as arguments.\n",
    "    :param flag: if False, disable gradient checkpointing.\n",
    "    \"\"\"\n",
    "    if flag:\n",
    "        args = tuple(inputs) + tuple(params)\n",
    "        return CheckpointFunction.apply(func, len(inputs), *args)\n",
    "    else:\n",
    "        return func(*inputs)\n",
    "\n",
    "\n",
    "class CheckpointFunction(th.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, run_function, length, *args):\n",
    "        ctx.run_function = run_function\n",
    "        ctx.input_tensors = list(args[:length])\n",
    "        ctx.input_params = list(args[length:])\n",
    "        with th.no_grad():\n",
    "            output_tensors = ctx.run_function(*ctx.input_tensors)\n",
    "        return output_tensors\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *output_grads):\n",
    "        ctx.input_tensors = [x.detach().requires_grad_(True) for x in ctx.input_tensors]\n",
    "        with th.enable_grad():\n",
    "            # Fixes a bug where the first op in run_function modifies the\n",
    "            # Tensor storage in place, which is not allowed for detach()'d\n",
    "            # Tensors.\n",
    "            shallow_copies = [x.view_as(x) for x in ctx.input_tensors]\n",
    "            output_tensors = ctx.run_function(*shallow_copies)\n",
    "        input_grads = th.autograd.grad(\n",
    "            output_tensors,\n",
    "            ctx.input_tensors + ctx.input_params,\n",
    "            output_grads,\n",
    "            allow_unused=True,\n",
    "        )\n",
    "        del ctx.input_tensors\n",
    "        del ctx.input_params\n",
    "        del output_tensors\n",
    "        return (None, None) + input_grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import math\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class PositionEmbeddingSine(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a more standard version of the position embedding, very similar to the one\n",
    "    used by the Attention is all you need paper, generalized to work on images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_pos_feats,\n",
    "        temperature: int = 10000,\n",
    "        normalize: bool = True,\n",
    "        scale: Optional[float] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert num_pos_feats % 2 == 0, \"Expecting even model width\"\n",
    "        self.num_pos_feats = num_pos_feats // 2\n",
    "        self.temperature = temperature\n",
    "        self.normalize = normalize\n",
    "        if scale is not None and normalize is False:\n",
    "            raise ValueError(\"normalize should be True if scale is passed\")\n",
    "        if scale is None:\n",
    "            scale = 2 * math.pi\n",
    "        self.scale = scale\n",
    "\n",
    "        self.cache = {}\n",
    "\n",
    "    def _encode_xy(self, x, y):\n",
    "        # The positions are expected to be normalized\n",
    "        assert len(x) == len(y) and x.ndim == y.ndim == 1\n",
    "        x_embed = x * self.scale\n",
    "        y_embed = y * self.scale\n",
    "\n",
    "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n",
    "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
    "\n",
    "        pos_x = x_embed[:, None] / dim_t\n",
    "        pos_y = y_embed[:, None] / dim_t\n",
    "        pos_x = torch.stack(\n",
    "            (pos_x[:, 0::2].sin(), pos_x[:, 1::2].cos()), dim=2\n",
    "        ).flatten(1)\n",
    "        pos_y = torch.stack(\n",
    "            (pos_y[:, 0::2].sin(), pos_y[:, 1::2].cos()), dim=2\n",
    "        ).flatten(1)\n",
    "        return pos_x, pos_y\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_boxes(self, x, y, w, h):\n",
    "        pos_x, pos_y = self._encode_xy(x, y)\n",
    "        pos = torch.cat((pos_y, pos_x, h[:, None], w[:, None]), dim=1)\n",
    "        return pos\n",
    "\n",
    "    encode = encode_boxes  # Backwards compatibility\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_points(self, x, y, labels):\n",
    "        (bx, nx), (by, ny), (bl, nl) = x.shape, y.shape, labels.shape\n",
    "        assert bx == by and nx == ny and bx == bl and nx == nl\n",
    "        pos_x, pos_y = self._encode_xy(x.flatten(), y.flatten())\n",
    "        pos_x, pos_y = pos_x.reshape(bx, nx, -1), pos_y.reshape(by, ny, -1)\n",
    "        pos = torch.cat((pos_y, pos_x, labels[:, :, None]), dim=2)\n",
    "        return pos\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        cache_key = (x.shape[-2], x.shape[-1])\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key][None].repeat(x.shape[0], 1, 1, 1)\n",
    "        y_embed = (\n",
    "            torch.arange(1, x.shape[-2] + 1, dtype=torch.float32, device=x.device)\n",
    "            .view(1, -1, 1)\n",
    "            .repeat(x.shape[0], 1, x.shape[-1])\n",
    "        )\n",
    "        x_embed = (\n",
    "            torch.arange(1, x.shape[-1] + 1, dtype=torch.float32, device=x.device)\n",
    "            .view(1, 1, -1)\n",
    "            .repeat(x.shape[0], x.shape[-2], 1)\n",
    "        )\n",
    "\n",
    "        if self.normalize:\n",
    "            eps = 1e-6\n",
    "            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
    "            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
    "\n",
    "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n",
    "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
    "\n",
    "        pos_x = x_embed[:, :, :, None] / dim_t\n",
    "        pos_y = y_embed[:, :, :, None] / dim_t\n",
    "        pos_x = torch.stack(\n",
    "            (pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4\n",
    "        ).flatten(3)\n",
    "        pos_y = torch.stack(\n",
    "            (pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4\n",
    "        ).flatten(3)\n",
    "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
    "        self.cache[cache_key] = pos[0]\n",
    "        return pos\n",
    "\n",
    "\n",
    "class PositionEmbeddingRandom(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding using random spatial frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_pos_feats: int = 64, scale: Optional[float] = None) -> None:\n",
    "        super().__init__()\n",
    "        if scale is None or scale <= 0.0:\n",
    "            scale = 1.0\n",
    "        self.register_buffer(\n",
    "            \"positional_encoding_gaussian_matrix\",\n",
    "            scale * torch.randn((2, num_pos_feats)),\n",
    "        )\n",
    "\n",
    "    def _pe_encoding(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Positionally encode points that are normalized to [0,1].\"\"\"\n",
    "        # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\n",
    "        coords = 2 * coords - 1\n",
    "        coords = coords @ self.positional_encoding_gaussian_matrix\n",
    "        coords = 2 * np.pi * coords\n",
    "        # outputs d_1 x ... x d_n x C shape\n",
    "        return torch.cat([torch.sin(coords), torch.cos(coords)], dim=-1)\n",
    "\n",
    "    def forward(self, size: Tuple[int, int]) -> torch.Tensor:\n",
    "        \"\"\"Generate positional encoding for a grid of the specified size.\"\"\"\n",
    "        h, w = size\n",
    "        device: Any = self.positional_encoding_gaussian_matrix.device\n",
    "        grid = torch.ones((h, w), device=device, dtype=torch.float32)\n",
    "        y_embed = grid.cumsum(dim=0) - 0.5\n",
    "        x_embed = grid.cumsum(dim=1) - 0.5\n",
    "        y_embed = y_embed / h\n",
    "        x_embed = x_embed / w\n",
    "\n",
    "        pe = self._pe_encoding(torch.stack([x_embed, y_embed], dim=-1))\n",
    "        return pe.permute(2, 0, 1)  # C x H x W\n",
    "\n",
    "    def forward_with_coords(\n",
    "        self, coords_input: torch.Tensor, image_size: Tuple[int, int]\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Positionally encode points that are not normalized to [0,1].\"\"\"\n",
    "        coords = coords_input.clone()\n",
    "        coords[:, :, 0] = coords[:, :, 0] / image_size[1]\n",
    "        coords[:, :, 1] = coords[:, :, 1] / image_size[0]\n",
    "        return self._pe_encoding(coords.to(torch.float))  # B x N x C\n",
    "\n",
    "\n",
    "# Rotary Positional Encoding, adapted from:\n",
    "# 1. https://github.com/meta-llama/codellama/blob/main/llama/model.py\n",
    "# 2. https://github.com/naver-ai/rope-vit\n",
    "# 3. https://github.com/lucidrains/rotary-embedding-torch\n",
    "\n",
    "\n",
    "def init_t_xy(end_x: int, end_y: int):\n",
    "    t = torch.arange(end_x * end_y, dtype=torch.float32)\n",
    "    t_x = (t % end_x).float()\n",
    "    t_y = torch.div(t, end_x, rounding_mode=\"floor\").float()\n",
    "    return t_x, t_y\n",
    "\n",
    "\n",
    "def compute_axial_cis(dim: int, end_x: int, end_y: int, theta: float = 10000.0):\n",
    "    freqs_x = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
    "    freqs_y = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
    "\n",
    "    t_x, t_y = init_t_xy(end_x, end_y)\n",
    "    freqs_x = torch.outer(t_x, freqs_x)\n",
    "    freqs_y = torch.outer(t_y, freqs_y)\n",
    "    freqs_cis_x = torch.polar(torch.ones_like(freqs_x), freqs_x)\n",
    "    freqs_cis_y = torch.polar(torch.ones_like(freqs_y), freqs_y)\n",
    "    return torch.cat([freqs_cis_x, freqs_cis_y], dim=-1)\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[-2], x.shape[-1])\n",
    "    shape = [d if i >= ndim - 2 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_enc(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    "    repeat_freqs_k: bool = False,\n",
    "):\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = (\n",
    "        torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "        if xk.shape[-2] != 0\n",
    "        else None\n",
    "    )\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    if xk_ is None:\n",
    "        # no keys to rotate, due to dropout\n",
    "        return xq_out.type_as(xq).to(xq.device), xk\n",
    "    # repeat freqs along seq_len dim to match k seq_len\n",
    "    if repeat_freqs_k:\n",
    "        r = xk_.shape[-2] // xq_.shape[-2]\n",
    "        freqs_cis = freqs_cis.repeat(*([1] * (freqs_cis.ndim - 2)), r, 1)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq).to(xq.device), xk_out.type_as(xk).to(xk.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "@dataclass\n",
    "class AttentionBlockConfig:\n",
    "    channels: int = 128\n",
    "    num_heads: int = 8\n",
    "    num_head_channels: int = -1\n",
    "    use_checkpoint: bool = False\n",
    "    use_new_attention_order: bool = True\n",
    "    num_groups: int = 32\n",
    "    dim_feedforward: int = 2048\n",
    "    dropout: float = 0.1\n",
    "    activation: str = \"relu\"\n",
    "    num_layers: int = 3\n",
    "    pos_enc_at_input: bool = True\n",
    "    batch_first: bool = True\n",
    "    rope_theta: float = 10000.0\n",
    "    feat_sizes: Tuple[int, int] = (32, 32)\n",
    "\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, cfg: AttentionBlockConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if cfg.num_head_channels == -1:\n",
    "            self.num_heads = cfg.num_heads\n",
    "        else:\n",
    "            assert (\n",
    "                cfg.channels % cfg.num_head_channels == 0\n",
    "            ), f\"Channels {cfg.channels} is not divisible by num_head_channels {cfg.num_head_channels}\"\n",
    "            self.num_heads = cfg.channels // cfg.num_head_channels\n",
    "\n",
    "        self.norm = nn.LayerNorm(cfg.channels)\n",
    "        \n",
    "        cross_attention = RoPEAttention(\n",
    "            embedding_dim=cfg.channels,\n",
    "            num_heads=self.num_heads,\n",
    "            dropout=cfg.dropout,\n",
    "            rope_theta=cfg.rope_theta,\n",
    "            rope_k_repeat=True,\n",
    "            feat_sizes=cfg.feat_sizes\n",
    "        )\n",
    "\n",
    "        self_attention = Attention(\n",
    "            embedding_dim=cfg.channels,\n",
    "            num_heads=self.num_heads,\n",
    "            dropout=cfg.dropout\n",
    "        )\n",
    "\n",
    "        memory_attn_layer = MemoryAttentionLayer(\n",
    "            activation=cfg.activation,\n",
    "            cross_attention=cross_attention,\n",
    "            d_model=cfg.channels,\n",
    "            dim_feedforward=cfg.dim_feedforward,\n",
    "            dropout=cfg.dropout,\n",
    "            pos_enc_at_attn=True,\n",
    "            pos_enc_at_cross_attn_keys=True,\n",
    "            pos_enc_at_cross_attn_queries=True,\n",
    "            self_attention=self_attention\n",
    "        )\n",
    "\n",
    "        self.memory_attention = MemoryAttention(\n",
    "            d_model=cfg.channels,\n",
    "            pos_enc_at_input=cfg.pos_enc_at_input,\n",
    "            layer=memory_attn_layer,\n",
    "            num_layers=cfg.num_layers\n",
    "        )\n",
    "        \n",
    "        self.proj_out = nn.Linear(cfg.channels, cfg.channels)\n",
    "\n",
    "    def forward(self, x, memory, curr_pos, memory_pos):\n",
    "        b, c, h, w = x.shape\n",
    "        x = x.reshape(b, h * w, c)  # Reshape to (batch_size, seq_length, d_model)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        memory = memory.reshape(b, h * w, c)\n",
    "        curr_pos = curr_pos.reshape(b, h * w, c)\n",
    "        memory_pos = memory_pos.reshape(b, h * w, c)\n",
    "\n",
    "        h = self.memory_attention(curr=x, memory=memory, curr_pos=curr_pos, memory_pos=memory_pos)\n",
    "        \n",
    "        h = self.proj_out(h)\n",
    "        h = h.view(b, h.size(1), c)  # Reshape to (batch_size, seq_length, channels)\n",
    "        h = h.view(b, c, h.size(1) // w, w)  # Reshape to (batch_size, channels, height, width)\n",
    "\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 3\n",
    "dropout_rate = 0.1\n",
    "batch_size = 16\n",
    "seq_length = 32\n",
    "memory_length = 32\n",
    "\n",
    "attention_block_cfg = AttentionBlockConfig(\n",
    "    channels=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout_rate,\n",
    "    num_head_channels=-1,\n",
    "    use_checkpoint=False,\n",
    "    num_groups=32\n",
    ")\n",
    "\n",
    "x = torch.randn(batch_size, d_model, seq_length, seq_length)\n",
    "memory = torch.randn(batch_size, d_model, memory_length, memory_length)\n",
    "curr_pos = torch.randn(batch_size, d_model, seq_length, seq_length)\n",
    "memory_pos = torch.randn(batch_size, d_model, memory_length, memory_length)\n",
    "\n",
    "attention_block = AttentionBlock(cfg=attention_block_cfg)\n",
    "\n",
    "output = attention_block(x, memory, curr_pos, memory_pos)\n",
    "\n",
    "print(output.shape)  # Expected output shape: (batch_size, channels, height, width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output shape: torch.Size([1, 3, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ozkan/miniconda3/envs/venv/lib/python3.11/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n",
      "  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "d_model = 512  # Dimension of model (embedding size)\n",
    "num_heads = 8  # Number of attention heads\n",
    "num_layers = 3  # Number of stacked attention layers\n",
    "dropout_rate = 0.1  # Dropout rate\n",
    "batch_size = 16  # Batch size\n",
    "seq_length = 14  # Sequence length (adjusted to match memory length)\n",
    "memory_length = 14  # Memory sequence length\n",
    "\n",
    "# Initialize cross-attention (RoPE) and self-attention mechanisms\n",
    "cross_attention = RoPEAttention(\n",
    "    embedding_dim=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dropout=dropout_rate,\n",
    "    rope_theta=10000.0,\n",
    "    rope_k_repeat=True,\n",
    "    feat_sizes=(64, 64)\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AttentionBlockConfig:\n",
    "    channels: int = 3  # Input channels\n",
    "    embedding_dim: int = 1024  # Embedding dimension\n",
    "    num_heads: int = 4  # Number of attention heads\n",
    "    num_groups: int = 32  # Number of groups for GroupNorm\n",
    "    feat_sizes: Tuple[int, int] = (32, 32)  # Feature map size (H, W)\n",
    "    rope_theta: float = 10000.0  # RoPE scaling factor\n",
    "    dropout_rate: float = 0.1  # Dropout rate\n",
    "  \n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, cfg: AttentionBlockConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "       \n",
    "        self.norm = normalization(cfg.channels, cfg.num_groups)\n",
    "        self.qkv = conv_nd(1, cfg.channels, cfg.channels * 3, 1)\n",
    "        self.pos_embedding = PositionEmbeddingSine(cfg.embedding_dim)\n",
    "        \n",
    "        self.cross_attention = RoPEAttention(\n",
    "            embedding_dim=cfg.embedding_dim,\n",
    "            num_heads=cfg.num_heads,\n",
    "            dropout=cfg.dropout_rate,\n",
    "            rope_theta=10000.0,\n",
    "            rope_k_repeat=True,\n",
    "            feat_sizes=(32, 32)\n",
    "        )\n",
    "        self.proj_out = zero_module(conv_nd(1, cfg.channels, cfg.channels, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c,*spatial= x.shape\n",
    "        x = x.reshape(b, c, -1)\n",
    "        qkv = self.qkv(self.norm(x))\n",
    "    \n",
    "        q, k, v = qkv.chunk(3, dim=1)\n",
    "        \n",
    "      \n",
    "        h = self.cross_attention(q, k, v)\n",
    "        h = self.proj_out(h)\n",
    "        return (x + h).reshape(b, c,*spatial)\n",
    "\n",
    "cfg = AttentionBlockConfig()\n",
    "attention_block = AttentionBlock(cfg=cfg)\n",
    "sample_input = torch.randn(1, 3, 32, 32)  \n",
    "output = attention_block(sample_input)\n",
    "print(\"Final output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
