{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import copy\n",
    "import math\n",
    "import warnings\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, nn\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "def init_t_xy(end_x: int, end_y: int):\n",
    "    t = torch.arange(end_x * end_y, dtype=torch.float32)\n",
    "    t_x = (t % end_x).float()\n",
    "    t_y = torch.div(t, end_x, rounding_mode=\"floor\").float()\n",
    "    return t_x, t_y\n",
    "\n",
    "\n",
    "def compute_axial_cis(dim: int, end_x: int, end_y: int, theta: float = 10000.0):\n",
    "    freqs_x = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
    "    freqs_y = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
    "\n",
    "    t_x, t_y = init_t_xy(end_x, end_y)\n",
    "    freqs_x = torch.outer(t_x, freqs_x)\n",
    "    freqs_y = torch.outer(t_y, freqs_y)\n",
    "    freqs_cis_x = torch.polar(torch.ones_like(freqs_x), freqs_x)\n",
    "    freqs_cis_y = torch.polar(torch.ones_like(freqs_y), freqs_y)\n",
    "    return torch.cat([freqs_cis_x, freqs_cis_y], dim=-1)\n",
    "\n",
    "\n",
    "def get_activation_fn(activation):\n",
    "    \"\"\"Return an activation function given a string\"\"\"\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    if activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    if activation == \"glu\":\n",
    "        return F.glu\n",
    "    raise RuntimeError(f\"activation should be relu/gelu, not {activation}.\")\n",
    "\n",
    "\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "# def get_sdpa_settings():\n",
    "#     if torch.cuda.is_available():\n",
    "#         old_gpu = torch.cuda.get_device_properties(0).major < 7\n",
    "#         # only use Flash Attention on Ampere (8.0) or newer GPUs\n",
    "#         use_flash_attn = torch.cuda.get_device_properties(0).major >= 8\n",
    "#         if not use_flash_attn:\n",
    "#             warnings.warn(\n",
    "#                 \"Flash Attention is disabled as it requires a GPU with Ampere (8.0) CUDA capability.\",\n",
    "#                 category=UserWarning,\n",
    "#                 stacklevel=2,\n",
    "#             )\n",
    "#         # keep math kernel for PyTorch versions before 2.2 (Flash Attention v2 is only\n",
    "#         # available on PyTorch 2.2+, while Flash Attention v1 cannot handle all cases)\n",
    "#         pytorch_version = tuple(int(v) for v in torch.__version__.split(\".\")[:2])\n",
    "#         if pytorch_version < (2, 2):\n",
    "#             warnings.warn(\n",
    "#                 f\"You are using PyTorch {torch.__version__} without Flash Attention v2 support. \"\n",
    "#                 \"Consider upgrading to PyTorch 2.2+ for Flash Attention v2 (which could be faster).\",\n",
    "#                 category=UserWarning,\n",
    "#                 stacklevel=2,\n",
    "#             )\n",
    "#         math_kernel_on = pytorch_version < (2, 2) or not use_flash_attn\n",
    "#     else:\n",
    "#         old_gpu = True\n",
    "#         use_flash_attn = False\n",
    "#         math_kernel_on = True\n",
    "\n",
    "#     return old_gpu, use_flash_attn, math_kernel_on\n",
    "# OLD_GPU, USE_FLASH_ATTN, MATH_KERNEL_ON = get_sdpa_settings()\n",
    "\n",
    "OLD_GPU = True\n",
    "USE_FLASH_ATTN = False\n",
    "MATH_KERNEL_ON = True\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[-2], x.shape[-1])\n",
    "    shape = [d if i >= ndim - 2 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_enc(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    "    repeat_freqs_k: bool = False,\n",
    "):\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = (\n",
    "        torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "        if xk.shape[-2] != 0\n",
    "        else None\n",
    "    )\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    if xk_ is None:\n",
    "        # no keys to rotate, due to dropout\n",
    "        return xq_out.type_as(xq).to(xq.device), xk\n",
    "    # repeat freqs along seq_len dim to match k seq_len\n",
    "    if repeat_freqs_k:\n",
    "        r = xk_.shape[-2] // xq_.shape[-2]\n",
    "        freqs_cis = freqs_cis.repeat(*([1] * (freqs_cis.ndim - 2)), r, 1)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq).to(xq.device), xk_out.type_as(xk).to(xk.device)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    An attention layer that allows for downscaling the size of the embedding\n",
    "    after projection to queries, keys, and values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        downsample_rate: int = 1,\n",
    "        dropout: float = 0.0,\n",
    "        kv_in_dim: int = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.kv_in_dim = kv_in_dim if kv_in_dim is not None else embedding_dim\n",
    "        self.internal_dim = embedding_dim // downsample_rate\n",
    "        self.num_heads = num_heads\n",
    "        assert (\n",
    "            self.internal_dim % num_heads == 0\n",
    "        ), \"num_heads must divide embedding_dim.\"\n",
    "\n",
    "        self.q_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "        self.k_proj = nn.Linear(self.kv_in_dim, self.internal_dim)\n",
    "        self.v_proj = nn.Linear(self.kv_in_dim, self.internal_dim)\n",
    "        self.out_proj = nn.Linear(self.internal_dim, embedding_dim)\n",
    "\n",
    "        self.dropout_p = dropout\n",
    "\n",
    "    def _separate_heads(self, x: Tensor, num_heads: int) -> Tensor:\n",
    "        b, n, c = x.shape\n",
    "        x = x.reshape(b, n, num_heads, c // num_heads)\n",
    "        return x.transpose(1, 2)  # B x N_heads x N_tokens x C_per_head\n",
    "\n",
    "    def _recombine_heads(self, x: Tensor) -> Tensor:\n",
    "        b, n_heads, n_tokens, c_per_head = x.shape\n",
    "        x = x.transpose(1, 2)\n",
    "        return x.reshape(b, n_tokens, n_heads * c_per_head)  # B x N_tokens x C\n",
    "\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
    "        # Input projections\n",
    "        q = self.q_proj(q)\n",
    "        k = self.k_proj(k)\n",
    "        v = self.v_proj(v)\n",
    "\n",
    "        # Separate into heads\n",
    "        q = self._separate_heads(q, self.num_heads)\n",
    "        k = self._separate_heads(k, self.num_heads)\n",
    "        v = self._separate_heads(v, self.num_heads)\n",
    "\n",
    "        dropout_p = self.dropout_p if self.training else 0.0\n",
    "        # Attention\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=USE_FLASH_ATTN,\n",
    "            # if Flash attention kernel is off, then math kernel needs to be enabled\n",
    "            enable_math=(OLD_GPU and dropout_p > 0.0) or MATH_KERNEL_ON,\n",
    "            enable_mem_efficient=OLD_GPU,\n",
    "        ):\n",
    "            out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
    "\n",
    "        out = self._recombine_heads(out)\n",
    "        out = self.out_proj(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class RoPEAttention(Attention):\n",
    "    \"\"\"Attention with rotary position encoding.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        rope_theta=10000.0,\n",
    "        # whether to repeat q rope to match k length\n",
    "        # this is needed for cross-attention to memories\n",
    "        rope_k_repeat=False,\n",
    "        feat_sizes=(32, 32),  # [w, h] for stride 16 feats at 512 resolution\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.compute_cis = partial(\n",
    "            compute_axial_cis, dim=self.internal_dim // self.num_heads, theta=rope_theta\n",
    "        )\n",
    "        freqs_cis = self.compute_cis(end_x=feat_sizes[0], end_y=feat_sizes[1])\n",
    "        self.freqs_cis = freqs_cis\n",
    "        self.rope_k_repeat = rope_k_repeat\n",
    "\n",
    "    def forward(\n",
    "        self, q: Tensor, k: Tensor, v: Tensor, num_k_exclude_rope: int = 0\n",
    "    ) -> Tensor:\n",
    "        # Input projections\n",
    "        q = self.q_proj(q)\n",
    "        k = self.k_proj(k)\n",
    "        v = self.v_proj(v)\n",
    "\n",
    "        # Separate into heads\n",
    "        q = self._separate_heads(q, self.num_heads)\n",
    "        k = self._separate_heads(k, self.num_heads)\n",
    "        v = self._separate_heads(v, self.num_heads)\n",
    "\n",
    "        # Apply rotary position encoding\n",
    "        w = h = math.sqrt(q.shape[-2])\n",
    "        self.freqs_cis = self.freqs_cis.to(q.device)\n",
    "        if self.freqs_cis.shape[0] != q.shape[-2]:\n",
    "            self.freqs_cis = self.compute_cis(end_x=w, end_y=h).to(q.device)\n",
    "        if q.shape[-2] != k.shape[-2]:\n",
    "            assert self.rope_k_repeat\n",
    "\n",
    "        num_k_rope = k.size(-2) - num_k_exclude_rope\n",
    "        q, k[:, :, :num_k_rope] = apply_rotary_enc(\n",
    "            q,\n",
    "            k[:, :, :num_k_rope],\n",
    "            freqs_cis=self.freqs_cis,\n",
    "            repeat_freqs_k=self.rope_k_repeat,\n",
    "        )\n",
    "\n",
    "        dropout_p = self.dropout_p if self.training else 0.0\n",
    "        # Attention\n",
    "        with torch.backends.cuda.sdp_kernel(\n",
    "            enable_flash=USE_FLASH_ATTN,\n",
    "            # if Flash attention kernel is off, then math kernel needs to be enabled\n",
    "            enable_math=(OLD_GPU and dropout_p > 0.0) or MATH_KERNEL_ON,\n",
    "            enable_mem_efficient=OLD_GPU,\n",
    "        ):\n",
    "            out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
    "\n",
    "        out = self._recombine_heads(out)\n",
    "        out = self.out_proj(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MemoryAttentionLayer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        activation: str,\n",
    "        cross_attention: nn.Module,\n",
    "        d_model: int,\n",
    "        dim_feedforward: int,\n",
    "        dropout: float,\n",
    "        pos_enc_at_attn: bool,\n",
    "        pos_enc_at_cross_attn_keys: bool,\n",
    "        pos_enc_at_cross_attn_queries: bool,\n",
    "        self_attention: nn.Module,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout_value = dropout\n",
    "        self.self_attn = self_attention\n",
    "        self.cross_attn_image = cross_attention\n",
    "\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation_str = activation\n",
    "        self.activation = get_activation_fn(activation)\n",
    "\n",
    "        # Where to add pos enc\n",
    "        self.pos_enc_at_attn = pos_enc_at_attn\n",
    "        self.pos_enc_at_cross_attn_queries = pos_enc_at_cross_attn_queries\n",
    "        self.pos_enc_at_cross_attn_keys = pos_enc_at_cross_attn_keys\n",
    "\n",
    "    def _forward_sa(self, tgt, query_pos):\n",
    "        # Self-Attention\n",
    "        tgt2 = self.norm1(tgt)\n",
    "        q = k = tgt2 + query_pos if self.pos_enc_at_attn else tgt2\n",
    "        tgt2 = self.self_attn(q, k, v=tgt2)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        return tgt\n",
    "\n",
    "    def _forward_ca(self, tgt, memory, query_pos, pos, num_k_exclude_rope=0):\n",
    "        kwds = {}\n",
    "        if num_k_exclude_rope > 0:\n",
    "            assert isinstance(self.cross_attn_image, RoPEAttention)\n",
    "            kwds = {\"num_k_exclude_rope\": num_k_exclude_rope}\n",
    "\n",
    "        # Cross-Attention\n",
    "        tgt2 = self.norm2(tgt)\n",
    "        tgt2 = self.cross_attn_image(\n",
    "            q=tgt2 + query_pos if self.pos_enc_at_cross_attn_queries else tgt2,\n",
    "            k=memory + pos if self.pos_enc_at_cross_attn_keys else memory,\n",
    "            v=memory,\n",
    "            **kwds,\n",
    "        )\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        return tgt\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt,\n",
    "        memory,\n",
    "        pos: Optional[Tensor] = None,\n",
    "        query_pos: Optional[Tensor] = None,\n",
    "        num_k_exclude_rope: int = 0,\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        # Self-Attn, Cross-Attn\n",
    "        tgt = self._forward_sa(tgt, query_pos)\n",
    "        tgt = self._forward_ca(tgt, memory, query_pos, pos, num_k_exclude_rope)\n",
    "        # MLP\n",
    "        tgt2 = self.norm3(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        return tgt\n",
    "\n",
    "\n",
    "class MemoryAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        pos_enc_at_input: bool,\n",
    "        layer: nn.Module,\n",
    "        num_layers: int,\n",
    "        batch_first: bool = True,  # Do layers expect batch first input?\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.layers = get_clones(layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.pos_enc_at_input = pos_enc_at_input\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        curr: torch.Tensor,  # self-attention inputs\n",
    "        memory: torch.Tensor,  # cross-attention inputs\n",
    "        curr_pos: Optional[Tensor] = None,  # pos_enc for self-attention inputs\n",
    "        memory_pos: Optional[Tensor] = None,  # pos_enc for cross-attention inputs\n",
    "        num_obj_ptr_tokens: int = 0,  # number of object pointer *tokens*\n",
    "    ):\n",
    "        if isinstance(curr, list):\n",
    "            assert isinstance(curr_pos, list)\n",
    "            assert len(curr) == len(curr_pos) == 1\n",
    "            curr, curr_pos = (\n",
    "                curr[0],\n",
    "                curr_pos[0],\n",
    "            )\n",
    "\n",
    "        assert (\n",
    "            curr.shape[1] == memory.shape[1]\n",
    "        ), \"Batch size must be the same for curr and memory\"\n",
    "\n",
    "        output = curr\n",
    "        if self.pos_enc_at_input and curr_pos is not None:\n",
    "            output = output + 0.1 * curr_pos\n",
    "\n",
    "        if self.batch_first:\n",
    "            # Convert to batch first\n",
    "            output = output.transpose(0, 1)\n",
    "            curr_pos = curr_pos.transpose(0, 1)\n",
    "            memory = memory.transpose(0, 1)\n",
    "            memory_pos = memory_pos.transpose(0, 1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            kwds = {}\n",
    "            if isinstance(layer.cross_attn_image, RoPEAttention):\n",
    "                kwds = {\"num_k_exclude_rope\": num_obj_ptr_tokens}\n",
    "\n",
    "            output = layer(\n",
    "                tgt=output,\n",
    "                memory=memory,\n",
    "                pos=memory_pos,\n",
    "                query_pos=curr_pos,\n",
    "                **kwds,\n",
    "            )\n",
    "        normed_output = self.norm(output)\n",
    "\n",
    "        if self.batch_first:\n",
    "            # Convert back to seq first\n",
    "            normed_output = normed_output.transpose(0, 1)\n",
    "            curr_pos = curr_pos.transpose(0, 1)\n",
    "\n",
    "        return normed_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 14, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d_model = 512  # Dimension of model (embedding size)\n",
    "num_heads = 8  # Number of attention heads\n",
    "num_layers = 3  # Number of stacked attention layers\n",
    "dropout_rate = 0.1  # Dropout rate\n",
    "batch_size = 16  # Batch size\n",
    "seq_length = 14  # Sequence length (adjusted to match memory length)\n",
    "memory_length = 14  # Memory sequence length\n",
    "\n",
    "# Initialize cross-attention (RoPE) and self-attention mechanisms\n",
    "cross_attention = RoPEAttention(\n",
    "    embedding_dim=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dropout=dropout_rate,\n",
    "    rope_theta=10000.0,\n",
    "    rope_k_repeat=True,\n",
    "    feat_sizes=(32, 32)\n",
    ")\n",
    "\n",
    "self_attention = Attention(\n",
    "    embedding_dim=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dropout=dropout_rate\n",
    ")\n",
    "\n",
    "\n",
    "memory_attn_layer = MemoryAttentionLayer(\n",
    "    activation='gelu',\n",
    "    cross_attention=cross_attention,\n",
    "    d_model=d_model,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=dropout_rate,\n",
    "    pos_enc_at_attn=True,\n",
    "    pos_enc_at_cross_attn_keys=True,\n",
    "    pos_enc_at_cross_attn_queries=True,\n",
    "    self_attention=self_attention\n",
    ")\n",
    "\n",
    "# Initialize the MemoryAttention model with multiple layers\n",
    "memory_attention_model = MemoryAttention(\n",
    "    d_model=d_model,\n",
    "    pos_enc_at_input=True,\n",
    "    layer=memory_attn_layer,\n",
    "    num_layers=num_layers\n",
    ")\n",
    "\n",
    "# Simulate input data: Current sequence and memory sequence\n",
    "curr_seq = torch.randn(batch_size, seq_length, d_model)  # Current input sequence\n",
    "memory_seq = torch.randn(batch_size, memory_length, d_model)  # Memory input sequence\n",
    "curr_pos_enc = torch.randn(batch_size, seq_length, d_model)  # Positional encodings for current sequence\n",
    "memory_pos_enc = torch.randn(batch_size, memory_length, d_model)  # Positional encodings for memory sequence\n",
    "\n",
    "\n",
    "num_obj_ptr_tokens = 0  # Adjust if certain tokens should be excluded from RoPE\n",
    "\n",
    "# Forward pass through the MemoryAttention model\n",
    "output = memory_attention_model(\n",
    "    curr=curr_seq,\n",
    "    memory=memory_seq,\n",
    "    curr_pos=curr_pos_enc,\n",
    "    memory_pos=memory_pos_enc,\n",
    "    num_obj_ptr_tokens=num_obj_ptr_tokens  # Number of object pointer tokens, if applicable\n",
    ")\n",
    "\n",
    "# The output [batch_size, seq_length, d_model]\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
