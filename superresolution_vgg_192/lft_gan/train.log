24-04-08 09:22:20.852 :   task: lft_gan
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution_vgg_192
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution_vgg_192/lft_gan
    log: superresolution_vgg_192/lft_gan
    options: superresolution_vgg_192/lft_gan/options
    models: superresolution_vgg_192/lft_gan/models
    images: superresolution_vgg_192/lft_gan/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_train/SR_5x5_4x/Stanford_Gantry
      dataroot_L: None
      H_size: 660
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_test/SR_5x5_4x
      dataroot_L: None
      H_size: 660
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: xavier_uniform
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_vgg_192
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: spectral
    init_type: xavier_normal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: ssim
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: softplusgan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 0.001
    G_optimizer_wd: 0
    D_optimizer_type: adam
    D_optimizer_lr: 1e-05
    D_optimizer_wd: 0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-08 09:22:20.867 : Number of train images: 165, iters: 83
24-04-08 09:22:25.065 : 
Networks name: get_model
Params number: 1163392
Net structure:
get_model(
  (pos_encoding): PositionEncoding()
  (conv_init0): Sequential(
    (0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
  )
  (conv_init): Sequential(
    (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (5): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (altblock): Sequential(
    (0): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsampling): Sequential(
    (0): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): PixelShuffle(upscale_factor=4)
    (2): LeakyReLU(negative_slope=0.2)
    (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
)

Networks name: Discriminator_VGG_192
Params number: 20699401
Net structure:
Discriminator_VGG_192(
  (features): Sequential(
    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (2): BatchNorm2d(64, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(128, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (6): LeakyReLU(negative_slope=0.2, inplace=True)
    (7): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (8): BatchNorm2d(128, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (9): LeakyReLU(negative_slope=0.2, inplace=True)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): BatchNorm2d(256, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (12): LeakyReLU(negative_slope=0.2, inplace=True)
    (13): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (14): BatchNorm2d(256, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (15): LeakyReLU(negative_slope=0.2, inplace=True)
    (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (17): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (18): LeakyReLU(negative_slope=0.2, inplace=True)
    (19): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (20): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (21): LeakyReLU(negative_slope=0.2, inplace=True)
    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (23): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (24): LeakyReLU(negative_slope=0.2, inplace=True)
    (25): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (26): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (27): LeakyReLU(negative_slope=0.2, inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (30): LeakyReLU(negative_slope=0.2, inplace=True)
    (31): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (32): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (33): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=4608, out_features=100, bias=True)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Linear(in_features=100, out_features=1, bias=True)
  )
)

24-04-08 09:22:25.137 : 
 |  mean  |  min   |  max   |  std   || shape               
 | -0.001 | -0.020 |  0.020 |  0.011 | torch.Size([64, 1, 1, 3, 3]) || conv_init0.0.weight
 |  0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.0.weight
 |  0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.2.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.4.weight
 | -0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.0.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.bias
 |  0.001 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.0.spa_trans.attention.in_proj_weight
 | -0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.0.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.0.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.0.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.bias
 | -0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.0.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.021 | torch.Size([128, 64]) || altblock.0.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.021 | torch.Size([64, 128]) || altblock.0.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.1.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.1.spa_trans.attention.in_proj_weight
 | -0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.1.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.1.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.1.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.bias
 | -0.000 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.1.ang_trans.attention.in_proj_weight
 |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.1.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.1.ang_trans.feed_forward.4.weight
 | -0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.2.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.2.spa_trans.attention.in_proj_weight
 | -0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.2.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.2.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.2.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.bias
 | -0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.2.ang_trans.attention.in_proj_weight
 | -0.001 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.2.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.2.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.3.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.bias
 | -0.001 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.3.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.3.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.3.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.3.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.bias
 |  0.000 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.3.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.021 | torch.Size([128, 64]) || altblock.3.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.3.ang_trans.feed_forward.4.weight
 | -0.000 | -0.015 |  0.015 |  0.009 | torch.Size([1024, 64, 1, 1]) || upsampling.0.weight
 | -0.000 | -0.020 |  0.020 |  0.012 | torch.Size([1, 64, 3, 3]) || upsampling.3.weight

24-04-08 09:23:44.315 :   task: lft_gan
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution_vgg_192
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution_vgg_192/lft_gan
    log: superresolution_vgg_192/lft_gan
    options: superresolution_vgg_192/lft_gan/options
    models: superresolution_vgg_192/lft_gan/models
    images: superresolution_vgg_192/lft_gan/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_train/SR_5x5_4x/Stanford_Gantry
      dataroot_L: None
      H_size: 660
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_test/SR_5x5_4x
      dataroot_L: None
      H_size: 660
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: xavier_uniform
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_vgg_128
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: spectral
    init_type: xavier_normal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: ssim
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: softplusgan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 0.001
    G_optimizer_wd: 0
    D_optimizer_type: adam
    D_optimizer_lr: 1e-05
    D_optimizer_wd: 0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-08 09:23:44.330 : Number of train images: 165, iters: 83
24-04-08 09:23:45.949 : 
Networks name: get_model
Params number: 1163392
Net structure:
get_model(
  (pos_encoding): PositionEncoding()
  (conv_init0): Sequential(
    (0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
  )
  (conv_init): Sequential(
    (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (5): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (altblock): Sequential(
    (0): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsampling): Sequential(
    (0): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): PixelShuffle(upscale_factor=4)
    (2): LeakyReLU(negative_slope=0.2)
    (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
)

Networks name: Discriminator_VGG_128
Params number: 14501129
Net structure:
Discriminator_VGG_128(
  (features): Sequential(
    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (2): BatchNorm2d(64, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(128, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (6): LeakyReLU(negative_slope=0.2, inplace=True)
    (7): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (8): BatchNorm2d(128, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (9): LeakyReLU(negative_slope=0.2, inplace=True)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): BatchNorm2d(256, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (12): LeakyReLU(negative_slope=0.2, inplace=True)
    (13): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (14): BatchNorm2d(256, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (15): LeakyReLU(negative_slope=0.2, inplace=True)
    (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (17): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (18): LeakyReLU(negative_slope=0.2, inplace=True)
    (19): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (20): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (21): LeakyReLU(negative_slope=0.2, inplace=True)
    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (23): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (24): LeakyReLU(negative_slope=0.2, inplace=True)
    (25): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (26): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (27): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=8192, out_features=100, bias=True)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Linear(in_features=100, out_features=1, bias=True)
  )
)

24-04-08 09:23:45.992 : 
 |  mean  |  min   |  max   |  std   || shape               
 | -0.000 | -0.020 |  0.020 |  0.012 | torch.Size([64, 1, 1, 3, 3]) || conv_init0.0.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.0.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.2.weight
 |  0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.0.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.0.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.0.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.0.spa_trans.feed_forward.4.weight
 |  0.001 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.0.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.bias
 | -0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.0.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.0.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.0.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.1.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.bias
 |  0.001 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.1.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.1.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.1.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.1.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.bias
 |  0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.1.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.1.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.1.ang_trans.feed_forward.4.weight
 | -0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.2.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.2.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.2.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.2.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.021 | torch.Size([64, 128, 1, 1, 1]) || altblock.2.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.bias
 | -0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.2.ang_trans.attention.in_proj_weight
 |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.2.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.2.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.3.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.3.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.015 | torch.Size([256, 128]) || altblock.3.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.3.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.3.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.bias
 |  0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.3.ang_trans.attention.in_proj_weight
 |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.3.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.3.ang_trans.feed_forward.4.weight
 | -0.000 | -0.015 |  0.015 |  0.009 | torch.Size([1024, 64, 1, 1]) || upsampling.0.weight
 |  0.000 | -0.020 |  0.020 |  0.011 | torch.Size([1, 64, 3, 3]) || upsampling.3.weight

24-04-08 09:25:05.893 :   task: lft_gan
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution_vgg_192
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution_vgg_192/lft_gan
    log: superresolution_vgg_192/lft_gan
    options: superresolution_vgg_192/lft_gan/options
    models: superresolution_vgg_192/lft_gan/models
    images: superresolution_vgg_192/lft_gan/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_train/SR_5x5_4x/Stanford_Gantry
      dataroot_L: None
      H_size: 660
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_test/SR_5x5_4x
      dataroot_L: None
      H_size: 660
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: xavier_uniform
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_vgg_192
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: 3
    init_type: orthogonal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: ssim
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: softplusgan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 0.001
    G_optimizer_wd: 0
    D_optimizer_type: adam
    D_optimizer_lr: 1e-05
    D_optimizer_wd: 0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-08 09:25:05.908 : Number of train images: 165, iters: 83
24-04-08 09:25:07.882 : 
Networks name: get_model
Params number: 1163392
Net structure:
get_model(
  (pos_encoding): PositionEncoding()
  (conv_init0): Sequential(
    (0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
  )
  (conv_init): Sequential(
    (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (5): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (altblock): Sequential(
    (0): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsampling): Sequential(
    (0): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): PixelShuffle(upscale_factor=4)
    (2): LeakyReLU(negative_slope=0.2)
    (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
)

Networks name: Discriminator_VGG_192
Params number: 20699401
Net structure:
Discriminator_VGG_192(
  (features): Sequential(
    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (2): BatchNorm2d(64, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(128, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (6): LeakyReLU(negative_slope=0.2, inplace=True)
    (7): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (8): BatchNorm2d(128, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (9): LeakyReLU(negative_slope=0.2, inplace=True)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): BatchNorm2d(256, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (12): LeakyReLU(negative_slope=0.2, inplace=True)
    (13): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (14): BatchNorm2d(256, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (15): LeakyReLU(negative_slope=0.2, inplace=True)
    (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (17): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (18): LeakyReLU(negative_slope=0.2, inplace=True)
    (19): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (20): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (21): LeakyReLU(negative_slope=0.2, inplace=True)
    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (23): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (24): LeakyReLU(negative_slope=0.2, inplace=True)
    (25): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (26): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (27): LeakyReLU(negative_slope=0.2, inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (30): LeakyReLU(negative_slope=0.2, inplace=True)
    (31): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (32): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (33): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=4608, out_features=100, bias=True)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Linear(in_features=100, out_features=1, bias=True)
  )
)

24-04-08 09:25:07.919 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.000 | -0.020 |  0.020 |  0.011 | torch.Size([64, 1, 1, 3, 3]) || conv_init0.0.weight
 |  0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.0.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.2.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.0.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.bias
 | -0.001 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.0.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.0.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.0.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.0.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.bias
 | -0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.0.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.0.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.0.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.1.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.1.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.1.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.1.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.1.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.bias
 | -0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.1.ang_trans.attention.in_proj_weight
 |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.1.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.021 | torch.Size([64, 128]) || altblock.1.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.2.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.2.spa_trans.attention.in_proj_weight
 | -0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.2.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.2.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.2.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.bias
 |  0.000 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.2.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.2.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.021 | torch.Size([64, 128]) || altblock.2.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.3.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.3.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.3.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.3.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.021 | torch.Size([64, 128, 1, 1, 1]) || altblock.3.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.bias
 |  0.000 | -0.125 |  0.125 |  0.073 | torch.Size([192, 64]) || altblock.3.ang_trans.attention.in_proj_weight
 |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.3.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.3.ang_trans.feed_forward.4.weight
 |  0.000 | -0.015 |  0.015 |  0.009 | torch.Size([1024, 64, 1, 1]) || upsampling.0.weight
 | -0.000 | -0.020 |  0.020 |  0.012 | torch.Size([1, 64, 3, 3]) || upsampling.3.weight

24-04-08 09:25:29.616 :   task: lft_gan
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution_vgg_192
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution_vgg_192/lft_gan
    log: superresolution_vgg_192/lft_gan
    options: superresolution_vgg_192/lft_gan/options
    models: superresolution_vgg_192/lft_gan/models
    images: superresolution_vgg_192/lft_gan/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_train/SR_5x5_4x/Stanford_Gantry
      dataroot_L: None
      H_size: 660
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_test/SR_5x5_4x
      dataroot_L: None
      H_size: 660
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: xavier_uniform
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_vgg_192
    in_nc: 3
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: 3
    init_type: orthogonal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: ssim
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: softplusgan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 0.001
    G_optimizer_wd: 0
    D_optimizer_type: adam
    D_optimizer_lr: 1e-05
    D_optimizer_wd: 0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-08 09:25:29.631 : Number of train images: 165, iters: 83
24-04-08 09:25:31.664 : 
Networks name: get_model
Params number: 1163392
Net structure:
get_model(
  (pos_encoding): PositionEncoding()
  (conv_init0): Sequential(
    (0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
  )
  (conv_init): Sequential(
    (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (5): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (altblock): Sequential(
    (0): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsampling): Sequential(
    (0): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): PixelShuffle(upscale_factor=4)
    (2): LeakyReLU(negative_slope=0.2)
    (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
)

Networks name: Discriminator_VGG_192
Params number: 20700553
Net structure:
Discriminator_VGG_192(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (2): BatchNorm2d(64, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(128, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (6): LeakyReLU(negative_slope=0.2, inplace=True)
    (7): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (8): BatchNorm2d(128, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (9): LeakyReLU(negative_slope=0.2, inplace=True)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): BatchNorm2d(256, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (12): LeakyReLU(negative_slope=0.2, inplace=True)
    (13): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (14): BatchNorm2d(256, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (15): LeakyReLU(negative_slope=0.2, inplace=True)
    (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (17): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (18): LeakyReLU(negative_slope=0.2, inplace=True)
    (19): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (20): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (21): LeakyReLU(negative_slope=0.2, inplace=True)
    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (23): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (24): LeakyReLU(negative_slope=0.2, inplace=True)
    (25): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (26): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (27): LeakyReLU(negative_slope=0.2, inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (30): LeakyReLU(negative_slope=0.2, inplace=True)
    (31): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (32): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (33): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=4608, out_features=100, bias=True)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Linear(in_features=100, out_features=1, bias=True)
  )
)

24-04-08 09:25:31.702 : 
 |  mean  |  min   |  max   |  std   || shape               
 | -0.000 | -0.020 |  0.020 |  0.012 | torch.Size([64, 1, 1, 3, 3]) || conv_init0.0.weight
 |  0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.0.weight
 |  0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.2.weight
 |  0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.4.weight
 | -0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.0.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.0.spa_trans.attention.in_proj_weight
 | -0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.0.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.0.spa_trans.feed_forward.4.weight
 |  0.001 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.0.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.bias
 | -0.000 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.0.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.021 | torch.Size([128, 64]) || altblock.0.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.0.ang_trans.feed_forward.4.weight
 | -0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.1.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.bias
 | -0.001 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.1.spa_trans.attention.in_proj_weight
 | -0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.1.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.1.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.1.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.bias
 | -0.000 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.1.ang_trans.attention.in_proj_weight
 |  0.001 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.1.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.1.ang_trans.feed_forward.4.weight
 | -0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.2.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.2.spa_trans.attention.in_proj_weight
 | -0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.2.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.2.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.2.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.bias
 |  0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.2.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.2.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.2.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.3.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.3.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.3.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.3.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.021 | torch.Size([64, 128, 1, 1, 1]) || altblock.3.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.bias
 | -0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.3.ang_trans.attention.in_proj_weight
 |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.3.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.3.ang_trans.feed_forward.4.weight
 | -0.000 | -0.015 |  0.015 |  0.009 | torch.Size([1024, 64, 1, 1]) || upsampling.0.weight
 | -0.000 | -0.020 |  0.020 |  0.012 | torch.Size([1, 64, 3, 3]) || upsampling.3.weight

24-04-08 12:08:50.833 :   task: lft_gan
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution_vgg_192
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution_vgg_192/lft_gan
    log: superresolution_vgg_192/lft_gan
    options: superresolution_vgg_192/lft_gan/options
    models: superresolution_vgg_192/lft_gan/models
    images: superresolution_vgg_192/lft_gan/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_train/SR_5x5_4x/Stanford_Gantry
      dataroot_L: None
      H_size: 660
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_test/SR_5x5_4x
      dataroot_L: None
      H_size: 660
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: xavier_uniform
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_vgg_192
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: 3
    init_type: orthogonal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: ssim
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: softplusgan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 0.001
    G_optimizer_wd: 0
    D_optimizer_type: adam
    D_optimizer_lr: 1e-05
    D_optimizer_wd: 0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-08 12:08:50.855 : Number of train images: 165, iters: 83
24-04-08 12:08:52.847 : 
Networks name: get_model
Params number: 1163392
Net structure:
get_model(
  (pos_encoding): PositionEncoding()
  (conv_init0): Sequential(
    (0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
  )
  (conv_init): Sequential(
    (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (5): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (altblock): Sequential(
    (0): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsampling): Sequential(
    (0): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): PixelShuffle(upscale_factor=4)
    (2): LeakyReLU(negative_slope=0.2)
    (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
)

Networks name: Discriminator_VGG_192
Params number: 20699401
Net structure:
Discriminator_VGG_192(
  (features): Sequential(
    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (2): BatchNorm2d(64, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(128, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (6): LeakyReLU(negative_slope=0.2, inplace=True)
    (7): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (8): BatchNorm2d(128, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (9): LeakyReLU(negative_slope=0.2, inplace=True)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): BatchNorm2d(256, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (12): LeakyReLU(negative_slope=0.2, inplace=True)
    (13): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (14): BatchNorm2d(256, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (15): LeakyReLU(negative_slope=0.2, inplace=True)
    (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (17): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (18): LeakyReLU(negative_slope=0.2, inplace=True)
    (19): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (20): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (21): LeakyReLU(negative_slope=0.2, inplace=True)
    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (23): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (24): LeakyReLU(negative_slope=0.2, inplace=True)
    (25): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (26): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (27): LeakyReLU(negative_slope=0.2, inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (30): LeakyReLU(negative_slope=0.2, inplace=True)
    (31): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (32): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (33): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=4608, out_features=100, bias=True)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Linear(in_features=100, out_features=1, bias=True)
  )
)

24-04-08 12:08:52.895 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.000 | -0.020 |  0.020 |  0.011 | torch.Size([64, 1, 1, 3, 3]) || conv_init0.0.weight
 |  0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.0.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.2.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.0.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.bias
 |  0.001 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.0.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.0.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.0.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.0.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.bias
 |  0.001 | -0.125 |  0.125 |  0.073 | torch.Size([192, 64]) || altblock.0.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.0.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.0.ang_trans.feed_forward.4.weight
 | -0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.1.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.1.spa_trans.attention.in_proj_weight
 | -0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.1.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.1.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.1.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.bias
 |  0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.1.ang_trans.attention.in_proj_weight
 |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.1.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.1.ang_trans.feed_forward.4.weight
 | -0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.2.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.bias
 | -0.001 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.2.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.2.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.2.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.2.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.bias
 |  0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.2.ang_trans.attention.in_proj_weight
 |  0.001 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.021 | torch.Size([128, 64]) || altblock.2.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.2.ang_trans.feed_forward.4.weight
 | -0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.3.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.3.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.3.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.3.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.3.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.bias
 | -0.000 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.3.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.3.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.021 | torch.Size([64, 128]) || altblock.3.ang_trans.feed_forward.4.weight
 |  0.000 | -0.015 |  0.015 |  0.009 | torch.Size([1024, 64, 1, 1]) || upsampling.0.weight
 |  0.000 | -0.020 |  0.020 |  0.012 | torch.Size([1, 64, 3, 3]) || upsampling.3.weight

24-04-08 12:34:54.923 :   task: lft_gan
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution_vgg_192
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution_vgg_192/lft_gan
    log: superresolution_vgg_192/lft_gan
    options: superresolution_vgg_192/lft_gan/options
    models: superresolution_vgg_192/lft_gan/models
    images: superresolution_vgg_192/lft_gan/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_train/SR_5x5_4x/Stanford_Gantry
      dataroot_L: None
      H_size: 660
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_test/SR_5x5_4x
      dataroot_L: None
      H_size: 660
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: xavier_uniform
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_vgg_192
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: 3
    init_type: orthogonal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: ssim
    G_lossfn_weight: 0.001
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: gan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 0.001
    G_optimizer_wd: 0
    D_optimizer_type: adam
    D_optimizer_lr: 1e-05
    D_optimizer_wd: 0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-08 12:34:54.959 : Number of train images: 165, iters: 83
24-04-08 12:34:57.134 : 
Networks name: get_model
Params number: 1163392
Net structure:
get_model(
  (pos_encoding): PositionEncoding()
  (conv_init0): Sequential(
    (0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
  )
  (conv_init): Sequential(
    (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (5): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (altblock): Sequential(
    (0): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsampling): Sequential(
    (0): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): PixelShuffle(upscale_factor=4)
    (2): LeakyReLU(negative_slope=0.2)
    (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
)

Networks name: Discriminator_VGG_192
Params number: 20699401
Net structure:
Discriminator_VGG_192(
  (features): Sequential(
    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (2): BatchNorm2d(64, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(128, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (6): LeakyReLU(negative_slope=0.2, inplace=True)
    (7): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (8): BatchNorm2d(128, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (9): LeakyReLU(negative_slope=0.2, inplace=True)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): BatchNorm2d(256, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (12): LeakyReLU(negative_slope=0.2, inplace=True)
    (13): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (14): BatchNorm2d(256, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (15): LeakyReLU(negative_slope=0.2, inplace=True)
    (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (17): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (18): LeakyReLU(negative_slope=0.2, inplace=True)
    (19): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (20): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (21): LeakyReLU(negative_slope=0.2, inplace=True)
    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (23): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (24): LeakyReLU(negative_slope=0.2, inplace=True)
    (25): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (26): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (27): LeakyReLU(negative_slope=0.2, inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (30): LeakyReLU(negative_slope=0.2, inplace=True)
    (31): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (32): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (33): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=4608, out_features=100, bias=True)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Linear(in_features=100, out_features=1, bias=True)
  )
)

24-04-08 12:34:57.171 : 
 |  mean  |  min   |  max   |  std   || shape               
 | -0.000 | -0.020 |  0.020 |  0.012 | torch.Size([64, 1, 1, 3, 3]) || conv_init0.0.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.0.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.2.weight
 |  0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.0.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.0.spa_trans.attention.in_proj_weight
 | -0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.0.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.0.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.0.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.bias
 | -0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.0.ang_trans.attention.in_proj_weight
 | -0.001 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.0.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.0.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.1.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.1.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.1.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.1.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.021 | torch.Size([64, 128, 1, 1, 1]) || altblock.1.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.bias
 |  0.000 | -0.125 |  0.125 |  0.073 | torch.Size([192, 64]) || altblock.1.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.021 | torch.Size([128, 64]) || altblock.1.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.021 | torch.Size([64, 128]) || altblock.1.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.2.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.2.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.2.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.2.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.2.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.bias
 | -0.001 | -0.125 |  0.125 |  0.073 | torch.Size([192, 64]) || altblock.2.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.2.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.2.ang_trans.feed_forward.4.weight
 | -0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.3.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.3.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.3.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.3.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.3.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.bias
 | -0.001 | -0.125 |  0.125 |  0.073 | torch.Size([192, 64]) || altblock.3.ang_trans.attention.in_proj_weight
 |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.3.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.3.ang_trans.feed_forward.4.weight
 | -0.000 | -0.015 |  0.015 |  0.009 | torch.Size([1024, 64, 1, 1]) || upsampling.0.weight
 | -0.000 | -0.020 |  0.020 |  0.012 | torch.Size([1, 64, 3, 3]) || upsampling.3.weight

24-04-08 12:50:21.304 :   task: lft_gan
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution_vgg_192
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution_vgg_192/lft_gan
    log: superresolution_vgg_192/lft_gan
    options: superresolution_vgg_192/lft_gan/options
    models: superresolution_vgg_192/lft_gan/models
    images: superresolution_vgg_192/lft_gan/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_train/SR_5x5_4x/Stanford_Gantry
      dataroot_L: None
      H_size: 660
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_test/SR_5x5_4x
      dataroot_L: None
      H_size: 660
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: xavier_uniform
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_vgg_192
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: 3
    init_type: orthogonal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: ssim
    G_lossfn_weight: 0.001
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: gan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 0.001
    G_optimizer_wd: 0
    D_optimizer_type: adam
    D_optimizer_lr: 1e-05
    D_optimizer_wd: 0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-08 12:50:59.151 :   task: lft_gan
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution_vgg_192
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution_vgg_192/lft_gan
    log: superresolution_vgg_192/lft_gan
    options: superresolution_vgg_192/lft_gan/options
    models: superresolution_vgg_192/lft_gan/models
    images: superresolution_vgg_192/lft_gan/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_train/SR_5x5_4x/Stanford_Gantry
      dataroot_L: None
      H_size: 660
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_test/SR_5x5_4x
      dataroot_L: None
      H_size: 660
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: xavier_uniform
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_vgg_192
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: 3
    init_type: orthogonal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: ssim
    G_lossfn_weight: 0.001
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: gan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 0.001
    G_optimizer_wd: 0
    D_optimizer_type: adam
    D_optimizer_lr: 1e-05
    D_optimizer_wd: 0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-08 12:50:59.184 : Number of train images: 165, iters: 83
24-04-08 12:51:01.270 : 
Networks name: get_model
Params number: 1163392
Net structure:
get_model(
  (pos_encoding): PositionEncoding()
  (conv_init0): Sequential(
    (0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
  )
  (conv_init): Sequential(
    (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (5): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (altblock): Sequential(
    (0): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsampling): Sequential(
    (0): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): PixelShuffle(upscale_factor=4)
    (2): LeakyReLU(negative_slope=0.2)
    (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
)

Networks name: Discriminator_VGG_192
Params number: 20699401
Net structure:
Discriminator_VGG_192(
  (features): Sequential(
    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (2): BatchNorm2d(64, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(128, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (6): LeakyReLU(negative_slope=0.2, inplace=True)
    (7): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (8): BatchNorm2d(128, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (9): LeakyReLU(negative_slope=0.2, inplace=True)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): BatchNorm2d(256, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (12): LeakyReLU(negative_slope=0.2, inplace=True)
    (13): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (14): BatchNorm2d(256, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (15): LeakyReLU(negative_slope=0.2, inplace=True)
    (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (17): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (18): LeakyReLU(negative_slope=0.2, inplace=True)
    (19): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (20): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (21): LeakyReLU(negative_slope=0.2, inplace=True)
    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (23): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (24): LeakyReLU(negative_slope=0.2, inplace=True)
    (25): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (26): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (27): LeakyReLU(negative_slope=0.2, inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (30): LeakyReLU(negative_slope=0.2, inplace=True)
    (31): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (32): BatchNorm2d(512, eps=0.0001, momentum=0.9, affine=True, track_running_stats=True)
    (33): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (classifier): Sequential(
    (0): Linear(in_features=4608, out_features=100, bias=True)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Linear(in_features=100, out_features=1, bias=True)
  )
)

24-04-08 12:51:01.326 : 
 |  mean  |  min   |  max   |  std   || shape               
 | -0.000 | -0.020 |  0.020 |  0.012 | torch.Size([64, 1, 1, 3, 3]) || conv_init0.0.weight
 |  0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.0.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.2.weight
 |  0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.0.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.0.spa_trans.attention.in_proj_weight
 | -0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.0.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.0.spa_trans.feed_forward.4.weight
 |  0.001 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.0.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.bias
 |  0.000 | -0.125 |  0.125 |  0.073 | torch.Size([192, 64]) || altblock.0.ang_trans.attention.in_proj_weight
 |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.021 | torch.Size([128, 64]) || altblock.0.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.0.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.1.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.1.spa_trans.attention.in_proj_weight
 | -0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.1.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.1.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.1.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.bias
 | -0.000 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.1.ang_trans.attention.in_proj_weight
 |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.1.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.1.ang_trans.feed_forward.4.weight
 | -0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.2.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.2.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.2.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.2.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.021 | torch.Size([64, 128, 1, 1, 1]) || altblock.2.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.bias
 | -0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.2.ang_trans.attention.in_proj_weight
 |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.021 | torch.Size([128, 64]) || altblock.2.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.2.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.3.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.3.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.3.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.3.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.3.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.bias
 |  0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.3.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.3.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.3.ang_trans.feed_forward.4.weight
 |  0.000 | -0.015 |  0.015 |  0.009 | torch.Size([1024, 64, 1, 1]) || upsampling.0.weight
 |  0.000 | -0.020 |  0.020 |  0.012 | torch.Size([1, 64, 3, 3]) || upsampling.3.weight

