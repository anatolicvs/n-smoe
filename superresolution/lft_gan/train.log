24-04-05 16:56:43.370 :   task: lft_gan
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution/lft_gan
    log: superresolution/lft_gan
    options: superresolution/lft_gan/options
    models: superresolution/lft_gan/models
    images: superresolution/lft_gan/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_train/SR_5x5_4x/Stanford_Gantry
      dataroot_L: None
      H_size: 512
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_test/SR_5x5_4x
      dataroot_L: None
      H_size: 512
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    scale: 4
  ]
  netD:[
    net_type: discriminator_unet
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: batchspectral
    init_type: normal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: l2sum
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: gan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 1e-05
    G_optimizer_wd: 0
    D_optimizer_type: adam
    D_optimizer_lr: 1e-05
    D_optimizer_wd: 0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-05 16:56:43.400 : Number of train images: 165, iters: 83
24-04-05 17:11:51.311 :   task: lft_gan
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution/lft_gan
    log: superresolution/lft_gan
    options: superresolution/lft_gan/options
    models: superresolution/lft_gan/models
    images: superresolution/lft_gan/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_train/SR_5x5_4x/Stanford_Gantry
      dataroot_L: None
      H_size: 512
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_test/SR_5x5_4x
      dataroot_L: None
      H_size: 512
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: xavier_uniform
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_unet
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: batchspectral
    init_type: normal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: l2sum
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: gan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 1e-05
    G_optimizer_wd: 0
    D_optimizer_type: adam
    D_optimizer_lr: 1e-05
    D_optimizer_wd: 0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-05 17:11:51.340 : Number of train images: 165, iters: 83
24-04-05 17:12:45.167 : 
Networks name: get_model
Params number: 1163392
Net structure:
get_model(
  (pos_encoding): PositionEncoding()
  (conv_init0): Sequential(
    (0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
  )
  (conv_init): Sequential(
    (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (5): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (altblock): Sequential(
    (0): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsampling): Sequential(
    (0): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): PixelShuffle(upscale_factor=4)
    (2): LeakyReLU(negative_slope=0.2)
    (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
)

Networks name: Discriminator_UNet
Params number: 4375745
Net structure:
Discriminator_UNet(
  (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

24-04-05 17:12:45.257 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.000 | -0.020 |  0.020 |  0.012 | torch.Size([64, 1, 1, 3, 3]) || conv_init0.0.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.0.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.2.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.0.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.0.spa_trans.attention.in_proj_weight
 | -0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.0.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.0.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.021 | torch.Size([64, 128, 1, 1, 1]) || altblock.0.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.bias
 | -0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.0.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.0.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.0.ang_trans.feed_forward.4.weight
 | -0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.1.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.1.spa_trans.attention.in_proj_weight
 | -0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.1.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.1.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.1.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.bias
 |  0.000 | -0.125 |  0.125 |  0.073 | torch.Size([192, 64]) || altblock.1.ang_trans.attention.in_proj_weight
 |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.021 | torch.Size([128, 64]) || altblock.1.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.1.ang_trans.feed_forward.4.weight
 | -0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.2.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.2.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.2.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.2.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.2.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.bias
 |  0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.2.ang_trans.attention.in_proj_weight
 |  0.001 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.2.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.2.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.3.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.3.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.3.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.3.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.3.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.bias
 | -0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.3.ang_trans.attention.in_proj_weight
 |  0.001 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.3.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.3.ang_trans.feed_forward.4.weight
 |  0.000 | -0.015 |  0.015 |  0.009 | torch.Size([1024, 64, 1, 1]) || upsampling.0.weight
 | -0.000 | -0.020 |  0.020 |  0.012 | torch.Size([1, 64, 3, 3]) || upsampling.3.weight

24-04-05 17:15:05.692 :   task: lft_gan
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution/lft_gan
    log: superresolution/lft_gan
    options: superresolution/lft_gan/options
    models: superresolution/lft_gan/models
    images: superresolution/lft_gan/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_train/SR_5x5_4x/Stanford_Gantry
      dataroot_L: None
      H_size: 512
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_test/SR_5x5_4x
      dataroot_L: None
      H_size: 512
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: xavier_uniform
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_unet
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: batchspectral
    init_type: normal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: l2sum
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: gan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 1e-05
    G_optimizer_wd: 0
    D_optimizer_type: adam
    D_optimizer_lr: 1e-05
    D_optimizer_wd: 0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-05 17:15:05.707 : Number of train images: 165, iters: 83
24-04-05 17:15:07.096 : 
Networks name: get_model
Params number: 1163392
Net structure:
get_model(
  (pos_encoding): PositionEncoding()
  (conv_init0): Sequential(
    (0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
  )
  (conv_init): Sequential(
    (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (5): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (altblock): Sequential(
    (0): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsampling): Sequential(
    (0): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): PixelShuffle(upscale_factor=4)
    (2): LeakyReLU(negative_slope=0.2)
    (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
)

Networks name: Discriminator_UNet
Params number: 4375745
Net structure:
Discriminator_UNet(
  (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

24-04-05 17:15:07.136 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.001 | -0.020 |  0.020 |  0.012 | torch.Size([64, 1, 1, 3, 3]) || conv_init0.0.weight
 |  0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.0.weight
 |  0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.2.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.0.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.0.spa_trans.attention.in_proj_weight
 | -0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.0.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.0.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.0.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.bias
 | -0.000 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.0.ang_trans.attention.in_proj_weight
 |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.0.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.021 | torch.Size([64, 128]) || altblock.0.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.1.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.1.spa_trans.attention.in_proj_weight
 | -0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.1.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.1.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.1.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.bias
 |  0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.1.ang_trans.attention.in_proj_weight
 |  0.001 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.1.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.021 | torch.Size([64, 128]) || altblock.1.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.2.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.2.spa_trans.attention.in_proj_weight
 | -0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.2.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.2.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.2.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.bias
 | -0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.2.ang_trans.attention.in_proj_weight
 |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.2.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.2.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.3.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.3.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.3.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.3.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.3.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.bias
 | -0.000 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.3.ang_trans.attention.in_proj_weight
 |  0.001 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.3.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.3.ang_trans.feed_forward.4.weight
 | -0.000 | -0.015 |  0.015 |  0.009 | torch.Size([1024, 64, 1, 1]) || upsampling.0.weight
 |  0.000 | -0.020 |  0.020 |  0.012 | torch.Size([1, 64, 3, 3]) || upsampling.3.weight

24-04-05 17:16:24.240 :   task: lft_gan
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution/lft_gan
    log: superresolution/lft_gan
    options: superresolution/lft_gan/options
    models: superresolution/lft_gan/models
    images: superresolution/lft_gan/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_train/SR_5x5_4x/Stanford_Gantry
      dataroot_L: None
      H_size: 512
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_test/SR_5x5_4x
      dataroot_L: None
      H_size: 512
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: xavier_uniform
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_unet
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: batchspectral
    init_type: normal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: l2sum
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: gan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 1e-05
    G_optimizer_wd: 0
    D_optimizer_type: adam
    D_optimizer_lr: 1e-05
    D_optimizer_wd: 0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-05 17:16:24.255 : Number of train images: 165, iters: 83
24-04-05 17:16:25.628 : 
Networks name: get_model
Params number: 1163392
Net structure:
get_model(
  (pos_encoding): PositionEncoding()
  (conv_init0): Sequential(
    (0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
  )
  (conv_init): Sequential(
    (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (5): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (altblock): Sequential(
    (0): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsampling): Sequential(
    (0): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): PixelShuffle(upscale_factor=4)
    (2): LeakyReLU(negative_slope=0.2)
    (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
)

Networks name: Discriminator_UNet
Params number: 4375745
Net structure:
Discriminator_UNet(
  (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

24-04-05 17:16:25.669 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.000 | -0.020 |  0.020 |  0.012 | torch.Size([64, 1, 1, 3, 3]) || conv_init0.0.weight
 |  0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.0.weight
 |  0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.2.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.4.weight
 | -0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.0.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.0.spa_trans.attention.in_proj_weight
 | -0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.0.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.0.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.021 | torch.Size([64, 128, 1, 1, 1]) || altblock.0.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.bias
 |  0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.0.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.021 | torch.Size([128, 64]) || altblock.0.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.0.ang_trans.feed_forward.4.weight
 | -0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.1.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.1.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.1.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.1.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.1.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.bias
 | -0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.1.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.1.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.1.ang_trans.feed_forward.4.weight
 | -0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.2.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.2.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.2.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.2.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.2.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.bias
 |  0.000 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.2.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.2.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.021 | torch.Size([64, 128]) || altblock.2.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.3.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.3.spa_trans.attention.in_proj_weight
 | -0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.3.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.3.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.3.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.bias
 |  0.000 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.3.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.3.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.3.ang_trans.feed_forward.4.weight
 |  0.000 | -0.015 |  0.015 |  0.009 | torch.Size([1024, 64, 1, 1]) || upsampling.0.weight
 | -0.001 | -0.020 |  0.020 |  0.012 | torch.Size([1, 64, 3, 3]) || upsampling.3.weight

24-04-05 17:17:25.582 :   task: lft_gan
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution/lft_gan
    log: superresolution/lft_gan
    options: superresolution/lft_gan/options
    models: superresolution/lft_gan/models
    images: superresolution/lft_gan/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_train/SR_5x5_4x/Stanford_Gantry
      dataroot_L: None
      H_size: 512
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_test/SR_5x5_4x
      dataroot_L: None
      H_size: 512
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: xavier_uniform
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_unet
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: batchspectral
    init_type: normal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: l2sum
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: gan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 1e-05
    G_optimizer_wd: 0
    D_optimizer_type: adam
    D_optimizer_lr: 1e-05
    D_optimizer_wd: 0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-05 17:17:25.611 : Number of train images: 165, iters: 83
24-04-05 17:17:27.015 : 
Networks name: get_model
Params number: 1163392
Net structure:
get_model(
  (pos_encoding): PositionEncoding()
  (conv_init0): Sequential(
    (0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
  )
  (conv_init): Sequential(
    (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (5): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (altblock): Sequential(
    (0): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsampling): Sequential(
    (0): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): PixelShuffle(upscale_factor=4)
    (2): LeakyReLU(negative_slope=0.2)
    (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
)

Networks name: Discriminator_UNet
Params number: 4375745
Net structure:
Discriminator_UNet(
  (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

24-04-05 17:17:27.056 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.000 | -0.020 |  0.020 |  0.012 | torch.Size([64, 1, 1, 3, 3]) || conv_init0.0.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.0.weight
 |  0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.2.weight
 |  0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.0.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.0.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.0.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.0.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.0.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.bias
 |  0.000 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.0.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.0.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.0.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.1.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.1.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.1.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.1.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.1.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.bias
 | -0.000 | -0.125 |  0.125 |  0.073 | torch.Size([192, 64]) || altblock.1.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.1.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.1.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.2.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.2.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.2.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.2.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.2.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.bias
 |  0.000 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.2.ang_trans.attention.in_proj_weight
 |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.2.ang_trans.feed_forward.1.weight
 |  0.001 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.2.ang_trans.feed_forward.4.weight
 | -0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.3.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.3.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.3.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.3.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.021 | torch.Size([64, 128, 1, 1, 1]) || altblock.3.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.bias
 | -0.000 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.3.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.3.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.3.ang_trans.feed_forward.4.weight
 | -0.000 | -0.015 |  0.015 |  0.009 | torch.Size([1024, 64, 1, 1]) || upsampling.0.weight
 |  0.000 | -0.020 |  0.020 |  0.012 | torch.Size([1, 64, 3, 3]) || upsampling.3.weight

24-04-05 17:19:29.740 :   task: lft_gan
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution/lft_gan
    log: superresolution/lft_gan
    options: superresolution/lft_gan/options
    models: superresolution/lft_gan/models
    images: superresolution/lft_gan/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_train/SR_5x5_4x/Stanford_Gantry
      dataroot_L: None
      H_size: 660
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_test/SR_5x5_4x
      dataroot_L: None
      H_size: 660
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: xavier_uniform
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_unet
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: batchspectral
    init_type: normal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: l2sum
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: gan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 1e-05
    G_optimizer_wd: 0
    D_optimizer_type: adam
    D_optimizer_lr: 1e-05
    D_optimizer_wd: 0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-05 17:19:29.770 : Number of train images: 165, iters: 83
24-04-05 17:19:31.249 : 
Networks name: get_model
Params number: 1163392
Net structure:
get_model(
  (pos_encoding): PositionEncoding()
  (conv_init0): Sequential(
    (0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
  )
  (conv_init): Sequential(
    (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (5): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (altblock): Sequential(
    (0): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsampling): Sequential(
    (0): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): PixelShuffle(upscale_factor=4)
    (2): LeakyReLU(negative_slope=0.2)
    (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
)

Networks name: Discriminator_UNet
Params number: 4375745
Net structure:
Discriminator_UNet(
  (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

24-04-05 17:19:31.292 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.000 | -0.020 |  0.020 |  0.012 | torch.Size([64, 1, 1, 3, 3]) || conv_init0.0.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.0.weight
 |  0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.2.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.0.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.0.spa_trans.attention.in_proj_weight
 | -0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.0.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.0.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.0.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.bias
 | -0.000 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.0.ang_trans.attention.in_proj_weight
 |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.0.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.0.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.1.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.1.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.1.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.1.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.1.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.bias
 | -0.000 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.1.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.021 | torch.Size([128, 64]) || altblock.1.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.1.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.2.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.2.spa_trans.attention.in_proj_weight
 | -0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.2.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.2.spa_trans.feed_forward.4.weight
 |  0.001 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.2.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.bias
 |  0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.2.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.2.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.2.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.3.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.3.spa_trans.attention.in_proj_weight
 | -0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.3.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.3.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.3.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.bias
 |  0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.3.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.3.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.3.ang_trans.feed_forward.4.weight
 |  0.000 | -0.015 |  0.015 |  0.009 | torch.Size([1024, 64, 1, 1]) || upsampling.0.weight
 |  0.001 | -0.020 |  0.020 |  0.012 | torch.Size([1, 64, 3, 3]) || upsampling.3.weight

24-04-05 17:26:39.005 : <epoch:  2, iter:     200, lr:1.000e-05> G_loss: 2.676e-04 F_loss: 8.317e-03 D_loss: 3.471e-03 D_real: 2.962e-03 D_fake: -1.975e-03 
24-04-05 17:33:08.496 : <epoch:  4, iter:     400, lr:1.000e-05> G_loss: 2.138e-04 F_loss: 4.762e-03 D_loss: 3.473e-03 D_real: 1.890e-01 D_fake: -1.052e-03 
24-04-05 17:39:40.843 : <epoch:  7, iter:     600, lr:1.000e-05> G_loss: 1.515e-04 F_loss: 2.698e-03 D_loss: 3.454e-03 D_real: -1.643e-02 D_fake: 4.650e-03 
24-04-05 17:46:10.856 : <epoch:  9, iter:     800, lr:1.000e-05> G_loss: 1.440e-04 F_loss: 2.425e-03 D_loss: 3.462e-03 D_real: 1.006e-03 D_fake: 1.641e-03 
24-04-05 17:52:42.834 : <epoch: 12, iter:   1,000, lr:1.000e-05> G_loss: 5.057e-06 F_loss: 2.972e-04 D_loss: 3.411e-03 D_real: 2.222e-02 D_fake: 2.193e-02 
24-04-07 16:30:47.420 :   task: lft_gan
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution/lft_gan
    log: superresolution/lft_gan
    options: superresolution/lft_gan/options
    models: superresolution/lft_gan/models
    images: superresolution/lft_gan/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_train/SR_5x5_4x/Stanford_Gantry
      dataroot_L: None
      H_size: 660
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_test/SR_5x5_4x
      dataroot_L: None
      H_size: 660
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: xavier_uniform
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_unet
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: spectral
    init_type: normal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: ragan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    D_optimizer_type: adam
    D_optimizer_lr: 1e-05
    D_optimizer_wd: 0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-07 16:30:47.480 : Number of train images: 165, iters: 83
24-04-07 16:30:49.293 : 
Networks name: get_model
Params number: 1163392
Net structure:
get_model(
  (pos_encoding): PositionEncoding()
  (conv_init0): Sequential(
    (0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
  )
  (conv_init): Sequential(
    (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (5): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (altblock): Sequential(
    (0): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsampling): Sequential(
    (0): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): PixelShuffle(upscale_factor=4)
    (2): LeakyReLU(negative_slope=0.2)
    (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
)

Networks name: Discriminator_UNet
Params number: 4375745
Net structure:
Discriminator_UNet(
  (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

24-04-07 16:30:49.343 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.001 | -0.020 |  0.020 |  0.011 | torch.Size([64, 1, 1, 3, 3]) || conv_init0.0.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.0.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.2.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.4.weight
 | -0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.0.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.0.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.0.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.0.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.0.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.bias
 | -0.000 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.0.ang_trans.attention.in_proj_weight
 | -0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.0.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.0.ang_trans.feed_forward.4.weight
 | -0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.1.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.1.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.1.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.1.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.021 | torch.Size([64, 128, 1, 1, 1]) || altblock.1.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.bias
 | -0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.1.ang_trans.attention.in_proj_weight
 |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.1.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.1.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.2.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.2.spa_trans.attention.in_proj_weight
 | -0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.2.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.2.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.2.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.bias
 |  0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.2.ang_trans.attention.in_proj_weight
 | -0.001 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.021 | torch.Size([128, 64]) || altblock.2.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.2.ang_trans.feed_forward.4.weight
 | -0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.3.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.3.spa_trans.attention.in_proj_weight
 | -0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.3.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.3.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.3.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.bias
 |  0.001 | -0.125 |  0.125 |  0.073 | torch.Size([192, 64]) || altblock.3.ang_trans.attention.in_proj_weight
 |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.3.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.021 | torch.Size([64, 128]) || altblock.3.ang_trans.feed_forward.4.weight
 | -0.000 | -0.015 |  0.015 |  0.009 | torch.Size([1024, 64, 1, 1]) || upsampling.0.weight
 |  0.001 | -0.020 |  0.020 |  0.012 | torch.Size([1, 64, 3, 3]) || upsampling.3.weight

24-04-07 16:38:17.334 :   task: lft_gan
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution/lft_gan
    log: superresolution/lft_gan
    options: superresolution/lft_gan/options
    models: superresolution/lft_gan/models
    images: superresolution/lft_gan/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_train/SR_5x5_4x/Stanford_Gantry
      dataroot_L: None
      H_size: 660
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_test/SR_5x5_4x
      dataroot_L: None
      H_size: 660
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: xavier_uniform
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_unet
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: spectral
    init_type: normal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: ragan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    D_optimizer_type: adam
    D_optimizer_lr: 1e-05
    D_optimizer_wd: 0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-07 16:38:17.387 : Number of train images: 165, iters: 83
24-04-07 16:38:19.032 : 
Networks name: get_model
Params number: 1163392
Net structure:
get_model(
  (pos_encoding): PositionEncoding()
  (conv_init0): Sequential(
    (0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
  )
  (conv_init): Sequential(
    (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (5): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (altblock): Sequential(
    (0): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsampling): Sequential(
    (0): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): PixelShuffle(upscale_factor=4)
    (2): LeakyReLU(negative_slope=0.2)
    (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
)

Networks name: Discriminator_UNet
Params number: 4375745
Net structure:
Discriminator_UNet(
  (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

24-04-07 16:38:19.082 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.001 | -0.020 |  0.020 |  0.012 | torch.Size([64, 1, 1, 3, 3]) || conv_init0.0.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.0.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.2.weight
 | -0.000 | -0.014 |  0.014 |  0.008 | torch.Size([64, 64, 1, 3, 3]) || conv_init.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.0.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.0.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.0.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.0.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.021 | torch.Size([64, 128, 1, 1, 1]) || altblock.0.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.bias
 |  0.000 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.0.ang_trans.attention.in_proj_weight
 |  0.000 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.021 | torch.Size([128, 64]) || altblock.0.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.0.ang_trans.feed_forward.4.weight
 | -0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.1.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.1.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.1.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.1.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.021 | torch.Size([64, 128, 1, 1, 1]) || altblock.1.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.bias
 | -0.000 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.1.ang_trans.attention.in_proj_weight
 |  0.001 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.021 | torch.Size([128, 64]) || altblock.1.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.021 | torch.Size([64, 128]) || altblock.1.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.2.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.2.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.2.spa_trans.feed_forward.1.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.2.spa_trans.feed_forward.4.weight
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.2.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.bias
 |  0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.2.ang_trans.attention.in_proj_weight
 |  0.001 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.bias
 |  0.000 | -0.035 |  0.035 |  0.021 | torch.Size([128, 64]) || altblock.2.ang_trans.feed_forward.1.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128]) || altblock.2.ang_trans.feed_forward.4.weight
 |  0.000 | -0.018 |  0.018 |  0.011 | torch.Size([128, 576]) || altblock.3.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.3.spa_trans.attention.in_proj_weight
 |  0.000 | -0.031 |  0.031 |  0.018 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([256, 128]) || altblock.3.spa_trans.feed_forward.1.weight
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([128, 256]) || altblock.3.spa_trans.feed_forward.4.weight
 |  0.000 | -0.035 |  0.035 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.3.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.bias
 |  0.001 | -0.125 |  0.125 |  0.073 | torch.Size([192, 64]) || altblock.3.ang_trans.attention.in_proj_weight
 |  0.001 | -0.043 |  0.043 |  0.025 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.bias
 | -0.000 | -0.035 |  0.035 |  0.020 | torch.Size([128, 64]) || altblock.3.ang_trans.feed_forward.1.weight
 | -0.000 | -0.035 |  0.035 |  0.021 | torch.Size([64, 128]) || altblock.3.ang_trans.feed_forward.4.weight
 | -0.000 | -0.015 |  0.015 |  0.009 | torch.Size([1024, 64, 1, 1]) || upsampling.0.weight
 |  0.000 | -0.020 |  0.020 |  0.012 | torch.Size([1, 64, 3, 3]) || upsampling.3.weight

24-04-07 16:45:10.005 : <epoch:  2, iter:     200, lr:2.000e-04> G_loss: 8.216e-07 F_loss: 3.750e-03 D_loss: 3.466e-03 D_real: 1.786e-03 D_fake: 1.716e-03 
24-04-07 16:52:12.559 : <epoch:  4, iter:     400, lr:2.000e-04> G_loss: 7.971e-07 F_loss: 4.284e-03 D_loss: 3.568e-03 D_real: 2.203e-01 D_fake: 1.823e-01 
24-04-07 16:59:13.461 : <epoch:  7, iter:     600, lr:2.000e-04> G_loss: 2.881e-06 F_loss: 1.150e-02 D_loss: 4.830e-03 D_real: 2.986e-01 D_fake: -1.753e-01 
24-04-07 17:06:14.120 : <epoch:  9, iter:     800, lr:2.000e-04> G_loss: 4.370e-07 F_loss: 1.510e-03 D_loss: 3.418e-03 D_real: -6.411e-02 D_fake: -4.500e-02 
24-04-07 17:13:15.779 : <epoch: 12, iter:   1,000, lr:2.000e-04> G_loss: 4.122e-07 F_loss: 1.156e-03 D_loss: 3.501e-03 D_real: 4.343e-01 D_fake: 4.206e-01 
24-04-07 17:20:16.355 : <epoch: 14, iter:   1,200, lr:2.000e-04> G_loss: 7.254e-07 F_loss: 3.943e-03 D_loss: 3.444e-03 D_real: 8.339e-02 D_fake: 9.340e-02 
24-04-07 17:27:19.730 : <epoch: 17, iter:   1,400, lr:2.000e-04> G_loss: 1.047e-06 F_loss: 7.957e-03 D_loss: 3.467e-03 D_real: 2.272e-01 D_fake: 2.291e-01 
24-04-07 17:34:20.557 : <epoch: 19, iter:   1,600, lr:2.000e-04> G_loss: 3.326e-07 F_loss: 1.659e-03 D_loss: 3.469e-03 D_real: 1.181e+00 D_fake: 1.180e+00 
24-04-07 17:41:21.486 : <epoch: 21, iter:   1,800, lr:2.000e-04> G_loss: 2.171e-07 F_loss: 9.800e-04 D_loss: 3.469e-03 D_real: -6.429e-02 D_fake: -6.562e-02 
24-04-07 17:48:22.389 : <epoch: 24, iter:   2,000, lr:2.000e-04> G_loss: 7.007e-07 F_loss: 4.421e-03 D_loss: 3.470e-03 D_real: 5.378e-01 D_fake: 5.360e-01 
24-04-07 17:55:24.285 : <epoch: 26, iter:   2,200, lr:2.000e-04> G_loss: 2.306e-07 F_loss: 5.979e-04 D_loss: 3.464e-03 D_real: -6.542e-01 D_fake: -6.537e-01 
24-04-07 18:02:25.385 : <epoch: 29, iter:   2,400, lr:2.000e-04> G_loss: 7.326e-07 F_loss: 4.245e-03 D_loss: 3.471e-03 D_real: -8.263e-01 D_fake: -8.281e-01 
24-04-07 18:09:26.300 : <epoch: 31, iter:   2,600, lr:2.000e-04> G_loss: 5.014e-07 F_loss: 3.707e-03 D_loss: 3.468e-03 D_real: -1.021e+00 D_fake: -1.022e+00 
24-04-07 18:16:27.447 : <epoch: 34, iter:   2,800, lr:2.000e-04> G_loss: 1.762e-07 F_loss: 1.345e-03 D_loss: 3.469e-03 D_real: -1.220e+00 D_fake: -1.222e+00 
24-04-07 18:23:28.602 : <epoch: 36, iter:   3,000, lr:2.000e-04> G_loss: 1.417e-06 F_loss: 8.048e-03 D_loss: 3.497e-03 D_real: -5.185e-01 D_fake: -5.284e-01 
24-04-07 18:30:29.445 : <epoch: 39, iter:   3,200, lr:2.000e-04> G_loss: 2.538e-07 F_loss: 1.549e-03 D_loss: 3.472e-03 D_real: 6.370e-01 D_fake: 6.346e-01 
24-04-07 18:37:30.255 : <epoch: 41, iter:   3,400, lr:2.000e-04> G_loss: 1.341e-07 F_loss: 4.761e-04 D_loss: 3.467e-03 D_real: -1.148e+00 D_fake: -1.149e+00 
24-04-07 18:44:30.948 : <epoch: 43, iter:   3,600, lr:2.000e-04> G_loss: 4.363e-07 F_loss: 3.481e-03 D_loss: 3.467e-03 D_real: 1.555e+00 D_fake: 1.554e+00 
24-04-07 18:51:34.254 : <epoch: 46, iter:   3,800, lr:2.000e-04> G_loss: 1.204e-06 F_loss: 1.269e-02 D_loss: 3.477e-03 D_real: -4.218e-01 D_fake: -4.251e-01 
24-04-07 18:58:34.899 : <epoch: 48, iter:   4,000, lr:2.000e-04> G_loss: 2.830e-07 F_loss: 1.345e-03 D_loss: 3.466e-03 D_real: 7.860e-01 D_fake: 7.858e-01 
24-04-07 19:05:35.732 : <epoch: 51, iter:   4,200, lr:2.000e-04> G_loss: 2.425e-07 F_loss: 1.145e-03 D_loss: 3.472e-03 D_real: 4.086e-01 D_fake: 4.062e-01 
24-04-07 19:12:36.544 : <epoch: 53, iter:   4,400, lr:2.000e-04> G_loss: 1.539e-07 F_loss: 6.544e-04 D_loss: 3.468e-03 D_real: -6.816e-01 D_fake: -6.826e-01 
24-04-07 19:19:37.352 : <epoch: 56, iter:   4,600, lr:2.000e-04> G_loss: 3.452e-07 F_loss: 5.306e-03 D_loss: 3.467e-03 D_real: 2.564e-02 D_fake: 2.511e-02 
24-04-07 19:26:38.064 : <epoch: 58, iter:   4,800, lr:2.000e-04> G_loss: 2.911e-07 F_loss: 1.518e-03 D_loss: 3.465e-03 D_real: -4.002e+00 D_fake: -4.001e+00 
24-04-07 19:33:38.463 : <epoch: 60, iter:   5,000, lr:2.000e-04> G_loss: 9.361e-07 F_loss: 4.552e-03 D_loss: 3.492e-03 D_real: -6.594e+00 D_fake: -6.601e+00 
24-04-07 19:33:38.464 : Saving the model.
24-04-07 19:33:38.800 : ---1--> Lego Knights.h5 | 62.76dB
24-04-07 19:33:38.965 : ---2--> Tarot Cards S.h5 | 59.24dB
24-04-07 19:33:38.977 : <epoch: 60, iter:   5,000, Average PSNR : 61.00dB

24-04-07 19:40:41.829 : <epoch: 63, iter:   5,200, lr:2.000e-04> G_loss: 1.583e-06 F_loss: 1.259e-02 D_loss: 3.462e-03 D_real: -2.822e+02 D_fake: -2.822e+02 
24-04-07 19:47:47.094 : <epoch: 65, iter:   5,400, lr:2.000e-04> G_loss: 6.620e-07 F_loss: 2.959e-03 D_loss: 3.462e-03 D_real: -2.819e+02 D_fake: -2.819e+02 
24-04-07 19:54:50.913 : <epoch: 68, iter:   5,600, lr:2.000e-04> G_loss: 4.757e-07 F_loss: 2.065e-03 D_loss: 3.467e-03 D_real: -2.816e+02 D_fake: -2.816e+02 
24-04-07 20:01:55.523 : <epoch: 70, iter:   5,800, lr:2.000e-04> G_loss: 2.908e-07 F_loss: 1.399e-03 D_loss: 3.470e-03 D_real: -2.809e+02 D_fake: -2.809e+02 
24-04-07 20:21:35.216 :   task: lft_gan
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution
    pretrained_netG: superresolution/lft_gan/models/5000_G.pth
    pretrained_netD: superresolution/lft_gan/models/5000_D.pth
    pretrained_netE: superresolution/lft_gan/models/5000_E.pth
    task: superresolution/lft_gan
    log: superresolution/lft_gan
    options: superresolution/lft_gan/options
    models: superresolution/lft_gan/models
    images: superresolution/lft_gan/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_train/SR_5x5_4x/Stanford_Gantry
      dataroot_L: None
      H_size: 660
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_test/SR_5x5_4x
      dataroot_L: None
      H_size: 660
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: xavier_uniform
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_unet
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: spectral
    init_type: normal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: ragan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    D_optimizer_type: adam
    D_optimizer_lr: 1e-05
    D_optimizer_wd: 0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-07 20:21:35.232 : Number of train images: 165, iters: 83
24-04-07 20:21:38.416 : 
Networks name: get_model
Params number: 1163392
Net structure:
get_model(
  (pos_encoding): PositionEncoding()
  (conv_init0): Sequential(
    (0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
  )
  (conv_init): Sequential(
    (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (3): LeakyReLU(negative_slope=0.2, inplace=True)
    (4): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (5): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (altblock): Sequential(
    (0): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsampling): Sequential(
    (0): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): PixelShuffle(upscale_factor=4)
    (2): LeakyReLU(negative_slope=0.2)
    (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
)

Networks name: Discriminator_UNet
Params number: 4375745
Net structure:
Discriminator_UNet(
  (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

24-04-07 20:21:38.524 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.000 | -0.028 |  0.027 |  0.013 | torch.Size([64, 1, 1, 3, 3]) || conv_init0.0.weight
 |  0.000 | -0.072 |  0.075 |  0.014 | torch.Size([64, 64, 1, 3, 3]) || conv_init.0.weight
 |  0.001 | -0.060 |  0.066 |  0.013 | torch.Size([64, 64, 1, 3, 3]) || conv_init.2.weight
 | -0.001 | -0.058 |  0.067 |  0.014 | torch.Size([64, 64, 1, 3, 3]) || conv_init.4.weight
 |  0.000 | -0.045 |  0.044 |  0.013 | torch.Size([128, 576]) || altblock.0.spa_trans.MLP.weight
 |  0.986 |  0.949 |  1.013 |  0.013 | torch.Size([128]) || altblock.0.spa_trans.norm.weight
 |  0.000 | -0.028 |  0.038 |  0.014 | torch.Size([128]) || altblock.0.spa_trans.norm.bias
 |  0.000 | -0.128 |  0.128 |  0.052 | torch.Size([384, 128]) || altblock.0.spa_trans.attention.in_proj_weight
 |  0.000 | -0.061 |  0.053 |  0.020 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.out_proj.weight
 |  0.974 |  0.935 |  0.998 |  0.010 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.weight
 | -0.001 | -0.030 |  0.033 |  0.018 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.bias
 |  0.001 | -0.054 |  0.050 |  0.015 | torch.Size([256, 128]) || altblock.0.spa_trans.feed_forward.1.weight
 | -0.000 | -0.050 |  0.049 |  0.015 | torch.Size([128, 256]) || altblock.0.spa_trans.feed_forward.4.weight
 |  0.000 | -0.053 |  0.050 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.0.spa_trans.linear.0.weight
 |  1.066 |  0.993 |  1.105 |  0.022 | torch.Size([64]) || altblock.0.ang_trans.norm.weight
 | -0.004 | -0.098 |  0.105 |  0.068 | torch.Size([64]) || altblock.0.ang_trans.norm.bias
 |  0.000 | -0.215 |  0.214 |  0.078 | torch.Size([192, 64]) || altblock.0.ang_trans.attention.in_proj_weight
 |  0.000 | -0.086 |  0.083 |  0.026 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.out_proj.weight
 |  0.982 |  0.942 |  0.998 |  0.010 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.weight
 | -0.000 | -0.016 |  0.016 |  0.007 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.bias
 | -0.000 | -0.078 |  0.072 |  0.022 | torch.Size([128, 64]) || altblock.0.ang_trans.feed_forward.1.weight
 |  0.000 | -0.062 |  0.064 |  0.021 | torch.Size([64, 128]) || altblock.0.ang_trans.feed_forward.4.weight
 | -0.000 | -0.058 |  0.069 |  0.012 | torch.Size([128, 576]) || altblock.1.spa_trans.MLP.weight
 |  1.009 |  0.981 |  1.038 |  0.012 | torch.Size([128]) || altblock.1.spa_trans.norm.weight
 | -0.002 | -0.027 |  0.025 |  0.012 | torch.Size([128]) || altblock.1.spa_trans.norm.bias
 |  0.000 | -0.131 |  0.142 |  0.053 | torch.Size([384, 128]) || altblock.1.spa_trans.attention.in_proj_weight
 |  0.000 | -0.049 |  0.052 |  0.019 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.out_proj.weight
 |  0.991 |  0.974 |  1.013 |  0.008 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.weight
 |  0.000 | -0.012 |  0.012 |  0.005 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.bias
 |  0.000 | -0.061 |  0.049 |  0.015 | torch.Size([256, 128]) || altblock.1.spa_trans.feed_forward.1.weight
 | -0.000 | -0.053 |  0.045 |  0.015 | torch.Size([128, 256]) || altblock.1.spa_trans.feed_forward.4.weight
 | -0.000 | -0.050 |  0.045 |  0.021 | torch.Size([64, 128, 1, 1, 1]) || altblock.1.spa_trans.linear.0.weight
 |  1.012 |  0.977 |  1.056 |  0.018 | torch.Size([64]) || altblock.1.ang_trans.norm.weight
 | -0.000 | -0.034 |  0.031 |  0.019 | torch.Size([64]) || altblock.1.ang_trans.norm.bias
 | -0.000 | -0.170 |  0.192 |  0.074 | torch.Size([192, 64]) || altblock.1.ang_trans.attention.in_proj_weight
 |  0.001 | -0.060 |  0.064 |  0.025 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.out_proj.weight
 |  0.979 |  0.958 |  0.998 |  0.009 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.weight
 | -0.000 | -0.016 |  0.014 |  0.007 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.bias
 | -0.000 | -0.057 |  0.056 |  0.020 | torch.Size([128, 64]) || altblock.1.ang_trans.feed_forward.1.weight
 |  0.000 | -0.049 |  0.050 |  0.020 | torch.Size([64, 128]) || altblock.1.ang_trans.feed_forward.4.weight
 |  0.000 | -0.044 |  0.044 |  0.012 | torch.Size([128, 576]) || altblock.2.spa_trans.MLP.weight
 |  1.028 |  0.974 |  1.078 |  0.019 | torch.Size([128]) || altblock.2.spa_trans.norm.weight
 |  0.001 | -0.063 |  0.053 |  0.030 | torch.Size([128]) || altblock.2.spa_trans.norm.bias
 | -0.000 | -0.160 |  0.160 |  0.054 | torch.Size([384, 128]) || altblock.2.spa_trans.attention.in_proj_weight
 |  0.000 | -0.059 |  0.056 |  0.019 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.out_proj.weight
 |  0.985 |  0.956 |  0.998 |  0.007 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.weight
 |  0.000 | -0.017 |  0.014 |  0.005 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.bias
 | -0.000 | -0.047 |  0.048 |  0.015 | torch.Size([256, 128]) || altblock.2.spa_trans.feed_forward.1.weight
 | -0.000 | -0.042 |  0.039 |  0.015 | torch.Size([128, 256]) || altblock.2.spa_trans.feed_forward.4.weight
 | -0.000 | -0.047 |  0.048 |  0.020 | torch.Size([64, 128, 1, 1, 1]) || altblock.2.spa_trans.linear.0.weight
 |  1.034 |  0.984 |  1.074 |  0.017 | torch.Size([64]) || altblock.2.ang_trans.norm.weight
 | -0.006 | -0.046 |  0.051 |  0.034 | torch.Size([64]) || altblock.2.ang_trans.norm.bias
 |  0.000 | -0.200 |  0.190 |  0.076 | torch.Size([192, 64]) || altblock.2.ang_trans.attention.in_proj_weight
 |  0.001 | -0.064 |  0.058 |  0.026 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.out_proj.weight
 |  0.989 |  0.978 |  1.001 |  0.005 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.weight
 | -0.000 | -0.006 |  0.004 |  0.002 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.bias
 |  0.000 | -0.045 |  0.048 |  0.020 | torch.Size([128, 64]) || altblock.2.ang_trans.feed_forward.1.weight
 |  0.000 | -0.046 |  0.047 |  0.020 | torch.Size([64, 128]) || altblock.2.ang_trans.feed_forward.4.weight
 | -0.000 | -0.038 |  0.039 |  0.012 | torch.Size([128, 576]) || altblock.3.spa_trans.MLP.weight
 |  1.038 |  0.988 |  1.084 |  0.021 | torch.Size([128]) || altblock.3.spa_trans.norm.weight
 | -0.001 | -0.058 |  0.062 |  0.029 | torch.Size([128]) || altblock.3.spa_trans.norm.bias
 | -0.000 | -0.159 |  0.166 |  0.055 | torch.Size([384, 128]) || altblock.3.spa_trans.attention.in_proj_weight
 |  0.000 | -0.056 |  0.056 |  0.019 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.out_proj.weight
 |  0.985 |  0.962 |  1.004 |  0.008 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.weight
 |  0.001 | -0.019 |  0.024 |  0.009 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.bias
 | -0.000 | -0.049 |  0.053 |  0.015 | torch.Size([256, 128]) || altblock.3.spa_trans.feed_forward.1.weight
 |  0.000 | -0.057 |  0.047 |  0.015 | torch.Size([128, 256]) || altblock.3.spa_trans.feed_forward.4.weight
 |  0.000 | -0.045 |  0.045 |  0.019 | torch.Size([64, 128, 1, 1, 1]) || altblock.3.spa_trans.linear.0.weight
 |  1.051 |  0.997 |  1.094 |  0.020 | torch.Size([64]) || altblock.3.ang_trans.norm.weight
 |  0.002 | -0.061 |  0.062 |  0.039 | torch.Size([64]) || altblock.3.ang_trans.norm.bias
 |  0.001 | -0.229 |  0.229 |  0.078 | torch.Size([192, 64]) || altblock.3.ang_trans.attention.in_proj_weight
 |  0.000 | -0.064 |  0.059 |  0.026 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.out_proj.weight
 |  0.982 |  0.966 |  0.999 |  0.007 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.weight
 |  0.000 | -0.015 |  0.017 |  0.006 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.bias
 |  0.000 | -0.052 |  0.059 |  0.021 | torch.Size([128, 64]) || altblock.3.ang_trans.feed_forward.1.weight
 | -0.001 | -0.061 |  0.056 |  0.020 | torch.Size([64, 128]) || altblock.3.ang_trans.feed_forward.4.weight
 | -0.000 | -0.044 |  0.041 |  0.009 | torch.Size([1024, 64, 1, 1]) || upsampling.0.weight
 |  0.000 | -0.021 |  0.017 |  0.005 | torch.Size([1, 64, 3, 3]) || upsampling.3.weight

24-04-07 20:28:30.849 : <epoch:  2, iter:   5,200, lr:2.000e-04> G_loss: 6.012e-07 F_loss: 2.304e-03 D_loss: 3.477e-03 D_real: -1.133e+01 D_fake: -1.133e+01 
24-04-07 20:35:34.727 : <epoch:  4, iter:   5,400, lr:2.000e-04> G_loss: 8.217e-07 F_loss: 3.689e-03 D_loss: 3.524e-03 D_real: -1.222e+02 D_fake: -1.222e+02 
24-04-07 20:42:38.875 : <epoch:  7, iter:   5,600, lr:2.000e-04> G_loss: 8.096e-07 F_loss: 2.077e-03 D_loss: 3.459e-03 D_real: -1.204e+02 D_fake: -1.204e+02 
24-04-07 20:49:42.610 : <epoch:  9, iter:   5,800, lr:2.000e-04> G_loss: 9.662e-07 F_loss: 3.161e-03 D_loss: 3.461e-03 D_real: -1.202e+02 D_fake: -1.202e+02 
24-04-07 20:56:46.613 : <epoch: 12, iter:   6,000, lr:2.000e-04> G_loss: 4.523e-07 F_loss: 1.586e-03 D_loss: 3.460e-03 D_real: -1.199e+02 D_fake: -1.199e+02 
24-04-07 21:03:50.408 : <epoch: 14, iter:   6,200, lr:2.000e-04> G_loss: 8.963e-07 F_loss: 2.721e-03 D_loss: 3.463e-03 D_real: -1.196e+02 D_fake: -1.196e+02 
24-04-07 21:10:54.473 : <epoch: 17, iter:   6,400, lr:2.000e-04> G_loss: 6.260e-07 F_loss: 2.929e-03 D_loss: 3.453e-03 D_real: -1.192e+02 D_fake: -1.192e+02 
24-04-07 21:17:58.336 : <epoch: 19, iter:   6,600, lr:2.000e-04> G_loss: 2.000e-07 F_loss: 8.614e-04 D_loss: 3.460e-03 D_real: -1.190e+02 D_fake: -1.190e+02 
24-04-07 21:25:02.078 : <epoch: 21, iter:   6,800, lr:2.000e-04> G_loss: 6.883e-07 F_loss: 2.520e-03 D_loss: 3.464e-03 D_real: -1.187e+02 D_fake: -1.187e+02 
24-04-07 21:32:06.137 : <epoch: 24, iter:   7,000, lr:2.000e-04> G_loss: 6.618e-07 F_loss: 1.382e-03 D_loss: 3.465e-03 D_real: -1.182e+02 D_fake: -1.182e+02 
24-04-07 21:39:10.003 : <epoch: 26, iter:   7,200, lr:2.000e-04> G_loss: 5.980e-07 F_loss: 2.728e-03 D_loss: 3.469e-03 D_real: -1.176e+02 D_fake: -1.176e+02 
24-04-07 21:46:13.932 : <epoch: 29, iter:   7,400, lr:2.000e-04> G_loss: 1.333e-07 F_loss: 4.431e-04 D_loss: 3.464e-03 D_real: -1.173e+02 D_fake: -1.173e+02 
24-04-07 21:53:17.755 : <epoch: 31, iter:   7,600, lr:2.000e-04> G_loss: 1.585e-07 F_loss: 5.449e-04 D_loss: 3.467e-03 D_real: -1.165e+02 D_fake: -1.165e+02 
24-04-07 22:00:21.479 : <epoch: 34, iter:   7,800, lr:2.000e-04> G_loss: 2.351e-07 F_loss: 8.511e-04 D_loss: 3.464e-03 D_real: -1.161e+02 D_fake: -1.161e+02 
24-04-07 22:07:25.429 : <epoch: 36, iter:   8,000, lr:2.000e-04> G_loss: 1.545e-06 F_loss: 5.807e-03 D_loss: 3.461e-03 D_real: -1.161e+02 D_fake: -1.161e+02 
24-04-07 22:14:29.150 : <epoch: 39, iter:   8,200, lr:2.000e-04> G_loss: 8.208e-07 F_loss: 3.396e-03 D_loss: 3.466e-03 D_real: -1.152e+02 D_fake: -1.152e+02 
24-04-07 22:21:33.101 : <epoch: 41, iter:   8,400, lr:2.000e-04> G_loss: 3.093e-07 F_loss: 1.774e-03 D_loss: 3.463e-03 D_real: -1.141e+02 D_fake: -1.141e+02 
24-04-07 22:28:36.862 : <epoch: 43, iter:   8,600, lr:2.000e-04> G_loss: 5.805e-07 F_loss: 2.125e-03 D_loss: 3.470e-03 D_real: -1.132e+02 D_fake: -1.132e+02 
24-04-07 22:35:40.750 : <epoch: 46, iter:   8,800, lr:2.000e-04> G_loss: 1.019e-06 F_loss: 3.689e-03 D_loss: 3.466e-03 D_real: -1.119e+02 D_fake: -1.119e+02 
24-04-07 22:42:44.292 : <epoch: 48, iter:   9,000, lr:2.000e-04> G_loss: 8.539e-07 F_loss: 2.753e-03 D_loss: 3.467e-03 D_real: -1.118e+02 D_fake: -1.118e+02 
24-04-07 22:49:48.157 : <epoch: 51, iter:   9,200, lr:2.000e-04> G_loss: 2.093e-07 F_loss: 8.286e-04 D_loss: 3.466e-03 D_real: -1.123e+02 D_fake: -1.123e+02 
24-04-07 22:56:52.012 : <epoch: 53, iter:   9,400, lr:2.000e-04> G_loss: 1.548e-07 F_loss: 4.012e-04 D_loss: 3.465e-03 D_real: -1.119e+02 D_fake: -1.119e+02 
24-04-07 23:03:55.687 : <epoch: 56, iter:   9,600, lr:2.000e-04> G_loss: 9.097e-07 F_loss: 3.900e-03 D_loss: 3.463e-03 D_real: -1.109e+02 D_fake: -1.109e+02 
24-04-07 23:10:59.204 : <epoch: 58, iter:   9,800, lr:2.000e-04> G_loss: 8.190e-07 F_loss: 3.488e-03 D_loss: 3.467e-03 D_real: -1.101e+02 D_fake: -1.101e+02 
24-04-07 23:18:02.842 : <epoch: 60, iter:  10,000, lr:2.000e-04> G_loss: 2.902e-07 F_loss: 9.754e-04 D_loss: 3.466e-03 D_real: -1.081e+02 D_fake: -1.081e+02 
24-04-07 23:18:02.842 : Saving the model.
24-04-07 23:18:03.200 : ---1--> Lego Knights.h5 | 64.15dB
24-04-07 23:18:03.378 : ---2--> Tarot Cards S.h5 | 59.04dB
24-04-07 23:18:03.392 : <epoch: 60, iter:  10,000, Average PSNR : 61.60dB

24-04-07 23:25:07.477 : <epoch: 63, iter:  10,200, lr:2.000e-04> G_loss: 2.483e-07 F_loss: 8.510e-04 D_loss: 3.473e-03 D_real: -1.090e+02 D_fake: -1.090e+02 
24-04-07 23:32:11.027 : <epoch: 65, iter:  10,400, lr:2.000e-04> G_loss: 1.542e-07 F_loss: 6.014e-04 D_loss: 3.475e-03 D_real: -1.101e+02 D_fake: -1.101e+02 
24-04-07 23:39:14.965 : <epoch: 68, iter:  10,600, lr:2.000e-04> G_loss: 8.959e-07 F_loss: 3.927e-03 D_loss: 3.468e-03 D_real: -1.109e+02 D_fake: -1.109e+02 
24-04-07 23:46:18.756 : <epoch: 70, iter:  10,800, lr:2.000e-04> G_loss: 1.375e-06 F_loss: 4.628e-03 D_loss: 3.469e-03 D_real: -1.111e+02 D_fake: -1.111e+02 
24-04-07 23:53:22.627 : <epoch: 73, iter:  11,000, lr:2.000e-04> G_loss: 9.002e-07 F_loss: 2.905e-03 D_loss: 3.471e-03 D_real: -1.084e+02 D_fake: -1.084e+02 
24-04-08 00:00:26.631 : <epoch: 75, iter:  11,200, lr:2.000e-04> G_loss: 1.516e-07 F_loss: 5.432e-04 D_loss: 3.467e-03 D_real: -1.061e+02 D_fake: -1.061e+02 
24-04-08 00:07:30.987 : <epoch: 78, iter:  11,400, lr:2.000e-04> G_loss: 7.309e-07 F_loss: 2.842e-03 D_loss: 3.465e-03 D_real: -1.075e+02 D_fake: -1.075e+02 
24-04-08 00:14:34.833 : <epoch: 80, iter:  11,600, lr:2.000e-04> G_loss: 6.316e-07 F_loss: 2.803e-03 D_loss: 3.467e-03 D_real: -1.072e+02 D_fake: -1.072e+02 
24-04-08 00:21:38.848 : <epoch: 82, iter:  11,800, lr:2.000e-04> G_loss: 6.476e-07 F_loss: 1.812e-03 D_loss: 3.478e-03 D_real: -1.057e+02 D_fake: -1.057e+02 
24-04-08 00:28:42.778 : <epoch: 85, iter:  12,000, lr:2.000e-04> G_loss: 5.315e-07 F_loss: 2.191e-03 D_loss: 3.465e-03 D_real: -1.046e+02 D_fake: -1.046e+02 
24-04-08 00:35:46.448 : <epoch: 87, iter:  12,200, lr:2.000e-04> G_loss: 9.691e-07 F_loss: 3.294e-03 D_loss: 3.468e-03 D_real: -1.025e+02 D_fake: -1.025e+02 
24-04-08 00:42:50.335 : <epoch: 90, iter:  12,400, lr:2.000e-04> G_loss: 8.914e-07 F_loss: 2.559e-03 D_loss: 3.466e-03 D_real: -1.029e+02 D_fake: -1.029e+02 
24-04-08 00:49:54.008 : <epoch: 92, iter:  12,600, lr:2.000e-04> G_loss: 5.427e-07 F_loss: 3.841e-03 D_loss: 3.463e-03 D_real: -9.907e+01 D_fake: -9.907e+01 
24-04-08 00:56:57.578 : <epoch: 95, iter:  12,800, lr:2.000e-04> G_loss: 1.274e-07 F_loss: 2.870e-04 D_loss: 3.467e-03 D_real: -9.892e+01 D_fake: -9.893e+01 
24-04-08 01:04:01.172 : <epoch: 97, iter:  13,000, lr:2.000e-04> G_loss: 5.721e-07 F_loss: 2.318e-03 D_loss: 3.459e-03 D_real: -1.002e+02 D_fake: -1.002e+02 
24-04-08 01:11:04.799 : <epoch: 99, iter:  13,200, lr:2.000e-04> G_loss: 2.847e-07 F_loss: 1.163e-03 D_loss: 3.464e-03 D_real: -1.006e+02 D_fake: -1.006e+02 
24-04-08 01:18:08.840 : <epoch:102, iter:  13,400, lr:2.000e-04> G_loss: 2.701e-07 F_loss: 9.486e-04 D_loss: 3.464e-03 D_real: -1.038e+02 D_fake: -1.038e+02 
24-04-08 01:25:12.559 : <epoch:104, iter:  13,600, lr:2.000e-04> G_loss: 8.835e-07 F_loss: 2.139e-03 D_loss: 3.496e-03 D_real: -1.009e+02 D_fake: -1.009e+02 
24-04-08 01:32:16.511 : <epoch:107, iter:  13,800, lr:2.000e-04> G_loss: 1.661e-07 F_loss: 3.634e-04 D_loss: 3.467e-03 D_real: -9.651e+01 D_fake: -9.651e+01 
24-04-08 01:39:20.164 : <epoch:109, iter:  14,000, lr:2.000e-04> G_loss: 1.335e-07 F_loss: 3.585e-04 D_loss: 3.472e-03 D_real: -9.494e+01 D_fake: -9.494e+01 
24-04-08 01:46:24.118 : <epoch:112, iter:  14,200, lr:2.000e-04> G_loss: 6.481e-07 F_loss: 2.116e-03 D_loss: 3.465e-03 D_real: -9.526e+01 D_fake: -9.526e+01 
24-04-08 01:53:27.442 : <epoch:114, iter:  14,400, lr:2.000e-04> G_loss: 1.093e-07 F_loss: 2.656e-04 D_loss: 3.466e-03 D_real: -9.282e+01 D_fake: -9.282e+01 
24-04-08 02:00:31.072 : <epoch:117, iter:  14,600, lr:2.000e-04> G_loss: 1.517e-07 F_loss: 4.955e-04 D_loss: 3.466e-03 D_real: -9.685e+01 D_fake: -9.685e+01 
24-04-08 02:07:34.736 : <epoch:119, iter:  14,800, lr:2.000e-04> G_loss: 4.640e-07 F_loss: 1.494e-03 D_loss: 3.465e-03 D_real: -9.805e+01 D_fake: -9.805e+01 
24-04-08 02:14:37.859 : <epoch:121, iter:  15,000, lr:2.000e-04> G_loss: 1.186e-07 F_loss: 4.862e-04 D_loss: 3.465e-03 D_real: -1.023e+02 D_fake: -1.023e+02 
24-04-08 02:14:37.859 : Saving the model.
24-04-08 02:14:38.198 : ---1--> Lego Knights.h5 | 63.54dB
24-04-08 02:14:38.387 : ---2--> Tarot Cards S.h5 | 59.30dB
24-04-08 02:14:38.402 : <epoch:121, iter:  15,000, Average PSNR : 61.42dB

24-04-08 02:21:41.805 : <epoch:124, iter:  15,200, lr:2.000e-04> G_loss: 7.839e-07 F_loss: 2.134e-03 D_loss: 3.471e-03 D_real: -1.104e+02 D_fake: -1.104e+02 
24-04-08 02:28:45.333 : <epoch:126, iter:  15,400, lr:2.000e-04> G_loss: 5.148e-07 F_loss: 1.641e-03 D_loss: 3.477e-03 D_real: -1.172e+02 D_fake: -1.172e+02 
24-04-08 02:35:49.750 : <epoch:129, iter:  15,600, lr:2.000e-04> G_loss: 9.558e-07 F_loss: 4.119e-03 D_loss: 3.469e-03 D_real: -1.311e+02 D_fake: -1.311e+02 
24-04-08 02:42:53.270 : <epoch:131, iter:  15,800, lr:2.000e-04> G_loss: 1.426e-07 F_loss: 4.615e-04 D_loss: 3.468e-03 D_real: -1.431e+02 D_fake: -1.431e+02 
24-04-08 02:49:56.922 : <epoch:134, iter:  16,000, lr:2.000e-04> G_loss: 3.327e-07 F_loss: 4.300e-04 D_loss: 3.515e-03 D_real: -6.668e+02 D_fake: -6.668e+02 
24-04-08 02:57:00.225 : <epoch:136, iter:  16,200, lr:2.000e-04> G_loss: 9.128e-07 F_loss: 3.224e-03 D_loss: 3.458e-03 D_real: -6.663e+02 D_fake: -6.663e+02 
24-04-08 03:04:03.597 : <epoch:139, iter:  16,400, lr:2.000e-04> G_loss: 1.033e-06 F_loss: 4.617e-03 D_loss: 3.459e-03 D_real: -6.662e+02 D_fake: -6.662e+02 
24-04-08 03:11:07.258 : <epoch:141, iter:  16,600, lr:2.000e-04> G_loss: 9.016e-07 F_loss: 2.346e-03 D_loss: 3.465e-03 D_real: -6.665e+02 D_fake: -6.665e+02 
24-04-08 03:18:10.626 : <epoch:143, iter:  16,800, lr:2.000e-04> G_loss: 1.711e-07 F_loss: 5.114e-04 D_loss: 3.467e-03 D_real: -6.659e+02 D_fake: -6.659e+02 
24-04-08 03:25:14.212 : <epoch:146, iter:  17,000, lr:2.000e-04> G_loss: 7.979e-07 F_loss: 1.967e-03 D_loss: 3.468e-03 D_real: -6.667e+02 D_fake: -6.667e+02 
24-04-08 03:32:17.758 : <epoch:148, iter:  17,200, lr:2.000e-04> G_loss: 1.271e-07 F_loss: 2.469e-04 D_loss: 3.492e-03 D_real: -6.664e+02 D_fake: -6.664e+02 
24-04-08 03:39:21.581 : <epoch:151, iter:  17,400, lr:2.000e-04> G_loss: 1.842e-07 F_loss: 5.932e-04 D_loss: 3.467e-03 D_real: -6.669e+02 D_fake: -6.669e+02 
24-04-08 03:46:25.221 : <epoch:153, iter:  17,600, lr:2.000e-04> G_loss: 5.030e-07 F_loss: 1.471e-03 D_loss: 3.461e-03 D_real: -6.686e+02 D_fake: -6.686e+02 
24-04-08 03:53:28.995 : <epoch:156, iter:  17,800, lr:2.000e-04> G_loss: 8.747e-07 F_loss: 1.717e-03 D_loss: 3.472e-03 D_real: -6.676e+02 D_fake: -6.676e+02 
24-04-08 04:00:32.896 : <epoch:158, iter:  18,000, lr:2.000e-04> G_loss: 8.546e-07 F_loss: 1.973e-03 D_loss: 3.463e-03 D_real: -6.672e+02 D_fake: -6.672e+02 
24-04-08 04:07:36.155 : <epoch:160, iter:  18,200, lr:2.000e-04> G_loss: 2.483e-07 F_loss: 5.860e-04 D_loss: 3.449e-03 D_real: -6.659e+02 D_fake: -6.659e+02 
24-04-08 04:14:39.926 : <epoch:163, iter:  18,400, lr:2.000e-04> G_loss: 1.520e-07 F_loss: 3.988e-04 D_loss: 3.488e-03 D_real: -6.658e+02 D_fake: -6.658e+02 
24-04-08 04:21:43.541 : <epoch:165, iter:  18,600, lr:2.000e-04> G_loss: 1.124e-06 F_loss: 2.775e-03 D_loss: 3.473e-03 D_real: -6.656e+02 D_fake: -6.656e+02 
24-04-08 04:28:47.555 : <epoch:168, iter:  18,800, lr:2.000e-04> G_loss: 3.127e-07 F_loss: 1.270e-03 D_loss: 3.467e-03 D_real: -6.660e+02 D_fake: -6.660e+02 
24-04-08 04:35:51.215 : <epoch:170, iter:  19,000, lr:2.000e-04> G_loss: 2.771e-07 F_loss: 8.887e-04 D_loss: 3.467e-03 D_real: -6.671e+02 D_fake: -6.671e+02 
24-04-08 04:42:55.087 : <epoch:173, iter:  19,200, lr:2.000e-04> G_loss: 4.768e-07 F_loss: 2.782e-03 D_loss: 3.468e-03 D_real: -6.660e+02 D_fake: -6.660e+02 
24-04-08 04:49:58.653 : <epoch:175, iter:  19,400, lr:2.000e-04> G_loss: 1.405e-07 F_loss: 5.106e-04 D_loss: 3.474e-03 D_real: -6.638e+02 D_fake: -6.638e+02 
24-04-08 04:57:02.404 : <epoch:178, iter:  19,600, lr:2.000e-04> G_loss: 7.115e-07 F_loss: 2.463e-03 D_loss: 3.465e-03 D_real: -6.634e+02 D_fake: -6.634e+02 
24-04-08 05:04:06.062 : <epoch:180, iter:  19,800, lr:2.000e-04> G_loss: 3.140e-07 F_loss: 1.203e-03 D_loss: 3.465e-03 D_real: -6.659e+02 D_fake: -6.659e+02 
24-04-08 05:11:10.441 : <epoch:182, iter:  20,000, lr:2.000e-04> G_loss: 5.000e-07 F_loss: 1.243e-03 D_loss: 3.472e-03 D_real: -6.651e+02 D_fake: -6.651e+02 
24-04-08 05:11:10.441 : Saving the model.
24-04-08 05:11:10.768 : ---1--> Lego Knights.h5 | 63.35dB
24-04-08 05:11:10.939 : ---2--> Tarot Cards S.h5 | 58.75dB
24-04-08 05:11:10.958 : <epoch:182, iter:  20,000, Average PSNR : 61.05dB

24-04-08 05:18:15.000 : <epoch:185, iter:  20,200, lr:2.000e-04> G_loss: 3.375e-07 F_loss: 1.231e-03 D_loss: 3.470e-03 D_real: -6.661e+02 D_fake: -6.661e+02 
24-04-08 05:25:18.708 : <epoch:187, iter:  20,400, lr:2.000e-04> G_loss: 3.586e-07 F_loss: 1.465e-03 D_loss: 3.483e-03 D_real: -6.623e+02 D_fake: -6.624e+02 
24-04-08 05:32:22.711 : <epoch:190, iter:  20,600, lr:2.000e-04> G_loss: 5.304e-07 F_loss: 2.812e-03 D_loss: 3.469e-03 D_real: -6.651e+02 D_fake: -6.651e+02 
24-04-08 05:39:26.359 : <epoch:192, iter:  20,800, lr:2.000e-04> G_loss: 1.038e-06 F_loss: 2.935e-03 D_loss: 3.468e-03 D_real: -6.649e+02 D_fake: -6.649e+02 
24-04-08 05:46:30.038 : <epoch:195, iter:  21,000, lr:2.000e-04> G_loss: 1.363e-07 F_loss: 4.013e-04 D_loss: 3.465e-03 D_real: -6.592e+02 D_fake: -6.592e+02 
24-04-08 05:53:33.498 : <epoch:197, iter:  21,200, lr:2.000e-04> G_loss: 2.428e-07 F_loss: 9.190e-04 D_loss: 3.467e-03 D_real: -6.550e+02 D_fake: -6.550e+02 
24-04-08 06:00:36.755 : <epoch:199, iter:  21,400, lr:2.000e-04> G_loss: 5.837e-07 F_loss: 2.072e-03 D_loss: 3.470e-03 D_real: -6.487e+02 D_fake: -6.487e+02 
24-04-08 06:07:40.265 : <epoch:202, iter:  21,600, lr:2.000e-04> G_loss: 1.284e-07 F_loss: 4.212e-04 D_loss: 3.467e-03 D_real: -6.388e+02 D_fake: -6.388e+02 
24-04-08 06:14:43.785 : <epoch:204, iter:  21,800, lr:2.000e-04> G_loss: 7.908e-07 F_loss: 2.256e-03 D_loss: 3.465e-03 D_real: -6.390e+02 D_fake: -6.390e+02 
24-04-08 06:21:47.605 : <epoch:207, iter:  22,000, lr:2.000e-04> G_loss: 3.794e-07 F_loss: 8.686e-04 D_loss: 3.528e-03 D_real: -6.573e+02 D_fake: -6.573e+02 
24-04-08 06:28:51.100 : <epoch:209, iter:  22,200, lr:2.000e-04> G_loss: 1.185e-06 F_loss: 2.590e-03 D_loss: 3.467e-03 D_real: -6.613e+02 D_fake: -6.613e+02 
24-04-08 06:35:54.786 : <epoch:212, iter:  22,400, lr:2.000e-04> G_loss: 4.658e-07 F_loss: 1.164e-03 D_loss: 3.468e-03 D_real: -6.628e+02 D_fake: -6.628e+02 
24-04-08 06:42:57.959 : <epoch:214, iter:  22,600, lr:2.000e-04> G_loss: 8.151e-07 F_loss: 2.293e-03 D_loss: 3.463e-03 D_real: -6.551e+02 D_fake: -6.551e+02 
24-04-08 06:50:00.055 : <epoch:217, iter:  22,800, lr:2.000e-04> G_loss: 5.864e-07 F_loss: 1.802e-03 D_loss: 3.467e-03 D_real: -6.525e+02 D_fake: -6.525e+02 
24-04-08 06:57:02.209 : <epoch:219, iter:  23,000, lr:2.000e-04> G_loss: 8.813e-07 F_loss: 4.767e-03 D_loss: 3.457e-03 D_real: -6.490e+02 D_fake: -6.490e+02 
24-04-08 07:04:03.844 : <epoch:221, iter:  23,200, lr:2.000e-04> G_loss: 2.910e-07 F_loss: 1.097e-03 D_loss: 3.465e-03 D_real: -6.584e+02 D_fake: -6.584e+02 
24-04-08 07:11:06.062 : <epoch:224, iter:  23,400, lr:2.000e-04> G_loss: 8.447e-07 F_loss: 2.421e-03 D_loss: 3.461e-03 D_real: -6.562e+02 D_fake: -6.562e+02 
24-04-08 07:18:08.177 : <epoch:226, iter:  23,600, lr:2.000e-04> G_loss: 6.467e-07 F_loss: 1.731e-03 D_loss: 3.471e-03 D_real: -6.367e+02 D_fake: -6.367e+02 
24-04-08 07:25:10.369 : <epoch:229, iter:  23,800, lr:2.000e-04> G_loss: 3.736e-07 F_loss: 1.504e-03 D_loss: 3.463e-03 D_real: -6.468e+02 D_fake: -6.468e+02 
24-04-08 07:32:12.766 : <epoch:231, iter:  24,000, lr:2.000e-04> G_loss: 4.812e-07 F_loss: 1.172e-03 D_loss: 3.492e-03 D_real: -6.449e+02 D_fake: -6.449e+02 
24-04-08 07:39:15.611 : <epoch:234, iter:  24,200, lr:2.000e-04> G_loss: 6.446e-07 F_loss: 1.631e-03 D_loss: 3.467e-03 D_real: -6.605e+02 D_fake: -6.605e+02 
24-04-08 07:46:19.296 : <epoch:236, iter:  24,400, lr:2.000e-04> G_loss: 6.209e-07 F_loss: 1.993e-03 D_loss: 3.470e-03 D_real: -6.489e+02 D_fake: -6.489e+02 
24-04-08 07:53:22.718 : <epoch:239, iter:  24,600, lr:2.000e-04> G_loss: 9.924e-07 F_loss: 1.925e-03 D_loss: 3.469e-03 D_real: -6.504e+02 D_fake: -6.504e+02 
24-04-08 08:00:26.193 : <epoch:241, iter:  24,800, lr:2.000e-04> G_loss: 3.514e-07 F_loss: 1.180e-03 D_loss: 3.466e-03 D_real: -6.740e+02 D_fake: -6.740e+02 
24-04-08 08:07:29.995 : <epoch:243, iter:  25,000, lr:2.000e-04> G_loss: 1.556e-07 F_loss: 3.855e-04 D_loss: 3.465e-03 D_real: -6.673e+02 D_fake: -6.673e+02 
24-04-08 08:07:29.996 : Saving the model.
24-04-08 08:07:30.374 : ---1--> Lego Knights.h5 | 63.60dB
24-04-08 08:07:30.556 : ---2--> Tarot Cards S.h5 | 59.31dB
24-04-08 08:07:30.574 : <epoch:243, iter:  25,000, Average PSNR : 61.46dB

24-04-08 08:14:34.198 : <epoch:246, iter:  25,200, lr:2.000e-04> G_loss: 4.853e-07 F_loss: 1.176e-03 D_loss: 3.477e-03 D_real: -6.755e+02 D_fake: -6.755e+02 
24-04-08 08:21:37.873 : <epoch:248, iter:  25,400, lr:2.000e-04> G_loss: 2.404e-07 F_loss: 8.483e-04 D_loss: 3.465e-03 D_real: -6.636e+02 D_fake: -6.636e+02 
24-04-08 08:28:41.539 : <epoch:251, iter:  25,600, lr:2.000e-04> G_loss: 1.828e-07 F_loss: 4.514e-04 D_loss: 3.492e-03 D_real: -7.380e+02 D_fake: -7.380e+02 
24-04-08 08:35:45.250 : <epoch:253, iter:  25,800, lr:2.000e-04> G_loss: 6.842e-07 F_loss: 3.376e-03 D_loss: 3.473e-03 D_real: -7.674e+02 D_fake: -7.674e+02 
24-04-08 08:42:49.200 : <epoch:256, iter:  26,000, lr:2.000e-04> G_loss: 9.522e-07 F_loss: 2.811e-03 D_loss: 3.460e-03 D_real: -7.831e+02 D_fake: -7.831e+02 
24-04-08 08:49:53.133 : <epoch:258, iter:  26,200, lr:2.000e-04> G_loss: 1.669e-07 F_loss: 4.301e-04 D_loss: 3.468e-03 D_real: -7.633e+02 D_fake: -7.633e+02 
24-04-08 08:56:56.829 : <epoch:260, iter:  26,400, lr:2.000e-04> G_loss: 7.597e-07 F_loss: 2.731e-03 D_loss: 3.467e-03 D_real: -7.886e+02 D_fake: -7.886e+02 
24-04-08 09:19:59.734 :   task: lft_gan
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution
    pretrained_netG: superresolution/lft_gan/models/25000_G.pth
    pretrained_netD: superresolution/lft_gan/models/25000_D.pth
    pretrained_netE: superresolution/lft_gan/models/25000_E.pth
    task: superresolution/lft_gan
    log: superresolution/lft_gan
    options: superresolution/lft_gan/options
    models: superresolution/lft_gan/models
    images: superresolution/lft_gan/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_train/SR_5x5_4x/Stanford_Gantry
      dataroot_L: None
      H_size: 660
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr-lf
      dataroot_H: /home/ozkan/works/diff-smoe/LFT/data_for_test/SR_5x5_4x
      dataroot_L: None
      H_size: 660
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: xavier_uniform
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_vgg_192
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: spectral
    init_type: xavier_normal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: ssim
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: softplusgan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 0.001
    G_optimizer_wd: 0
    D_optimizer_type: adam
    D_optimizer_lr: 1e-05
    D_optimizer_wd: 0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-08 09:19:59.752 : Number of train images: 165, iters: 83
