24-04-20 17:55:14.180 :   task: lft_gan_unet
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution/lft_gan_unet
    log: superresolution/lft_gan_unet
    options: superresolution/lft_gan_unet/options
    models: superresolution/lft_gan_unet/models
    images: superresolution/lft_gan_unet/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /mnt/d/LF/data_for_train
      data_name: ALL
      angRes: 5
      dataroot_L: None
      H_size: 660
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr-lf
      dataroot_H: /mnt/d/LF/data_for_test
      angRes: 5
      dataroot_L: None
      data_name: ALL
      H_size: 660
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: kaiming_normal
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_unet
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: 3
    init_type: kaiming_normal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: l2sum
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: gan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    G_scheduler_T_max: 2000000
    D_scheduler_T_max: 2000000
    G_scheduler_eta_min: 1e-07
    D_scheduler_eta_min: 1e-07
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 0.001
    G_optimizer_wd: 0
    G_clip_value: 1.0
    D_optimizer_type: adam
    D_optimizer_lr: 0.001
    D_optimizer_wd: 0
    D_clip_value: 1.0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-20 17:55:14.254 : Number of train images: 1,820, iters: 910
24-04-20 17:56:17.871 :   task: lft_gan_unet
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution/lft_gan_unet
    log: superresolution/lft_gan_unet
    options: superresolution/lft_gan_unet/options
    models: superresolution/lft_gan_unet/models
    images: superresolution/lft_gan_unet/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /mnt/d/LF/data_for_train
      data_name: ALL
      angRes: 5
      dataroot_L: None
      H_size: 660
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr-lf
      dataroot_H: /mnt/d/LF/data_for_test
      angRes: 5
      dataroot_L: None
      data_name: ALL
      H_size: 660
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: kaiming_normal
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_unet
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: 3
    init_type: kaiming_normal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: l2sum
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: gan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    G_scheduler_T_max: 2000000
    D_scheduler_T_max: 2000000
    G_scheduler_eta_min: 1e-07
    D_scheduler_eta_min: 1e-07
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 0.001
    G_optimizer_wd: 0
    G_clip_value: 1.0
    D_optimizer_type: adam
    D_optimizer_lr: 0.001
    D_optimizer_wd: 0
    D_clip_value: 1.0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-20 17:56:17.909 : Number of train images: 1,820, iters: 910
24-04-20 17:56:20.165 : 
Networks name: LFT
Params number: 1163792
Net structure:
LFT(
  (pos_encoding): PositionEncoding()
  (conv_init0): Sequential(
    (0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
  )
  (conv_init): Sequential(
    (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
    (3): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): LeakyReLU(negative_slope=0.2, inplace=True)
    (6): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (7): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (altblock): Sequential(
    (0): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsampling): Sequential(
    (0): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): PixelShuffle(upscale_factor=4)
    (2): LeakyReLU(negative_slope=0.2)
    (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (resizer): MullerResizer(
    (weights): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (1): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (2): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (3): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (4): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (5): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (6): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (7): Parameter containing: [torch.float32 of size 1 (cuda:0)]
    )
    (biases): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (1): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (2): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (3): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (4): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (5): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (6): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (7): Parameter containing: [torch.float32 of size 1 (cuda:0)]
    )
  )
)

Networks name: Discriminator_UNet
Params number: 4375745
Net structure:
Discriminator_UNet(
  (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

24-04-20 17:56:20.234 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.006 | -1.486 |  1.175 |  0.467 | torch.Size([64, 1, 1, 3, 3]) || conv_init0.0.weight
 | -0.000 | -0.267 |  0.219 |  0.059 | torch.Size([64, 64, 1, 3, 3]) || conv_init.0.weight
 |  0.414 | -0.189 |  0.994 |  0.315 | torch.Size([64]) || conv_init.1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.1.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.1.running_mean
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || conv_init.1.running_var
 |  0.000 | -0.247 |  0.230 |  0.059 | torch.Size([64, 64, 1, 3, 3]) || conv_init.3.weight
 |  0.390 | -0.181 |  0.951 |  0.360 | torch.Size([64]) || conv_init.4.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.4.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.4.running_mean
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || conv_init.4.running_var
 |  0.000 | -0.287 |  0.243 |  0.059 | torch.Size([64, 64, 1, 3, 3]) || conv_init.6.weight
 |  0.412 | -0.165 |  0.987 |  0.348 | torch.Size([64]) || conv_init.7.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.7.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.7.running_mean
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || conv_init.7.running_var
 | -0.000 | -0.241 |  0.271 |  0.059 | torch.Size([128, 576]) || altblock.0.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.0.spa_trans.attention.in_proj_weight
 | -0.000 | -0.480 |  0.474 |  0.126 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.bias
 | -0.001 | -0.508 |  0.500 |  0.124 | torch.Size([256, 128]) || altblock.0.spa_trans.feed_forward.1.weight
 | -0.000 | -0.346 |  0.350 |  0.088 | torch.Size([128, 256]) || altblock.0.spa_trans.feed_forward.4.weight
 | -0.000 | -0.514 |  0.451 |  0.124 | torch.Size([64, 128, 1, 1, 1]) || altblock.0.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.bias
 | -0.001 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.0.ang_trans.attention.in_proj_weight
 |  0.002 | -0.654 |  0.684 |  0.173 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.bias
 |  0.004 | -0.692 |  0.734 |  0.177 | torch.Size([128, 64]) || altblock.0.ang_trans.feed_forward.1.weight
 | -0.000 | -0.451 |  0.479 |  0.126 | torch.Size([64, 128]) || altblock.0.ang_trans.feed_forward.4.weight
 |  0.000 | -0.246 |  0.266 |  0.059 | torch.Size([128, 576]) || altblock.1.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.1.spa_trans.attention.in_proj_weight
 |  0.001 | -0.491 |  0.465 |  0.126 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.bias
 | -0.001 | -0.482 |  0.482 |  0.125 | torch.Size([256, 128]) || altblock.1.spa_trans.feed_forward.1.weight
 | -0.000 | -0.381 |  0.377 |  0.088 | torch.Size([128, 256]) || altblock.1.spa_trans.feed_forward.4.weight
 |  0.001 | -0.432 |  0.501 |  0.124 | torch.Size([64, 128, 1, 1, 1]) || altblock.1.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.bias
 | -0.000 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.1.ang_trans.attention.in_proj_weight
 | -0.003 | -0.603 |  0.599 |  0.179 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.bias
 |  0.001 | -0.689 |  0.670 |  0.177 | torch.Size([128, 64]) || altblock.1.ang_trans.feed_forward.1.weight
 |  0.001 | -0.468 |  0.485 |  0.124 | torch.Size([64, 128]) || altblock.1.ang_trans.feed_forward.4.weight
 |  0.000 | -0.243 |  0.253 |  0.059 | torch.Size([128, 576]) || altblock.2.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.bias
 |  0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.2.spa_trans.attention.in_proj_weight
 |  0.000 | -0.447 |  0.491 |  0.126 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.bias
 | -0.001 | -0.506 |  0.504 |  0.124 | torch.Size([256, 128]) || altblock.2.spa_trans.feed_forward.1.weight
 |  0.001 | -0.389 |  0.375 |  0.088 | torch.Size([128, 256]) || altblock.2.spa_trans.feed_forward.4.weight
 | -0.001 | -0.439 |  0.468 |  0.125 | torch.Size([64, 128, 1, 1, 1]) || altblock.2.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.bias
 | -0.000 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.2.ang_trans.attention.in_proj_weight
 |  0.003 | -0.604 |  0.595 |  0.176 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.bias
 | -0.003 | -0.681 |  0.676 |  0.177 | torch.Size([128, 64]) || altblock.2.ang_trans.feed_forward.1.weight
 |  0.003 | -0.446 |  0.496 |  0.125 | torch.Size([64, 128]) || altblock.2.ang_trans.feed_forward.4.weight
 |  0.000 | -0.257 |  0.246 |  0.059 | torch.Size([128, 576]) || altblock.3.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.bias
 | -0.000 | -0.088 |  0.088 |  0.051 | torch.Size([384, 128]) || altblock.3.spa_trans.attention.in_proj_weight
 |  0.000 | -0.484 |  0.521 |  0.125 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.bias
 | -0.000 | -0.496 |  0.532 |  0.125 | torch.Size([256, 128]) || altblock.3.spa_trans.feed_forward.1.weight
 | -0.000 | -0.394 |  0.356 |  0.089 | torch.Size([128, 256]) || altblock.3.spa_trans.feed_forward.4.weight
 |  0.001 | -0.487 |  0.596 |  0.126 | torch.Size([64, 128, 1, 1, 1]) || altblock.3.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.bias
 |  0.000 | -0.125 |  0.125 |  0.072 | torch.Size([192, 64]) || altblock.3.ang_trans.attention.in_proj_weight
 | -0.005 | -0.722 |  0.613 |  0.177 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.out_proj.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.bias
 |  0.001 | -0.643 |  0.649 |  0.177 | torch.Size([128, 64]) || altblock.3.ang_trans.feed_forward.1.weight
 |  0.002 | -0.493 |  0.478 |  0.125 | torch.Size([64, 128]) || altblock.3.ang_trans.feed_forward.4.weight
 |  0.000 | -0.835 |  0.776 |  0.177 | torch.Size([1024, 64, 1, 1]) || upsampling.0.weight
 |  0.002 | -0.181 |  0.205 |  0.059 | torch.Size([1, 64, 3, 3]) || upsampling.3.weight
 |  0.034 |  0.034 |  0.034 |    nan | torch.Size([1]) || resizer.weights.0
 | -0.041 | -0.041 | -0.041 |    nan | torch.Size([1]) || resizer.weights.1
 | -0.047 | -0.047 | -0.047 |    nan | torch.Size([1]) || resizer.weights.2
 |  0.001 |  0.001 |  0.001 |    nan | torch.Size([1]) || resizer.weights.3
 |  0.014 |  0.014 |  0.014 |    nan | torch.Size([1]) || resizer.weights.4
 |  0.009 |  0.009 |  0.009 |    nan | torch.Size([1]) || resizer.weights.5
 | -0.006 | -0.006 | -0.006 |    nan | torch.Size([1]) || resizer.weights.6
 | -0.047 | -0.047 | -0.047 |    nan | torch.Size([1]) || resizer.weights.7
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.0
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.1
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.2
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.3
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.4
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.5
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.6
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.7

24-04-22 10:05:01.596 :   task: lft_gan_unet
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution/lft_gan_unet
    log: superresolution/lft_gan_unet
    options: superresolution/lft_gan_unet/options
    models: superresolution/lft_gan_unet/models
    images: superresolution/lft_gan_unet/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /mnt/d/LF/data_for_train
      data_name: ALL
      angRes: 5
      dataroot_L: None
      H_size: 660
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr-lf
      dataroot_H: /mnt/d/LF/data_for_test
      angRes: 5
      dataroot_L: None
      data_name: ALL
      H_size: 660
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: kaiming_normal
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_unet
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: 3
    init_type: kaiming_normal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: l2sum
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: gan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    G_scheduler_T_max: 2000000
    D_scheduler_T_max: 2000000
    G_scheduler_eta_min: 1e-07
    D_scheduler_eta_min: 1e-07
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 0.001
    G_optimizer_wd: 0
    G_clip_value: 1.0
    D_optimizer_type: adam
    D_optimizer_lr: 0.001
    D_optimizer_wd: 0
    D_clip_value: 1.0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-22 10:05:01.784 : Number of train images: 1,820, iters: 910
24-04-22 10:07:27.845 :   task: lft_gan_unet
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution/lft_gan_unet
    log: superresolution/lft_gan_unet
    options: superresolution/lft_gan_unet/options
    models: superresolution/lft_gan_unet/models
    images: superresolution/lft_gan_unet/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /mnt/d/LF/data_for_train
      data_name: ALL
      angRes: 5
      dataroot_L: None
      H_size: 660
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr-lf
      dataroot_H: /mnt/d/LF/data_for_test
      angRes: 5
      dataroot_L: None
      data_name: ALL
      H_size: 660
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: kaiming_normal
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_unet
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: 3
    init_type: kaiming_normal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: l2sum
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: gan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    G_scheduler_T_max: 2000000
    D_scheduler_T_max: 2000000
    G_scheduler_eta_min: 1e-07
    D_scheduler_eta_min: 1e-07
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 0.001
    G_optimizer_wd: 0
    G_clip_value: 1.0
    D_optimizer_type: adam
    D_optimizer_lr: 0.001
    D_optimizer_wd: 0
    D_clip_value: 1.0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-22 10:07:27.933 : Number of train images: 1,820, iters: 910
24-04-22 10:07:30.114 : 
Networks name: LFT
Params number: 1164624
Net structure:
LFT(
  (pos_encoding): PositionEncoding()
  (conv_init0): Sequential(
    (0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
  )
  (conv_init): Sequential(
    (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
    (3): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): LeakyReLU(negative_slope=0.2, inplace=True)
    (6): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (7): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (altblock): Sequential(
    (0): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Class_Attention(
          (q): Linear(in_features=128, out_features=128, bias=False)
          (k): Linear(in_features=128, out_features=128, bias=False)
          (v): Linear(in_features=128, out_features=128, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Class_Attention(
          (q): Linear(in_features=64, out_features=64, bias=False)
          (k): Linear(in_features=64, out_features=64, bias=False)
          (v): Linear(in_features=64, out_features=64, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Class_Attention(
          (q): Linear(in_features=128, out_features=128, bias=False)
          (k): Linear(in_features=128, out_features=128, bias=False)
          (v): Linear(in_features=128, out_features=128, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Class_Attention(
          (q): Linear(in_features=64, out_features=64, bias=False)
          (k): Linear(in_features=64, out_features=64, bias=False)
          (v): Linear(in_features=64, out_features=64, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Class_Attention(
          (q): Linear(in_features=128, out_features=128, bias=False)
          (k): Linear(in_features=128, out_features=128, bias=False)
          (v): Linear(in_features=128, out_features=128, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Class_Attention(
          (q): Linear(in_features=64, out_features=64, bias=False)
          (k): Linear(in_features=64, out_features=64, bias=False)
          (v): Linear(in_features=64, out_features=64, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Class_Attention(
          (q): Linear(in_features=128, out_features=128, bias=False)
          (k): Linear(in_features=128, out_features=128, bias=False)
          (v): Linear(in_features=128, out_features=128, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Class_Attention(
          (q): Linear(in_features=64, out_features=64, bias=False)
          (k): Linear(in_features=64, out_features=64, bias=False)
          (v): Linear(in_features=64, out_features=64, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsampling): Sequential(
    (0): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): PixelShuffle(upscale_factor=4)
    (2): LeakyReLU(negative_slope=0.2)
    (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (resizer): MullerResizer(
    (weights): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (1): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (2): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (3): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (4): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (5): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (6): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (7): Parameter containing: [torch.float32 of size 1 (cuda:0)]
    )
    (biases): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (1): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (2): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (3): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (4): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (5): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (6): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (7): Parameter containing: [torch.float32 of size 1 (cuda:0)]
    )
  )
)

Networks name: Discriminator_UNet
Params number: 4375745
Net structure:
Discriminator_UNet(
  (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

24-04-22 10:07:30.276 : 
 |  mean  |  min   |  max   |  std   || shape               
 | -0.017 | -1.384 |  1.578 |  0.470 | torch.Size([64, 1, 1, 3, 3]) || conv_init0.0.weight
 | -0.000 | -0.242 |  0.228 |  0.059 | torch.Size([64, 64, 1, 3, 3]) || conv_init.0.weight
 |  0.403 | -0.179 |  0.994 |  0.363 | torch.Size([64]) || conv_init.1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.1.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.1.running_mean
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || conv_init.1.running_var
 | -0.001 | -0.269 |  0.232 |  0.059 | torch.Size([64, 64, 1, 3, 3]) || conv_init.3.weight
 |  0.411 | -0.195 |  0.978 |  0.320 | torch.Size([64]) || conv_init.4.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.4.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.4.running_mean
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || conv_init.4.running_var
 |  0.001 | -0.277 |  0.243 |  0.059 | torch.Size([64, 64, 1, 3, 3]) || conv_init.6.weight
 |  0.383 | -0.197 |  0.980 |  0.351 | torch.Size([64]) || conv_init.7.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.7.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.7.running_mean
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || conv_init.7.running_var
 |  0.000 | -0.292 |  0.281 |  0.059 | torch.Size([128, 576]) || altblock.0.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.spa_trans.attention.lamb
 |  0.002 | -0.587 |  0.541 |  0.124 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.q.weight
 | -0.002 | -0.545 |  0.497 |  0.127 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.k.weight
 |  0.001 | -0.462 |  0.525 |  0.125 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.v.weight
 | -0.001 | -0.519 |  0.560 |  0.125 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.attention.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.bias
 | -0.001 | -0.509 |  0.525 |  0.126 | torch.Size([256, 128]) || altblock.0.spa_trans.feed_forward.1.weight
 |  0.000 | -0.327 |  0.365 |  0.089 | torch.Size([128, 256]) || altblock.0.spa_trans.feed_forward.4.weight
 |  0.001 | -0.511 |  0.492 |  0.122 | torch.Size([64, 128, 1, 1, 1]) || altblock.0.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.ang_trans.attention.lamb
 |  0.001 | -0.646 |  0.569 |  0.176 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.q.weight
 | -0.001 | -0.604 |  0.637 |  0.178 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.k.weight
 |  0.001 | -0.588 |  0.627 |  0.175 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.v.weight
 |  0.003 | -0.615 |  0.633 |  0.177 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.attention.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.bias
 |  0.001 | -0.666 |  0.626 |  0.177 | torch.Size([128, 64]) || altblock.0.ang_trans.feed_forward.1.weight
 |  0.002 | -0.534 |  0.516 |  0.126 | torch.Size([64, 128]) || altblock.0.ang_trans.feed_forward.4.weight
 |  0.000 | -0.262 |  0.228 |  0.059 | torch.Size([128, 576]) || altblock.1.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.spa_trans.attention.lamb
 |  0.001 | -0.468 |  0.477 |  0.125 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.q.weight
 | -0.001 | -0.494 |  0.504 |  0.126 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.k.weight
 | -0.001 | -0.567 |  0.492 |  0.123 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.v.weight
 |  0.001 | -0.468 |  0.494 |  0.125 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.attention.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.bias
 | -0.001 | -0.557 |  0.500 |  0.124 | torch.Size([256, 128]) || altblock.1.spa_trans.feed_forward.1.weight
 |  0.000 | -0.357 |  0.397 |  0.088 | torch.Size([128, 256]) || altblock.1.spa_trans.feed_forward.4.weight
 |  0.000 | -0.461 |  0.443 |  0.126 | torch.Size([64, 128, 1, 1, 1]) || altblock.1.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.ang_trans.attention.lamb
 | -0.005 | -0.673 |  0.737 |  0.175 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.q.weight
 | -0.002 | -0.596 |  0.641 |  0.178 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.k.weight
 | -0.000 | -0.606 |  0.590 |  0.177 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.v.weight
 | -0.000 | -0.636 |  0.608 |  0.178 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.attention.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.bias
 | -0.000 | -0.598 |  0.666 |  0.175 | torch.Size([128, 64]) || altblock.1.ang_trans.feed_forward.1.weight
 | -0.002 | -0.452 |  0.478 |  0.126 | torch.Size([64, 128]) || altblock.1.ang_trans.feed_forward.4.weight
 | -0.000 | -0.281 |  0.262 |  0.059 | torch.Size([128, 576]) || altblock.2.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.spa_trans.attention.lamb
 |  0.000 | -0.587 |  0.622 |  0.125 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.q.weight
 | -0.001 | -0.482 |  0.502 |  0.125 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.k.weight
 |  0.001 | -0.560 |  0.488 |  0.124 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.v.weight
 |  0.001 | -0.526 |  0.479 |  0.126 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.attention.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.bias
 |  0.000 | -0.545 |  0.540 |  0.125 | torch.Size([256, 128]) || altblock.2.spa_trans.feed_forward.1.weight
 | -0.000 | -0.404 |  0.350 |  0.089 | torch.Size([128, 256]) || altblock.2.spa_trans.feed_forward.4.weight
 |  0.000 | -0.436 |  0.451 |  0.125 | torch.Size([64, 128, 1, 1, 1]) || altblock.2.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.ang_trans.attention.lamb
 |  0.004 | -0.584 |  0.659 |  0.175 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.q.weight
 |  0.002 | -0.617 |  0.579 |  0.176 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.k.weight
 |  0.002 | -0.706 |  0.689 |  0.178 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.v.weight
 |  0.000 | -0.651 |  0.574 |  0.180 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.attention.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.bias
 |  0.002 | -0.742 |  0.740 |  0.177 | torch.Size([128, 64]) || altblock.2.ang_trans.feed_forward.1.weight
 | -0.000 | -0.447 |  0.434 |  0.124 | torch.Size([64, 128]) || altblock.2.ang_trans.feed_forward.4.weight
 | -0.000 | -0.266 |  0.238 |  0.059 | torch.Size([128, 576]) || altblock.3.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.spa_trans.attention.lamb
 |  0.000 | -0.523 |  0.545 |  0.125 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.q.weight
 | -0.000 | -0.495 |  0.466 |  0.126 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.k.weight
 | -0.000 | -0.473 |  0.481 |  0.125 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.v.weight
 |  0.001 | -0.470 |  0.502 |  0.126 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.attention.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.bias
 |  0.000 | -0.475 |  0.519 |  0.124 | torch.Size([256, 128]) || altblock.3.spa_trans.feed_forward.1.weight
 | -0.000 | -0.348 |  0.343 |  0.089 | torch.Size([128, 256]) || altblock.3.spa_trans.feed_forward.4.weight
 |  0.000 | -0.464 |  0.563 |  0.126 | torch.Size([64, 128, 1, 1, 1]) || altblock.3.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.ang_trans.attention.lamb
 |  0.002 | -0.625 |  0.603 |  0.178 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.q.weight
 |  0.005 | -0.653 |  0.605 |  0.173 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.k.weight
 |  0.001 | -0.660 |  0.691 |  0.178 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.v.weight
 |  0.002 | -0.691 |  0.648 |  0.173 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.attention.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.bias
 |  0.002 | -0.636 |  0.642 |  0.178 | torch.Size([128, 64]) || altblock.3.ang_trans.feed_forward.1.weight
 |  0.003 | -0.439 |  0.455 |  0.124 | torch.Size([64, 128]) || altblock.3.ang_trans.feed_forward.4.weight
 |  0.001 | -0.765 |  0.749 |  0.177 | torch.Size([1024, 64, 1, 1]) || upsampling.0.weight
 | -0.004 | -0.179 |  0.169 |  0.058 | torch.Size([1, 64, 3, 3]) || upsampling.3.weight
 |  0.046 |  0.046 |  0.046 |    nan | torch.Size([1]) || resizer.weights.0
 |  0.035 |  0.035 |  0.035 |    nan | torch.Size([1]) || resizer.weights.1
 | -0.009 | -0.009 | -0.009 |    nan | torch.Size([1]) || resizer.weights.2
 | -0.030 | -0.030 | -0.030 |    nan | torch.Size([1]) || resizer.weights.3
 | -0.011 | -0.011 | -0.011 |    nan | torch.Size([1]) || resizer.weights.4
 | -0.032 | -0.032 | -0.032 |    nan | torch.Size([1]) || resizer.weights.5
 | -0.036 | -0.036 | -0.036 |    nan | torch.Size([1]) || resizer.weights.6
 |  0.008 |  0.008 |  0.008 |    nan | torch.Size([1]) || resizer.weights.7
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.0
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.1
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.2
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.3
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.4
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.5
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.6
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.7

24-04-22 10:11:02.262 :   task: lft_gan_unet
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution/lft_gan_unet
    log: superresolution/lft_gan_unet
    options: superresolution/lft_gan_unet/options
    models: superresolution/lft_gan_unet/models
    images: superresolution/lft_gan_unet/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /mnt/d/LF/data_for_train
      data_name: ALL
      angRes: 5
      dataroot_L: None
      H_size: 660
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr-lf
      dataroot_H: /mnt/d/LF/data_for_test
      angRes: 5
      dataroot_L: None
      data_name: ALL
      H_size: 660
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: kaiming_normal
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_unet
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: 3
    init_type: kaiming_normal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: l2sum
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: gan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    G_scheduler_T_max: 2000000
    D_scheduler_T_max: 2000000
    G_scheduler_eta_min: 1e-07
    D_scheduler_eta_min: 1e-07
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 0.001
    G_optimizer_wd: 0
    G_clip_value: 1.0
    D_optimizer_type: adam
    D_optimizer_lr: 0.001
    D_optimizer_wd: 0
    D_clip_value: 1.0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-22 10:11:02.297 : Number of train images: 1,820, iters: 910
24-04-22 10:11:03.844 : 
Networks name: LFT
Params number: 1164624
Net structure:
LFT(
  (pos_encoding): PositionEncoding()
  (conv_init0): Sequential(
    (0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
  )
  (conv_init): Sequential(
    (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
    (3): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): LeakyReLU(negative_slope=0.2, inplace=True)
    (6): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (7): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (altblock): Sequential(
    (0): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Class_Attention(
          (q): Linear(in_features=128, out_features=128, bias=False)
          (k): Linear(in_features=128, out_features=128, bias=False)
          (v): Linear(in_features=128, out_features=128, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Class_Attention(
          (q): Linear(in_features=64, out_features=64, bias=False)
          (k): Linear(in_features=64, out_features=64, bias=False)
          (v): Linear(in_features=64, out_features=64, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Class_Attention(
          (q): Linear(in_features=128, out_features=128, bias=False)
          (k): Linear(in_features=128, out_features=128, bias=False)
          (v): Linear(in_features=128, out_features=128, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Class_Attention(
          (q): Linear(in_features=64, out_features=64, bias=False)
          (k): Linear(in_features=64, out_features=64, bias=False)
          (v): Linear(in_features=64, out_features=64, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Class_Attention(
          (q): Linear(in_features=128, out_features=128, bias=False)
          (k): Linear(in_features=128, out_features=128, bias=False)
          (v): Linear(in_features=128, out_features=128, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Class_Attention(
          (q): Linear(in_features=64, out_features=64, bias=False)
          (k): Linear(in_features=64, out_features=64, bias=False)
          (v): Linear(in_features=64, out_features=64, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Class_Attention(
          (q): Linear(in_features=128, out_features=128, bias=False)
          (k): Linear(in_features=128, out_features=128, bias=False)
          (v): Linear(in_features=128, out_features=128, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Class_Attention(
          (q): Linear(in_features=64, out_features=64, bias=False)
          (k): Linear(in_features=64, out_features=64, bias=False)
          (v): Linear(in_features=64, out_features=64, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsampling): Sequential(
    (0): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): PixelShuffle(upscale_factor=4)
    (2): LeakyReLU(negative_slope=0.2)
    (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (resizer): MullerResizer(
    (weights): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (1): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (2): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (3): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (4): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (5): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (6): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (7): Parameter containing: [torch.float32 of size 1 (cuda:0)]
    )
    (biases): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (1): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (2): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (3): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (4): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (5): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (6): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (7): Parameter containing: [torch.float32 of size 1 (cuda:0)]
    )
  )
)

Networks name: Discriminator_UNet
Params number: 4375745
Net structure:
Discriminator_UNet(
  (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

24-04-22 10:11:03.915 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.009 | -1.628 |  1.414 |  0.466 | torch.Size([64, 1, 1, 3, 3]) || conv_init0.0.weight
 |  0.000 | -0.254 |  0.244 |  0.059 | torch.Size([64, 64, 1, 3, 3]) || conv_init.0.weight
 |  0.414 | -0.192 |  0.995 |  0.366 | torch.Size([64]) || conv_init.1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.1.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.1.running_mean
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || conv_init.1.running_var
 | -0.000 | -0.244 |  0.239 |  0.059 | torch.Size([64, 64, 1, 3, 3]) || conv_init.3.weight
 |  0.410 | -0.179 |  0.999 |  0.358 | torch.Size([64]) || conv_init.4.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.4.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.4.running_mean
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || conv_init.4.running_var
 |  0.000 | -0.244 |  0.272 |  0.060 | torch.Size([64, 64, 1, 3, 3]) || conv_init.6.weight
 |  0.435 | -0.198 |  0.964 |  0.341 | torch.Size([64]) || conv_init.7.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.7.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.7.running_mean
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || conv_init.7.running_var
 | -0.000 | -0.248 |  0.265 |  0.059 | torch.Size([128, 576]) || altblock.0.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.spa_trans.attention.lamb
 |  0.000 | -0.464 |  0.487 |  0.125 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.q.weight
 | -0.002 | -0.510 |  0.498 |  0.125 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.k.weight
 |  0.000 | -0.529 |  0.449 |  0.126 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.v.weight
 | -0.000 | -0.437 |  0.470 |  0.125 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.attention.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.bias
 |  0.000 | -0.495 |  0.526 |  0.125 | torch.Size([256, 128]) || altblock.0.spa_trans.feed_forward.1.weight
 |  0.001 | -0.323 |  0.351 |  0.089 | torch.Size([128, 256]) || altblock.0.spa_trans.feed_forward.4.weight
 |  0.001 | -0.541 |  0.431 |  0.126 | torch.Size([64, 128, 1, 1, 1]) || altblock.0.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.ang_trans.attention.lamb
 | -0.000 | -0.597 |  0.721 |  0.177 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.q.weight
 | -0.001 | -0.606 |  0.645 |  0.174 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.k.weight
 |  0.001 | -0.614 |  0.746 |  0.175 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.v.weight
 | -0.001 | -0.614 |  0.595 |  0.179 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.attention.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.bias
 | -0.000 | -0.665 |  0.653 |  0.176 | torch.Size([128, 64]) || altblock.0.ang_trans.feed_forward.1.weight
 |  0.000 | -0.479 |  0.448 |  0.124 | torch.Size([64, 128]) || altblock.0.ang_trans.feed_forward.4.weight
 | -0.000 | -0.253 |  0.259 |  0.059 | torch.Size([128, 576]) || altblock.1.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.spa_trans.attention.lamb
 | -0.000 | -0.457 |  0.451 |  0.125 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.q.weight
 |  0.000 | -0.537 |  0.456 |  0.125 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.k.weight
 | -0.001 | -0.473 |  0.498 |  0.125 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.v.weight
 | -0.000 | -0.461 |  0.455 |  0.125 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.attention.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.bias
 |  0.000 | -0.490 |  0.523 |  0.125 | torch.Size([256, 128]) || altblock.1.spa_trans.feed_forward.1.weight
 | -0.001 | -0.352 |  0.358 |  0.088 | torch.Size([128, 256]) || altblock.1.spa_trans.feed_forward.4.weight
 | -0.000 | -0.494 |  0.536 |  0.124 | torch.Size([64, 128, 1, 1, 1]) || altblock.1.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.ang_trans.attention.lamb
 | -0.000 | -0.608 |  0.728 |  0.176 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.q.weight
 | -0.003 | -0.622 |  0.654 |  0.176 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.k.weight
 | -0.005 | -0.611 |  0.713 |  0.178 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.v.weight
 |  0.001 | -0.577 |  0.595 |  0.175 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.attention.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.bias
 | -0.001 | -0.738 |  0.632 |  0.177 | torch.Size([128, 64]) || altblock.1.ang_trans.feed_forward.1.weight
 | -0.003 | -0.433 |  0.491 |  0.126 | torch.Size([64, 128]) || altblock.1.ang_trans.feed_forward.4.weight
 | -0.000 | -0.248 |  0.277 |  0.059 | torch.Size([128, 576]) || altblock.2.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.spa_trans.attention.lamb
 | -0.000 | -0.431 |  0.462 |  0.124 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.q.weight
 |  0.000 | -0.491 |  0.537 |  0.124 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.k.weight
 | -0.001 | -0.495 |  0.494 |  0.124 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.v.weight
 |  0.001 | -0.455 |  0.499 |  0.126 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.attention.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.bias
 | -0.001 | -0.558 |  0.469 |  0.124 | torch.Size([256, 128]) || altblock.2.spa_trans.feed_forward.1.weight
 |  0.000 | -0.361 |  0.349 |  0.088 | torch.Size([128, 256]) || altblock.2.spa_trans.feed_forward.4.weight
 | -0.001 | -0.536 |  0.478 |  0.125 | torch.Size([64, 128, 1, 1, 1]) || altblock.2.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.ang_trans.attention.lamb
 | -0.003 | -0.585 |  0.665 |  0.179 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.q.weight
 | -0.001 | -0.617 |  0.557 |  0.175 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.k.weight
 | -0.000 | -0.649 |  0.657 |  0.176 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.v.weight
 | -0.000 | -0.622 |  0.598 |  0.175 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.attention.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.bias
 | -0.001 | -0.619 |  0.715 |  0.177 | torch.Size([128, 64]) || altblock.2.ang_trans.feed_forward.1.weight
 | -0.000 | -0.399 |  0.467 |  0.123 | torch.Size([64, 128]) || altblock.2.ang_trans.feed_forward.4.weight
 | -0.000 | -0.288 |  0.243 |  0.059 | torch.Size([128, 576]) || altblock.3.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.spa_trans.attention.lamb
 |  0.001 | -0.465 |  0.465 |  0.125 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.q.weight
 | -0.000 | -0.526 |  0.518 |  0.125 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.k.weight
 |  0.001 | -0.489 |  0.492 |  0.124 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.v.weight
 | -0.000 | -0.494 |  0.439 |  0.126 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.attention.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.bias
 | -0.001 | -0.478 |  0.489 |  0.124 | torch.Size([256, 128]) || altblock.3.spa_trans.feed_forward.1.weight
 | -0.000 | -0.356 |  0.376 |  0.088 | torch.Size([128, 256]) || altblock.3.spa_trans.feed_forward.4.weight
 | -0.000 | -0.457 |  0.551 |  0.126 | torch.Size([64, 128, 1, 1, 1]) || altblock.3.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.ang_trans.attention.lamb
 | -0.001 | -0.647 |  0.614 |  0.176 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.q.weight
 |  0.003 | -0.626 |  0.763 |  0.176 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.k.weight
 |  0.000 | -0.788 |  0.647 |  0.178 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.v.weight
 |  0.001 | -0.674 |  0.618 |  0.177 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.attention.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.bias
 |  0.000 | -0.660 |  0.611 |  0.178 | torch.Size([128, 64]) || altblock.3.ang_trans.feed_forward.1.weight
 |  0.002 | -0.467 |  0.419 |  0.124 | torch.Size([64, 128]) || altblock.3.ang_trans.feed_forward.4.weight
 | -0.001 | -0.853 |  0.759 |  0.177 | torch.Size([1024, 64, 1, 1]) || upsampling.0.weight
 | -0.001 | -0.174 |  0.191 |  0.059 | torch.Size([1, 64, 3, 3]) || upsampling.3.weight
 | -0.018 | -0.018 | -0.018 |    nan | torch.Size([1]) || resizer.weights.0
 | -0.039 | -0.039 | -0.039 |    nan | torch.Size([1]) || resizer.weights.1
 |  0.043 |  0.043 |  0.043 |    nan | torch.Size([1]) || resizer.weights.2
 |  0.049 |  0.049 |  0.049 |    nan | torch.Size([1]) || resizer.weights.3
 | -0.017 | -0.017 | -0.017 |    nan | torch.Size([1]) || resizer.weights.4
 | -0.013 | -0.013 | -0.013 |    nan | torch.Size([1]) || resizer.weights.5
 | -0.017 | -0.017 | -0.017 |    nan | torch.Size([1]) || resizer.weights.6
 |  0.010 |  0.010 |  0.010 |    nan | torch.Size([1]) || resizer.weights.7
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.0
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.1
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.2
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.3
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.4
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.5
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.6
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.7

24-04-22 10:15:32.062 : <epoch:  0, iter:     200, lr:1.000e-03> G_loss: 3.868e+03 F_loss: 4.310e+00 D_loss: 3.526e-01 D_real: 7.196e+00 D_fake: -7.052e+01 
24-04-22 10:20:11.612 : <epoch:  0, iter:     400, lr:1.000e-03> G_loss: 1.960e+02 F_loss: 1.918e+00 D_loss: 5.931e-02 D_real: 1.193e+01 D_fake: -1.184e+01 
24-04-22 10:24:48.617 : <epoch:  0, iter:     600, lr:1.000e-03> G_loss: 6.111e+01 F_loss: 1.918e+00 D_loss: 2.511e-02 D_real: 8.707e-01 D_fake: -4.880e+00 
24-04-22 10:28:59.346 :   task: lft_gan_unet
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution/lft_gan_unet
    log: superresolution/lft_gan_unet
    options: superresolution/lft_gan_unet/options
    models: superresolution/lft_gan_unet/models
    images: superresolution/lft_gan_unet/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /mnt/d/LF/data_for_train
      data_name: ALL
      angRes: 5
      dataroot_L: None
      H_size: 660
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr-lf
      dataroot_H: /mnt/d/LF/data_for_test
      angRes: 5
      dataroot_L: None
      data_name: ALL
      H_size: 660
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: kaiming_normal
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_unet
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: 3
    init_type: kaiming_normal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: l2sum
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: gan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    G_scheduler_T_max: 2000000
    D_scheduler_T_max: 2000000
    G_scheduler_eta_min: 1e-07
    D_scheduler_eta_min: 1e-07
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 0.001
    G_optimizer_wd: 0
    G_clip_value: 1.0
    D_optimizer_type: adam
    D_optimizer_lr: 0.001
    D_optimizer_wd: 0
    D_clip_value: 1.0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-22 10:28:59.382 : Number of train images: 1,820, iters: 910
24-04-22 10:29:01.068 : 
Networks name: LFT
Params number: 1165776
Net structure:
LFT(
  (pos_encoding): PositionEncoding()
  (conv_init0): Sequential(
    (0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
  )
  (conv_init): Sequential(
    (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
    (3): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): LeakyReLU(negative_slope=0.2, inplace=True)
    (6): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (7): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (altblock): Sequential(
    (0): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=128, out_features=384, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=64, out_features=192, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=128, out_features=384, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=64, out_features=192, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=128, out_features=384, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=64, out_features=192, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=128, out_features=384, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=64, out_features=192, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsampling): Sequential(
    (0): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): PixelShuffle(upscale_factor=4)
    (2): LeakyReLU(negative_slope=0.2)
    (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (resizer): MullerResizer(
    (weights): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (1): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (2): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (3): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (4): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (5): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (6): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (7): Parameter containing: [torch.float32 of size 1 (cuda:0)]
    )
    (biases): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (1): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (2): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (3): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (4): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (5): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (6): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (7): Parameter containing: [torch.float32 of size 1 (cuda:0)]
    )
  )
)

Networks name: Discriminator_UNet
Params number: 4375745
Net structure:
Discriminator_UNet(
  (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

24-04-22 10:29:01.141 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.004 | -1.529 |  1.522 |  0.466 | torch.Size([64, 1, 1, 3, 3]) || conv_init0.0.weight
 |  0.000 | -0.245 |  0.258 |  0.059 | torch.Size([64, 64, 1, 3, 3]) || conv_init.0.weight
 |  0.397 | -0.199 |  0.897 |  0.332 | torch.Size([64]) || conv_init.1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.1.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.1.running_mean
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || conv_init.1.running_var
 |  0.000 | -0.237 |  0.235 |  0.059 | torch.Size([64, 64, 1, 3, 3]) || conv_init.3.weight
 |  0.392 | -0.175 |  0.968 |  0.311 | torch.Size([64]) || conv_init.4.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.4.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.4.running_mean
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || conv_init.4.running_var
 |  0.000 | -0.258 |  0.254 |  0.059 | torch.Size([64, 64, 1, 3, 3]) || conv_init.6.weight
 |  0.439 | -0.191 |  0.986 |  0.369 | torch.Size([64]) || conv_init.7.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.7.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.7.running_mean
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || conv_init.7.running_var
 | -0.000 | -0.228 |  0.258 |  0.059 | torch.Size([128, 576]) || altblock.0.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.spa_trans.attention.lamb
 | -0.001 | -0.539 |  0.498 |  0.125 | torch.Size([384, 128]) || altblock.0.spa_trans.attention.qkv.weight
 | -0.000 | -0.584 |  0.477 |  0.126 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.attention.proj.bias
 | -0.069 | -1.199 |  1.043 |  0.519 | torch.Size([8, 8]) || altblock.0.spa_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.spa_trans.attention.proj_l.bias
 |  0.057 | -1.226 |  1.069 |  0.521 | torch.Size([8, 8]) || altblock.0.spa_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.spa_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.bias
 |  0.000 | -0.566 |  0.510 |  0.126 | torch.Size([256, 128]) || altblock.0.spa_trans.feed_forward.1.weight
 | -0.000 | -0.394 |  0.395 |  0.088 | torch.Size([128, 256]) || altblock.0.spa_trans.feed_forward.4.weight
 |  0.001 | -0.455 |  0.559 |  0.124 | torch.Size([64, 128, 1, 1, 1]) || altblock.0.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.ang_trans.attention.lamb
 | -0.001 | -0.712 |  0.714 |  0.179 | torch.Size([192, 64]) || altblock.0.ang_trans.attention.qkv.weight
 |  0.001 | -0.619 |  0.586 |  0.177 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.attention.proj.bias
 |  0.004 | -0.723 |  1.128 |  0.423 | torch.Size([8, 8]) || altblock.0.ang_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.ang_trans.attention.proj_l.bias
 |  0.166 | -0.887 |  1.423 |  0.489 | torch.Size([8, 8]) || altblock.0.ang_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.ang_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.bias
 |  0.003 | -0.677 |  0.608 |  0.176 | torch.Size([128, 64]) || altblock.0.ang_trans.feed_forward.1.weight
 |  0.002 | -0.452 |  0.436 |  0.125 | torch.Size([64, 128]) || altblock.0.ang_trans.feed_forward.4.weight
 | -0.000 | -0.277 |  0.247 |  0.059 | torch.Size([128, 576]) || altblock.1.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.spa_trans.attention.lamb
 |  0.000 | -0.541 |  0.491 |  0.126 | torch.Size([384, 128]) || altblock.1.spa_trans.attention.qkv.weight
 |  0.001 | -0.514 |  0.464 |  0.125 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.attention.proj.bias
 |  0.094 | -1.387 |  1.254 |  0.523 | torch.Size([8, 8]) || altblock.1.spa_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.spa_trans.attention.proj_l.bias
 | -0.077 | -1.013 |  0.753 |  0.446 | torch.Size([8, 8]) || altblock.1.spa_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.spa_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.bias
 | -0.001 | -0.497 |  0.480 |  0.125 | torch.Size([256, 128]) || altblock.1.spa_trans.feed_forward.1.weight
 | -0.001 | -0.359 |  0.388 |  0.089 | torch.Size([128, 256]) || altblock.1.spa_trans.feed_forward.4.weight
 |  0.001 | -0.471 |  0.438 |  0.125 | torch.Size([64, 128, 1, 1, 1]) || altblock.1.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.ang_trans.attention.lamb
 | -0.000 | -0.697 |  0.683 |  0.177 | torch.Size([192, 64]) || altblock.1.ang_trans.attention.qkv.weight
 |  0.004 | -0.644 |  0.686 |  0.176 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.attention.proj.bias
 | -0.038 | -1.230 |  1.157 |  0.488 | torch.Size([8, 8]) || altblock.1.ang_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.ang_trans.attention.proj_l.bias
 |  0.076 | -1.331 |  1.013 |  0.469 | torch.Size([8, 8]) || altblock.1.ang_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.ang_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.bias
 |  0.002 | -0.673 |  0.706 |  0.176 | torch.Size([128, 64]) || altblock.1.ang_trans.feed_forward.1.weight
 | -0.000 | -0.449 |  0.458 |  0.126 | torch.Size([64, 128]) || altblock.1.ang_trans.feed_forward.4.weight
 |  0.000 | -0.236 |  0.253 |  0.059 | torch.Size([128, 576]) || altblock.2.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.spa_trans.attention.lamb
 |  0.000 | -0.549 |  0.542 |  0.125 | torch.Size([384, 128]) || altblock.2.spa_trans.attention.qkv.weight
 |  0.001 | -0.490 |  0.605 |  0.125 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.attention.proj.bias
 | -0.015 | -1.173 |  1.124 |  0.500 | torch.Size([8, 8]) || altblock.2.spa_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.spa_trans.attention.proj_l.bias
 | -0.012 | -1.182 |  1.059 |  0.523 | torch.Size([8, 8]) || altblock.2.spa_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.spa_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.bias
 | -0.001 | -0.495 |  0.512 |  0.124 | torch.Size([256, 128]) || altblock.2.spa_trans.feed_forward.1.weight
 |  0.000 | -0.352 |  0.391 |  0.088 | torch.Size([128, 256]) || altblock.2.spa_trans.feed_forward.4.weight
 | -0.003 | -0.438 |  0.468 |  0.124 | torch.Size([64, 128, 1, 1, 1]) || altblock.2.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.ang_trans.attention.lamb
 |  0.002 | -0.670 |  0.702 |  0.177 | torch.Size([192, 64]) || altblock.2.ang_trans.attention.qkv.weight
 |  0.000 | -0.547 |  0.627 |  0.173 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.attention.proj.bias
 |  0.001 | -1.700 |  1.074 |  0.506 | torch.Size([8, 8]) || altblock.2.ang_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.ang_trans.attention.proj_l.bias
 | -0.104 | -1.178 |  1.325 |  0.525 | torch.Size([8, 8]) || altblock.2.ang_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.ang_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.bias
 | -0.002 | -0.682 |  0.692 |  0.175 | torch.Size([128, 64]) || altblock.2.ang_trans.feed_forward.1.weight
 | -0.001 | -0.516 |  0.466 |  0.125 | torch.Size([64, 128]) || altblock.2.ang_trans.feed_forward.4.weight
 | -0.000 | -0.284 |  0.235 |  0.059 | torch.Size([128, 576]) || altblock.3.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.spa_trans.attention.lamb
 | -0.001 | -0.518 |  0.522 |  0.125 | torch.Size([384, 128]) || altblock.3.spa_trans.attention.qkv.weight
 |  0.000 | -0.469 |  0.490 |  0.125 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.attention.proj.bias
 | -0.033 | -1.585 |  0.943 |  0.485 | torch.Size([8, 8]) || altblock.3.spa_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.spa_trans.attention.proj_l.bias
 |  0.012 | -0.923 |  1.307 |  0.455 | torch.Size([8, 8]) || altblock.3.spa_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.spa_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.bias
 |  0.001 | -0.528 |  0.575 |  0.125 | torch.Size([256, 128]) || altblock.3.spa_trans.feed_forward.1.weight
 |  0.001 | -0.346 |  0.416 |  0.089 | torch.Size([128, 256]) || altblock.3.spa_trans.feed_forward.4.weight
 | -0.002 | -0.549 |  0.482 |  0.126 | torch.Size([64, 128, 1, 1, 1]) || altblock.3.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.ang_trans.attention.lamb
 | -0.002 | -0.666 |  0.709 |  0.177 | torch.Size([192, 64]) || altblock.3.ang_trans.attention.qkv.weight
 |  0.000 | -0.607 |  0.618 |  0.178 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.attention.proj.bias
 |  0.038 | -1.029 |  1.111 |  0.506 | torch.Size([8, 8]) || altblock.3.ang_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.ang_trans.attention.proj_l.bias
 |  0.067 | -1.066 |  1.106 |  0.467 | torch.Size([8, 8]) || altblock.3.ang_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.ang_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.bias
 |  0.001 | -0.689 |  0.622 |  0.175 | torch.Size([128, 64]) || altblock.3.ang_trans.feed_forward.1.weight
 | -0.000 | -0.518 |  0.452 |  0.125 | torch.Size([64, 128]) || altblock.3.ang_trans.feed_forward.4.weight
 | -0.000 | -0.759 |  0.753 |  0.176 | torch.Size([1024, 64, 1, 1]) || upsampling.0.weight
 | -0.001 | -0.263 |  0.157 |  0.062 | torch.Size([1, 64, 3, 3]) || upsampling.3.weight
 |  0.016 |  0.016 |  0.016 |    nan | torch.Size([1]) || resizer.weights.0
 |  0.028 |  0.028 |  0.028 |    nan | torch.Size([1]) || resizer.weights.1
 | -0.034 | -0.034 | -0.034 |    nan | torch.Size([1]) || resizer.weights.2
 |  0.001 |  0.001 |  0.001 |    nan | torch.Size([1]) || resizer.weights.3
 | -0.011 | -0.011 | -0.011 |    nan | torch.Size([1]) || resizer.weights.4
 |  0.031 |  0.031 |  0.031 |    nan | torch.Size([1]) || resizer.weights.5
 | -0.016 | -0.016 | -0.016 |    nan | torch.Size([1]) || resizer.weights.6
 | -0.036 | -0.036 | -0.036 |    nan | torch.Size([1]) || resizer.weights.7
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.0
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.1
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.2
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.3
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.4
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.5
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.6
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.7

24-04-22 10:29:24.010 :   task: lft_gan_unet
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution/lft_gan_unet
    log: superresolution/lft_gan_unet
    options: superresolution/lft_gan_unet/options
    models: superresolution/lft_gan_unet/models
    images: superresolution/lft_gan_unet/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /mnt/d/LF/data_for_train
      data_name: ALL
      angRes: 5
      dataroot_L: None
      H_size: 660
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr-lf
      dataroot_H: /mnt/d/LF/data_for_test
      angRes: 5
      dataroot_L: None
      data_name: ALL
      H_size: 660
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: kaiming_normal
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_unet
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: 3
    init_type: kaiming_normal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: l2sum
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: gan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    G_scheduler_T_max: 2000000
    D_scheduler_T_max: 2000000
    G_scheduler_eta_min: 1e-07
    D_scheduler_eta_min: 1e-07
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 0.001
    G_optimizer_wd: 0
    G_clip_value: 1.0
    D_optimizer_type: adam
    D_optimizer_lr: 0.001
    D_optimizer_wd: 0
    D_clip_value: 1.0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-22 10:29:24.043 : Number of train images: 1,820, iters: 910
24-04-22 10:29:25.711 : 
Networks name: LFT
Params number: 1165776
Net structure:
LFT(
  (pos_encoding): PositionEncoding()
  (conv_init0): Sequential(
    (0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
  )
  (conv_init): Sequential(
    (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
    (3): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): LeakyReLU(negative_slope=0.2, inplace=True)
    (6): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (7): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (altblock): Sequential(
    (0): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=128, out_features=384, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=64, out_features=192, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=128, out_features=384, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=64, out_features=192, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=128, out_features=384, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=64, out_features=192, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=128, out_features=384, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=64, out_features=192, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsampling): Sequential(
    (0): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): PixelShuffle(upscale_factor=4)
    (2): LeakyReLU(negative_slope=0.2)
    (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (resizer): MullerResizer(
    (weights): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (1): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (2): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (3): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (4): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (5): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (6): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (7): Parameter containing: [torch.float32 of size 1 (cuda:0)]
    )
    (biases): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (1): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (2): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (3): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (4): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (5): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (6): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (7): Parameter containing: [torch.float32 of size 1 (cuda:0)]
    )
  )
)

Networks name: Discriminator_UNet
Params number: 4375745
Net structure:
Discriminator_UNet(
  (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

24-04-22 10:29:25.776 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.002 | -1.367 |  1.353 |  0.457 | torch.Size([64, 1, 1, 3, 3]) || conv_init0.0.weight
 |  0.000 | -0.225 |  0.222 |  0.059 | torch.Size([64, 64, 1, 3, 3]) || conv_init.0.weight
 |  0.391 | -0.165 |  0.997 |  0.361 | torch.Size([64]) || conv_init.1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.1.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.1.running_mean
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || conv_init.1.running_var
 |  0.000 | -0.231 |  0.257 |  0.059 | torch.Size([64, 64, 1, 3, 3]) || conv_init.3.weight
 |  0.321 | -0.180 |  0.969 |  0.341 | torch.Size([64]) || conv_init.4.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.4.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.4.running_mean
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || conv_init.4.running_var
 | -0.000 | -0.238 |  0.226 |  0.059 | torch.Size([64, 64, 1, 3, 3]) || conv_init.6.weight
 |  0.354 | -0.193 |  0.997 |  0.326 | torch.Size([64]) || conv_init.7.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.7.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.7.running_mean
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || conv_init.7.running_var
 |  0.000 | -0.237 |  0.263 |  0.059 | torch.Size([128, 576]) || altblock.0.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.spa_trans.attention.lamb
 |  0.000 | -0.571 |  0.539 |  0.125 | torch.Size([384, 128]) || altblock.0.spa_trans.attention.qkv.weight
 |  0.001 | -0.470 |  0.512 |  0.126 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.attention.proj.bias
 |  0.021 | -1.082 |  1.250 |  0.485 | torch.Size([8, 8]) || altblock.0.spa_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.spa_trans.attention.proj_l.bias
 | -0.103 | -0.992 |  1.321 |  0.486 | torch.Size([8, 8]) || altblock.0.spa_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.spa_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.bias
 | -0.001 | -0.492 |  0.558 |  0.125 | torch.Size([256, 128]) || altblock.0.spa_trans.feed_forward.1.weight
 |  0.000 | -0.374 |  0.357 |  0.089 | torch.Size([128, 256]) || altblock.0.spa_trans.feed_forward.4.weight
 |  0.002 | -0.446 |  0.447 |  0.126 | torch.Size([64, 128, 1, 1, 1]) || altblock.0.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.ang_trans.attention.lamb
 | -0.001 | -0.646 |  0.662 |  0.176 | torch.Size([192, 64]) || altblock.0.ang_trans.attention.qkv.weight
 |  0.003 | -0.611 |  0.641 |  0.175 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.attention.proj.bias
 | -0.056 | -1.714 |  1.166 |  0.532 | torch.Size([8, 8]) || altblock.0.ang_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.ang_trans.attention.proj_l.bias
 |  0.016 | -1.512 |  1.149 |  0.568 | torch.Size([8, 8]) || altblock.0.ang_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.ang_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.bias
 |  0.002 | -0.634 |  0.606 |  0.176 | torch.Size([128, 64]) || altblock.0.ang_trans.feed_forward.1.weight
 |  0.000 | -0.626 |  0.458 |  0.125 | torch.Size([64, 128]) || altblock.0.ang_trans.feed_forward.4.weight
 |  0.000 | -0.233 |  0.256 |  0.059 | torch.Size([128, 576]) || altblock.1.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.spa_trans.attention.lamb
 | -0.001 | -0.533 |  0.563 |  0.125 | torch.Size([384, 128]) || altblock.1.spa_trans.attention.qkv.weight
 |  0.000 | -0.494 |  0.471 |  0.126 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.attention.proj.bias
 |  0.072 | -1.054 |  1.435 |  0.549 | torch.Size([8, 8]) || altblock.1.spa_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.spa_trans.attention.proj_l.bias
 |  0.103 | -0.784 |  1.669 |  0.526 | torch.Size([8, 8]) || altblock.1.spa_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.spa_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.bias
 |  0.001 | -0.499 |  0.513 |  0.124 | torch.Size([256, 128]) || altblock.1.spa_trans.feed_forward.1.weight
 | -0.000 | -0.346 |  0.408 |  0.089 | torch.Size([128, 256]) || altblock.1.spa_trans.feed_forward.4.weight
 | -0.001 | -0.453 |  0.482 |  0.125 | torch.Size([64, 128, 1, 1, 1]) || altblock.1.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.ang_trans.attention.lamb
 |  0.000 | -0.816 |  0.703 |  0.177 | torch.Size([192, 64]) || altblock.1.ang_trans.attention.qkv.weight
 | -0.002 | -0.627 |  0.675 |  0.173 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.attention.proj.bias
 | -0.025 | -1.038 |  1.398 |  0.517 | torch.Size([8, 8]) || altblock.1.ang_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.ang_trans.attention.proj_l.bias
 | -0.001 | -1.361 |  1.677 |  0.577 | torch.Size([8, 8]) || altblock.1.ang_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.ang_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.bias
 | -0.001 | -0.633 |  0.658 |  0.178 | torch.Size([128, 64]) || altblock.1.ang_trans.feed_forward.1.weight
 |  0.001 | -0.481 |  0.500 |  0.124 | torch.Size([64, 128]) || altblock.1.ang_trans.feed_forward.4.weight
 |  0.000 | -0.249 |  0.243 |  0.059 | torch.Size([128, 576]) || altblock.2.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.spa_trans.attention.lamb
 | -0.000 | -0.522 |  0.563 |  0.125 | torch.Size([384, 128]) || altblock.2.spa_trans.attention.qkv.weight
 |  0.000 | -0.536 |  0.500 |  0.125 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.attention.proj.bias
 | -0.052 | -1.456 |  1.118 |  0.507 | torch.Size([8, 8]) || altblock.2.spa_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.spa_trans.attention.proj_l.bias
 | -0.138 | -1.501 |  1.278 |  0.448 | torch.Size([8, 8]) || altblock.2.spa_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.spa_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.bias
 | -0.000 | -0.542 |  0.494 |  0.126 | torch.Size([256, 128]) || altblock.2.spa_trans.feed_forward.1.weight
 |  0.000 | -0.401 |  0.386 |  0.088 | torch.Size([128, 256]) || altblock.2.spa_trans.feed_forward.4.weight
 | -0.001 | -0.448 |  0.468 |  0.125 | torch.Size([64, 128, 1, 1, 1]) || altblock.2.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.ang_trans.attention.lamb
 | -0.002 | -0.733 |  0.817 |  0.176 | torch.Size([192, 64]) || altblock.2.ang_trans.attention.qkv.weight
 | -0.001 | -0.672 |  0.710 |  0.176 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.attention.proj.bias
 |  0.105 | -1.229 |  1.241 |  0.571 | torch.Size([8, 8]) || altblock.2.ang_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.ang_trans.attention.proj_l.bias
 |  0.064 | -1.082 |  1.289 |  0.472 | torch.Size([8, 8]) || altblock.2.ang_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.ang_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.bias
 |  0.002 | -0.739 |  0.632 |  0.175 | torch.Size([128, 64]) || altblock.2.ang_trans.feed_forward.1.weight
 |  0.000 | -0.448 |  0.409 |  0.126 | torch.Size([64, 128]) || altblock.2.ang_trans.feed_forward.4.weight
 |  0.000 | -0.240 |  0.236 |  0.059 | torch.Size([128, 576]) || altblock.3.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.spa_trans.attention.lamb
 | -0.001 | -0.513 |  0.586 |  0.125 | torch.Size([384, 128]) || altblock.3.spa_trans.attention.qkv.weight
 |  0.002 | -0.552 |  0.528 |  0.125 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.attention.proj.bias
 |  0.010 | -0.886 |  1.317 |  0.486 | torch.Size([8, 8]) || altblock.3.spa_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.spa_trans.attention.proj_l.bias
 | -0.072 | -1.377 |  1.456 |  0.593 | torch.Size([8, 8]) || altblock.3.spa_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.spa_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.bias
 |  0.001 | -0.491 |  0.545 |  0.125 | torch.Size([256, 128]) || altblock.3.spa_trans.feed_forward.1.weight
 | -0.000 | -0.369 |  0.383 |  0.089 | torch.Size([128, 256]) || altblock.3.spa_trans.feed_forward.4.weight
 |  0.000 | -0.476 |  0.460 |  0.124 | torch.Size([64, 128, 1, 1, 1]) || altblock.3.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.ang_trans.attention.lamb
 | -0.000 | -0.700 |  0.726 |  0.178 | torch.Size([192, 64]) || altblock.3.ang_trans.attention.qkv.weight
 |  0.001 | -0.598 |  0.588 |  0.179 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.attention.proj.bias
 |  0.123 | -1.250 |  1.272 |  0.538 | torch.Size([8, 8]) || altblock.3.ang_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.ang_trans.attention.proj_l.bias
 |  0.050 | -1.052 |  1.202 |  0.481 | torch.Size([8, 8]) || altblock.3.ang_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.ang_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.bias
 |  0.002 | -0.619 |  0.657 |  0.178 | torch.Size([128, 64]) || altblock.3.ang_trans.feed_forward.1.weight
 | -0.002 | -0.469 |  0.453 |  0.124 | torch.Size([64, 128]) || altblock.3.ang_trans.feed_forward.4.weight
 |  0.000 | -0.770 |  0.766 |  0.176 | torch.Size([1024, 64, 1, 1]) || upsampling.0.weight
 | -0.002 | -0.189 |  0.180 |  0.063 | torch.Size([1, 64, 3, 3]) || upsampling.3.weight
 | -0.017 | -0.017 | -0.017 |    nan | torch.Size([1]) || resizer.weights.0
 |  0.033 |  0.033 |  0.033 |    nan | torch.Size([1]) || resizer.weights.1
 | -0.012 | -0.012 | -0.012 |    nan | torch.Size([1]) || resizer.weights.2
 | -0.014 | -0.014 | -0.014 |    nan | torch.Size([1]) || resizer.weights.3
 |  0.043 |  0.043 |  0.043 |    nan | torch.Size([1]) || resizer.weights.4
 |  0.018 |  0.018 |  0.018 |    nan | torch.Size([1]) || resizer.weights.5
 |  0.013 |  0.013 |  0.013 |    nan | torch.Size([1]) || resizer.weights.6
 | -0.047 | -0.047 | -0.047 |    nan | torch.Size([1]) || resizer.weights.7
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.0
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.1
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.2
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.3
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.4
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.5
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.6
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.7

24-04-22 10:31:42.159 :   task: lft_gan_unet
  model: gan
  gpu_ids: [0]
  scale: 4
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution/lft_gan_unet
    log: superresolution/lft_gan_unet
    options: superresolution/lft_gan_unet/options
    models: superresolution/lft_gan_unet/models
    images: superresolution/lft_gan_unet/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /mnt/d/LF/data_for_train
      data_name: ALL
      angRes: 5
      dataroot_L: None
      H_size: 660
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 4
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr-lf
      dataroot_H: /mnt/d/LF/data_for_test
      angRes: 5
      dataroot_L: None
      data_name: ALL
      H_size: 660
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 4
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: kaiming_normal
    init_bn_type: uniform
    init_gain: 0.2
    scale: 4
  ]
  netD:[
    net_type: discriminator_unet
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: 3
    init_type: kaiming_normal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: l2sum
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: gan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    G_scheduler_T_max: 2000000
    D_scheduler_T_max: 2000000
    G_scheduler_eta_min: 1e-07
    D_scheduler_eta_min: 1e-07
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 0.001
    G_optimizer_wd: 0
    G_clip_value: 1.0
    D_optimizer_type: adam
    D_optimizer_lr: 0.001
    D_optimizer_wd: 0
    D_clip_value: 1.0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-22 10:31:42.192 : Number of train images: 1,820, iters: 910
24-04-22 10:31:43.731 : 
Networks name: LFT
Params number: 1165776
Net structure:
LFT(
  (pos_encoding): PositionEncoding()
  (conv_init0): Sequential(
    (0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
  )
  (conv_init): Sequential(
    (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
    (3): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): LeakyReLU(negative_slope=0.2, inplace=True)
    (6): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (7): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (altblock): Sequential(
    (0): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=128, out_features=384, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=64, out_features=192, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=128, out_features=384, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=64, out_features=192, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=128, out_features=384, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=64, out_features=192, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=128, out_features=384, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=64, out_features=192, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsampling): Sequential(
    (0): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): PixelShuffle(upscale_factor=4)
    (2): LeakyReLU(negative_slope=0.2)
    (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (resizer): MullerResizer(
    (weights): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (1): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (2): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (3): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (4): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (5): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (6): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (7): Parameter containing: [torch.float32 of size 1 (cuda:0)]
    )
    (biases): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (1): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (2): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (3): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (4): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (5): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (6): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (7): Parameter containing: [torch.float32 of size 1 (cuda:0)]
    )
  )
)

Networks name: Discriminator_UNet
Params number: 4375745
Net structure:
Discriminator_UNet(
  (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

24-04-22 10:31:43.808 : 
 |  mean  |  min   |  max   |  std   || shape               
 | -0.016 | -1.477 |  1.341 |  0.455 | torch.Size([64, 1, 1, 3, 3]) || conv_init0.0.weight
 |  0.000 | -0.244 |  0.235 |  0.059 | torch.Size([64, 64, 1, 3, 3]) || conv_init.0.weight
 |  0.385 | -0.183 |  0.992 |  0.307 | torch.Size([64]) || conv_init.1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.1.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.1.running_mean
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || conv_init.1.running_var
 |  0.001 | -0.238 |  0.278 |  0.059 | torch.Size([64, 64, 1, 3, 3]) || conv_init.3.weight
 |  0.356 | -0.175 |  0.947 |  0.333 | torch.Size([64]) || conv_init.4.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.4.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.4.running_mean
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || conv_init.4.running_var
 | -0.000 | -0.239 |  0.224 |  0.059 | torch.Size([64, 64, 1, 3, 3]) || conv_init.6.weight
 |  0.355 | -0.189 |  0.996 |  0.304 | torch.Size([64]) || conv_init.7.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.7.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.7.running_mean
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || conv_init.7.running_var
 | -0.000 | -0.260 |  0.253 |  0.059 | torch.Size([128, 576]) || altblock.0.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.spa_trans.attention.lamb
 | -0.000 | -0.513 |  0.538 |  0.126 | torch.Size([384, 128]) || altblock.0.spa_trans.attention.qkv.weight
 |  0.001 | -0.539 |  0.519 |  0.126 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.attention.proj.bias
 | -0.079 | -1.131 |  1.222 |  0.486 | torch.Size([8, 8]) || altblock.0.spa_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.spa_trans.attention.proj_l.bias
 | -0.055 | -1.085 |  1.050 |  0.505 | torch.Size([8, 8]) || altblock.0.spa_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.spa_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.bias
 | -0.000 | -0.514 |  0.503 |  0.125 | torch.Size([256, 128]) || altblock.0.spa_trans.feed_forward.1.weight
 | -0.001 | -0.335 |  0.359 |  0.089 | torch.Size([128, 256]) || altblock.0.spa_trans.feed_forward.4.weight
 | -0.000 | -0.465 |  0.498 |  0.125 | torch.Size([64, 128, 1, 1, 1]) || altblock.0.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.ang_trans.attention.lamb
 |  0.001 | -0.648 |  0.665 |  0.176 | torch.Size([192, 64]) || altblock.0.ang_trans.attention.qkv.weight
 |  0.005 | -0.610 |  0.660 |  0.179 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.attention.proj.bias
 |  0.072 | -1.042 |  1.456 |  0.521 | torch.Size([8, 8]) || altblock.0.ang_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.ang_trans.attention.proj_l.bias
 | -0.063 | -1.083 |  0.929 |  0.476 | torch.Size([8, 8]) || altblock.0.ang_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.ang_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.bias
 |  0.002 | -0.720 |  0.656 |  0.176 | torch.Size([128, 64]) || altblock.0.ang_trans.feed_forward.1.weight
 |  0.002 | -0.466 |  0.598 |  0.125 | torch.Size([64, 128]) || altblock.0.ang_trans.feed_forward.4.weight
 |  0.001 | -0.268 |  0.249 |  0.059 | torch.Size([128, 576]) || altblock.1.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.spa_trans.attention.lamb
 | -0.000 | -0.534 |  0.522 |  0.125 | torch.Size([384, 128]) || altblock.1.spa_trans.attention.qkv.weight
 |  0.001 | -0.541 |  0.593 |  0.126 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.attention.proj.bias
 |  0.078 | -1.467 |  1.208 |  0.508 | torch.Size([8, 8]) || altblock.1.spa_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.spa_trans.attention.proj_l.bias
 |  0.044 | -1.096 |  1.375 |  0.490 | torch.Size([8, 8]) || altblock.1.spa_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.spa_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.bias
 | -0.001 | -0.527 |  0.500 |  0.125 | torch.Size([256, 128]) || altblock.1.spa_trans.feed_forward.1.weight
 | -0.000 | -0.348 |  0.323 |  0.088 | torch.Size([128, 256]) || altblock.1.spa_trans.feed_forward.4.weight
 |  0.002 | -0.536 |  0.522 |  0.124 | torch.Size([64, 128, 1, 1, 1]) || altblock.1.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.ang_trans.attention.lamb
 | -0.001 | -0.676 |  0.702 |  0.178 | torch.Size([192, 64]) || altblock.1.ang_trans.attention.qkv.weight
 |  0.003 | -0.583 |  0.630 |  0.174 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.attention.proj.bias
 | -0.012 | -1.450 |  1.523 |  0.551 | torch.Size([8, 8]) || altblock.1.ang_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.ang_trans.attention.proj_l.bias
 | -0.059 | -0.958 |  1.164 |  0.459 | torch.Size([8, 8]) || altblock.1.ang_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.ang_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.bias
 | -0.000 | -0.711 |  0.613 |  0.175 | torch.Size([128, 64]) || altblock.1.ang_trans.feed_forward.1.weight
 | -0.000 | -0.517 |  0.439 |  0.128 | torch.Size([64, 128]) || altblock.1.ang_trans.feed_forward.4.weight
 | -0.000 | -0.244 |  0.284 |  0.059 | torch.Size([128, 576]) || altblock.2.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.spa_trans.attention.lamb
 | -0.001 | -0.568 |  0.488 |  0.126 | torch.Size([384, 128]) || altblock.2.spa_trans.attention.qkv.weight
 | -0.000 | -0.449 |  0.451 |  0.125 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.attention.proj.bias
 |  0.088 | -1.688 |  1.101 |  0.532 | torch.Size([8, 8]) || altblock.2.spa_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.spa_trans.attention.proj_l.bias
 |  0.045 | -0.767 |  1.135 |  0.460 | torch.Size([8, 8]) || altblock.2.spa_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.spa_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.bias
 |  0.000 | -0.483 |  0.511 |  0.124 | torch.Size([256, 128]) || altblock.2.spa_trans.feed_forward.1.weight
 |  0.001 | -0.362 |  0.390 |  0.088 | torch.Size([128, 256]) || altblock.2.spa_trans.feed_forward.4.weight
 |  0.001 | -0.459 |  0.530 |  0.124 | torch.Size([64, 128, 1, 1, 1]) || altblock.2.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.ang_trans.attention.lamb
 | -0.001 | -0.678 |  0.625 |  0.178 | torch.Size([192, 64]) || altblock.2.ang_trans.attention.qkv.weight
 |  0.002 | -0.580 |  0.594 |  0.172 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.attention.proj.bias
 | -0.149 | -1.073 |  1.049 |  0.498 | torch.Size([8, 8]) || altblock.2.ang_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.ang_trans.attention.proj_l.bias
 | -0.057 | -1.139 |  0.877 |  0.445 | torch.Size([8, 8]) || altblock.2.ang_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.ang_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.bias
 | -0.002 | -0.810 |  0.664 |  0.179 | torch.Size([128, 64]) || altblock.2.ang_trans.feed_forward.1.weight
 | -0.001 | -0.489 |  0.448 |  0.124 | torch.Size([64, 128]) || altblock.2.ang_trans.feed_forward.4.weight
 | -0.000 | -0.254 |  0.248 |  0.059 | torch.Size([128, 576]) || altblock.3.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.spa_trans.attention.lamb
 |  0.000 | -0.556 |  0.600 |  0.125 | torch.Size([384, 128]) || altblock.3.spa_trans.attention.qkv.weight
 |  0.000 | -0.550 |  0.456 |  0.126 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.attention.proj.bias
 |  0.007 | -0.938 |  0.984 |  0.468 | torch.Size([8, 8]) || altblock.3.spa_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.spa_trans.attention.proj_l.bias
 |  0.035 | -0.898 |  1.237 |  0.506 | torch.Size([8, 8]) || altblock.3.spa_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.spa_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.bias
 |  0.001 | -0.522 |  0.508 |  0.125 | torch.Size([256, 128]) || altblock.3.spa_trans.feed_forward.1.weight
 |  0.000 | -0.361 |  0.381 |  0.088 | torch.Size([128, 256]) || altblock.3.spa_trans.feed_forward.4.weight
 | -0.001 | -0.506 |  0.429 |  0.125 | torch.Size([64, 128, 1, 1, 1]) || altblock.3.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.ang_trans.attention.lamb
 |  0.001 | -0.721 |  0.639 |  0.178 | torch.Size([192, 64]) || altblock.3.ang_trans.attention.qkv.weight
 |  0.002 | -0.572 |  0.617 |  0.177 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.attention.proj.bias
 |  0.035 | -1.233 |  1.100 |  0.522 | torch.Size([8, 8]) || altblock.3.ang_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.ang_trans.attention.proj_l.bias
 | -0.021 | -0.959 |  1.086 |  0.395 | torch.Size([8, 8]) || altblock.3.ang_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.ang_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.bias
 |  0.000 | -0.778 |  0.623 |  0.178 | torch.Size([128, 64]) || altblock.3.ang_trans.feed_forward.1.weight
 |  0.000 | -0.507 |  0.545 |  0.125 | torch.Size([64, 128]) || altblock.3.ang_trans.feed_forward.4.weight
 |  0.001 | -0.749 |  0.708 |  0.178 | torch.Size([1024, 64, 1, 1]) || upsampling.0.weight
 | -0.005 | -0.201 |  0.162 |  0.059 | torch.Size([1, 64, 3, 3]) || upsampling.3.weight
 | -0.045 | -0.045 | -0.045 |    nan | torch.Size([1]) || resizer.weights.0
 |  0.004 |  0.004 |  0.004 |    nan | torch.Size([1]) || resizer.weights.1
 |  0.036 |  0.036 |  0.036 |    nan | torch.Size([1]) || resizer.weights.2
 |  0.010 |  0.010 |  0.010 |    nan | torch.Size([1]) || resizer.weights.3
 | -0.038 | -0.038 | -0.038 |    nan | torch.Size([1]) || resizer.weights.4
 | -0.002 | -0.002 | -0.002 |    nan | torch.Size([1]) || resizer.weights.5
 |  0.041 |  0.041 |  0.041 |    nan | torch.Size([1]) || resizer.weights.6
 |  0.004 |  0.004 |  0.004 |    nan | torch.Size([1]) || resizer.weights.7
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.0
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.1
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.2
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.3
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.4
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.5
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.6
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.7

