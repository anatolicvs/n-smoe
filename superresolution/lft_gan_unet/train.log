24-04-22 17:03:25.466 :   task: lft_gan_unet
  model: gan
  gpu_ids: [0]
  scale: 2
  n_channels: 3
  sigma: [0, 50]
  sigma_test: 15
  merge_bn: False
  merge_bn_startpoint: 400000
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netD: None
    pretrained_netE: None
    task: superresolution/lft_gan_unet
    log: superresolution/lft_gan_unet
    options: superresolution/lft_gan_unet/options
    models: superresolution/lft_gan_unet/models
    images: superresolution/lft_gan_unet/images
    pretrained_optimizerG: None
    pretrained_optimizerD: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr-lf
      dataroot_H: /mnt/d/LF/data_for_train
      data_name: ALL
      angRes: 5
      dataroot_L: None
      H_size: 660
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 2
      phw: 32
      stride: 2
      phase: train
      scale: 2
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr-lf
      dataroot_H: /mnt/d/LF/data_for_test
      angRes: 5
      dataroot_L: None
      data_name: ALL
      H_size: 660
      dataloader_num_workers: 8
      dataloader_batch_size: 1
      phw: 32
      stride: 2
      phase: test
      scale: 2
      n_channels: 3
    ]
  ]
  netG:[
    net_type: lft_gan
    angRes: 5
    scale_factor: 4
    channels: 64
    init_type: kaiming_normal
    init_bn_type: uniform
    init_gain: 0.2
    scale: 2
  ]
  netD:[
    net_type: discriminator_unet
    in_nc: 1
    base_nc: 64
    act_mode: BL
    n_layers: 3
    norm_type: 3
    init_type: kaiming_normal
    init_bn_type: uniform
    init_gain: 0.2
  ]
  train:[
    G_lossfn_type: l2sum
    G_lossfn_weight: 0.01
    F_lossfn_type: l1
    F_lossfn_weight: 1
    F_feature_layer: 34
    F_weights: 1.0
    F_use_input_norm: True
    F_use_range_norm: False
    gan_type: gan
    D_lossfn_weight: 0.005
    E_decay: 0.999
    G_scheduler_T_max: 2000000
    D_scheduler_T_max: 2000000
    G_scheduler_eta_min: 1e-07
    D_scheduler_eta_min: 1e-07
    D_init_iters: 0
    G_optimizer_type: adam
    G_optimizer_lr: 0.001
    G_optimizer_wd: 0
    G_clip_value: 1.0
    D_optimizer_type: adam
    D_optimizer_lr: 0.001
    D_optimizer_wd: 0
    D_clip_value: 1.0
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    G_scheduler_gamma: 0.5
    G_optimizer_reuse: False
    D_scheduler_type: MultiStepLR
    D_scheduler_milestones: [200000, 800000, 1200000, 2000000]
    D_scheduler_gamma: 0.5
    D_optimizer_reuse: False
    G_param_strict: True
    D_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/train_lft_gan.json
  is_train: True
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

24-04-22 17:03:25.842 : Number of train images: 9,036, iters: 4,518
24-04-22 17:03:27.610 : 
Networks name: LFT
Params number: 1165776
Net structure:
LFT(
  (pos_encoding): PositionEncoding()
  (conv_init0): Sequential(
    (0): Conv3d(1, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
  )
  (conv_init): Sequential(
    (0): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.2, inplace=True)
    (3): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): LeakyReLU(negative_slope=0.2, inplace=True)
    (6): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)
    (7): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): LeakyReLU(negative_slope=0.2, inplace=True)
  )
  (altblock): Sequential(
    (0): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=128, out_features=384, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=64, out_features=192, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=128, out_features=384, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=64, out_features=192, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=128, out_features=384, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=64, out_features=192, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): AltFilter(
      (spa_trans): SpaTrans(
        (MLP): Linear(in_features=576, out_features=128, bias=False)
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=128, out_features=384, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=128, out_features=256, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=256, out_features=128, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
        (linear): Sequential(
          (0): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
      )
      (ang_trans): AngTrans(
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attention): Attention_talking_head(
          (qkv): Linear(in_features=64, out_features=192, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_l): Linear(in_features=8, out_features=8, bias=True)
          (proj_w): Linear(in_features=8, out_features=8, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): Sequential(
          (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (1): Linear(in_features=64, out_features=128, bias=False)
          (2): ReLU(inplace=True)
          (3): Dropout(p=0.0, inplace=False)
          (4): Linear(in_features=128, out_features=64, bias=False)
          (5): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsampling): Sequential(
    (0): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): PixelShuffle(upscale_factor=4)
    (2): LeakyReLU(negative_slope=0.2)
    (3): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (resizer): MullerResizer(
    (weights): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (1): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (2): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (3): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (4): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (5): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (6): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (7): Parameter containing: [torch.float32 of size 1 (cuda:0)]
    )
    (biases): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (1): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (2): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (3): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (4): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (5): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (6): Parameter containing: [torch.float32 of size 1 (cuda:0)]
        (7): Parameter containing: [torch.float32 of size 1 (cuda:0)]
    )
  )
)

Networks name: Discriminator_UNet
Params number: 4375745
Net structure:
Discriminator_UNet(
  (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
  (conv4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (conv9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

24-04-22 17:03:27.676 : 
 |  mean  |  min   |  max   |  std   || shape               
 | -0.015 | -1.378 |  1.449 |  0.468 | torch.Size([64, 1, 1, 3, 3]) || conv_init0.0.weight
 | -0.000 | -0.257 |  0.239 |  0.059 | torch.Size([64, 64, 1, 3, 3]) || conv_init.0.weight
 |  0.439 | -0.196 |  0.991 |  0.333 | torch.Size([64]) || conv_init.1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.1.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.1.running_mean
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || conv_init.1.running_var
 |  0.000 | -0.231 |  0.256 |  0.059 | torch.Size([64, 64, 1, 3, 3]) || conv_init.3.weight
 |  0.390 | -0.199 |  0.983 |  0.329 | torch.Size([64]) || conv_init.4.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.4.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.4.running_mean
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || conv_init.4.running_var
 | -0.000 | -0.260 |  0.226 |  0.059 | torch.Size([64, 64, 1, 3, 3]) || conv_init.6.weight
 |  0.421 | -0.155 |  0.990 |  0.347 | torch.Size([64]) || conv_init.7.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.7.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || conv_init.7.running_mean
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || conv_init.7.running_var
 | -0.000 | -0.266 |  0.232 |  0.059 | torch.Size([128, 576]) || altblock.0.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.spa_trans.attention.lamb
 |  0.001 | -0.521 |  0.536 |  0.126 | torch.Size([384, 128]) || altblock.0.spa_trans.attention.qkv.weight
 |  0.000 | -0.488 |  0.512 |  0.126 | torch.Size([128, 128]) || altblock.0.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.attention.proj.bias
 |  0.098 | -1.214 |  1.161 |  0.518 | torch.Size([8, 8]) || altblock.0.spa_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.spa_trans.attention.proj_l.bias
 | -0.085 | -1.133 |  1.156 |  0.513 | torch.Size([8, 8]) || altblock.0.spa_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.spa_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.0.spa_trans.feed_forward.0.bias
 | -0.001 | -0.475 |  0.525 |  0.125 | torch.Size([256, 128]) || altblock.0.spa_trans.feed_forward.1.weight
 |  0.001 | -0.355 |  0.369 |  0.088 | torch.Size([128, 256]) || altblock.0.spa_trans.feed_forward.4.weight
 |  0.002 | -0.498 |  0.466 |  0.127 | torch.Size([64, 128, 1, 1, 1]) || altblock.0.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.ang_trans.attention.lamb
 | -0.000 | -0.631 |  0.659 |  0.175 | torch.Size([192, 64]) || altblock.0.ang_trans.attention.qkv.weight
 |  0.000 | -0.615 |  0.634 |  0.178 | torch.Size([64, 64]) || altblock.0.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.attention.proj.bias
 |  0.070 | -1.389 |  1.421 |  0.504 | torch.Size([8, 8]) || altblock.0.ang_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.ang_trans.attention.proj_l.bias
 | -0.041 | -1.058 |  1.249 |  0.493 | torch.Size([8, 8]) || altblock.0.ang_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.0.ang_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.0.ang_trans.feed_forward.0.bias
 | -0.000 | -0.624 |  0.646 |  0.176 | torch.Size([128, 64]) || altblock.0.ang_trans.feed_forward.1.weight
 |  0.002 | -0.477 |  0.490 |  0.126 | torch.Size([64, 128]) || altblock.0.ang_trans.feed_forward.4.weight
 | -0.000 | -0.261 |  0.234 |  0.059 | torch.Size([128, 576]) || altblock.1.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.spa_trans.attention.lamb
 |  0.000 | -0.543 |  0.537 |  0.125 | torch.Size([384, 128]) || altblock.1.spa_trans.attention.qkv.weight
 |  0.002 | -0.455 |  0.604 |  0.125 | torch.Size([128, 128]) || altblock.1.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.attention.proj.bias
 |  0.006 | -1.262 |  0.737 |  0.426 | torch.Size([8, 8]) || altblock.1.spa_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.spa_trans.attention.proj_l.bias
 | -0.015 | -1.074 |  1.118 |  0.514 | torch.Size([8, 8]) || altblock.1.spa_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.spa_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.1.spa_trans.feed_forward.0.bias
 | -0.001 | -0.552 |  0.498 |  0.125 | torch.Size([256, 128]) || altblock.1.spa_trans.feed_forward.1.weight
 | -0.001 | -0.346 |  0.378 |  0.088 | torch.Size([128, 256]) || altblock.1.spa_trans.feed_forward.4.weight
 |  0.000 | -0.450 |  0.492 |  0.126 | torch.Size([64, 128, 1, 1, 1]) || altblock.1.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.ang_trans.attention.lamb
 | -0.002 | -0.700 |  0.743 |  0.177 | torch.Size([192, 64]) || altblock.1.ang_trans.attention.qkv.weight
 | -0.003 | -0.599 |  0.749 |  0.179 | torch.Size([64, 64]) || altblock.1.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.attention.proj.bias
 |  0.009 | -1.301 |  1.084 |  0.519 | torch.Size([8, 8]) || altblock.1.ang_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.ang_trans.attention.proj_l.bias
 | -0.073 | -1.441 |  0.906 |  0.476 | torch.Size([8, 8]) || altblock.1.ang_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.1.ang_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.1.ang_trans.feed_forward.0.bias
 | -0.001 | -0.610 |  0.632 |  0.177 | torch.Size([128, 64]) || altblock.1.ang_trans.feed_forward.1.weight
 | -0.001 | -0.411 |  0.465 |  0.124 | torch.Size([64, 128]) || altblock.1.ang_trans.feed_forward.4.weight
 |  0.000 | -0.248 |  0.249 |  0.059 | torch.Size([128, 576]) || altblock.2.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.spa_trans.attention.lamb
 | -0.000 | -0.511 |  0.535 |  0.125 | torch.Size([384, 128]) || altblock.2.spa_trans.attention.qkv.weight
 |  0.001 | -0.529 |  0.484 |  0.126 | torch.Size([128, 128]) || altblock.2.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.attention.proj.bias
 | -0.040 | -1.035 |  1.854 |  0.500 | torch.Size([8, 8]) || altblock.2.spa_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.spa_trans.attention.proj_l.bias
 | -0.081 | -1.184 |  1.006 |  0.488 | torch.Size([8, 8]) || altblock.2.spa_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.spa_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.2.spa_trans.feed_forward.0.bias
 | -0.000 | -0.475 |  0.451 |  0.125 | torch.Size([256, 128]) || altblock.2.spa_trans.feed_forward.1.weight
 | -0.000 | -0.388 |  0.333 |  0.088 | torch.Size([128, 256]) || altblock.2.spa_trans.feed_forward.4.weight
 | -0.001 | -0.485 |  0.499 |  0.126 | torch.Size([64, 128, 1, 1, 1]) || altblock.2.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.ang_trans.attention.lamb
 |  0.000 | -0.703 |  0.833 |  0.176 | torch.Size([192, 64]) || altblock.2.ang_trans.attention.qkv.weight
 | -0.003 | -0.610 |  0.740 |  0.177 | torch.Size([64, 64]) || altblock.2.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.attention.proj.bias
 | -0.043 | -0.953 |  0.996 |  0.480 | torch.Size([8, 8]) || altblock.2.ang_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.ang_trans.attention.proj_l.bias
 | -0.013 | -0.839 |  0.985 |  0.427 | torch.Size([8, 8]) || altblock.2.ang_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.2.ang_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.2.ang_trans.feed_forward.0.bias
 | -0.000 | -0.717 |  0.730 |  0.175 | torch.Size([128, 64]) || altblock.2.ang_trans.feed_forward.1.weight
 |  0.002 | -0.441 |  0.430 |  0.125 | torch.Size([64, 128]) || altblock.2.ang_trans.feed_forward.4.weight
 |  0.000 | -0.272 |  0.241 |  0.059 | torch.Size([128, 576]) || altblock.3.spa_trans.MLP.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.spa_trans.attention.lamb
 | -0.000 | -0.501 |  0.629 |  0.125 | torch.Size([384, 128]) || altblock.3.spa_trans.attention.qkv.weight
 | -0.001 | -0.456 |  0.511 |  0.125 | torch.Size([128, 128]) || altblock.3.spa_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.attention.proj.bias
 |  0.030 | -1.495 |  1.245 |  0.536 | torch.Size([8, 8]) || altblock.3.spa_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.spa_trans.attention.proj_l.bias
 | -0.070 | -1.204 |  1.213 |  0.494 | torch.Size([8, 8]) || altblock.3.spa_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.spa_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([128]) || altblock.3.spa_trans.feed_forward.0.bias
 | -0.000 | -0.544 |  0.476 |  0.125 | torch.Size([256, 128]) || altblock.3.spa_trans.feed_forward.1.weight
 | -0.000 | -0.383 |  0.373 |  0.088 | torch.Size([128, 256]) || altblock.3.spa_trans.feed_forward.4.weight
 |  0.001 | -0.495 |  0.505 |  0.125 | torch.Size([64, 128, 1, 1, 1]) || altblock.3.spa_trans.linear.0.weight
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.norm.bias
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.ang_trans.attention.lamb
 | -0.000 | -0.723 |  0.812 |  0.177 | torch.Size([192, 64]) || altblock.3.ang_trans.attention.qkv.weight
 | -0.001 | -0.639 |  0.573 |  0.174 | torch.Size([64, 64]) || altblock.3.ang_trans.attention.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.attention.proj.bias
 | -0.089 | -1.182 |  0.983 |  0.507 | torch.Size([8, 8]) || altblock.3.ang_trans.attention.proj_l.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.ang_trans.attention.proj_l.bias
 | -0.021 | -0.886 |  0.892 |  0.433 | torch.Size([8, 8]) || altblock.3.ang_trans.attention.proj_w.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([8]) || altblock.3.ang_trans.attention.proj_w.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([64]) || altblock.3.ang_trans.feed_forward.0.bias
 | -0.005 | -0.698 |  0.616 |  0.177 | torch.Size([128, 64]) || altblock.3.ang_trans.feed_forward.1.weight
 | -0.001 | -0.449 |  0.477 |  0.126 | torch.Size([64, 128]) || altblock.3.ang_trans.feed_forward.4.weight
 |  0.001 | -0.897 |  0.823 |  0.175 | torch.Size([1024, 64, 1, 1]) || upsampling.0.weight
 | -0.003 | -0.225 |  0.176 |  0.058 | torch.Size([1, 64, 3, 3]) || upsampling.3.weight
 |  0.036 |  0.036 |  0.036 |    nan | torch.Size([1]) || resizer.weights.0
 | -0.011 | -0.011 | -0.011 |    nan | torch.Size([1]) || resizer.weights.1
 |  0.004 |  0.004 |  0.004 |    nan | torch.Size([1]) || resizer.weights.2
 | -0.015 | -0.015 | -0.015 |    nan | torch.Size([1]) || resizer.weights.3
 | -0.016 | -0.016 | -0.016 |    nan | torch.Size([1]) || resizer.weights.4
 |  0.049 |  0.049 |  0.049 |    nan | torch.Size([1]) || resizer.weights.5
 |  0.048 |  0.048 |  0.048 |    nan | torch.Size([1]) || resizer.weights.6
 | -0.004 | -0.004 | -0.004 |    nan | torch.Size([1]) || resizer.weights.7
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.0
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.1
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.2
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.3
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.4
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.5
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.6
 |  0.000 |  0.000 |  0.000 |    nan | torch.Size([1]) || resizer.biases.7

